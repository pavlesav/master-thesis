{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e523e35",
   "metadata": {},
   "source": [
    "# Parliamentary Speech Segment Embeddings\n",
    "\n",
    "This notebook calculates segment embeddings for parliamentary datasets (AT, HR, GB) based on the `Text` column grouped by `Segment_ID`.\n",
    "\n",
    "## Key Features:\n",
    "- **Multi-language support**: English and native language embeddings for AT and HR, English-only for GB.\n",
    "- **Google Colab compatibility**: Includes setup for Google Colab.\n",
    "- **Efficient processing**: GPU optimization for embedding generation.\n",
    "- **Chunking for long texts**: Handles texts exceeding the token limit.\n",
    "\n",
    "## Input:\n",
    "Pre-segmented datasets with `Text` and `Segment_ID` columns.\n",
    "\n",
    "## Output:\n",
    "Dataframes with additional columns for segment embeddings:\n",
    "- `segment_embeddings_english`\n",
    "- `segment_embeddings_native_language` (for AT and HR only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906366bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GOOGLE COLAB SETUP ===\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install sentence-transformers tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# GPU check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected. Processing will be slower.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d6512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD DATASETS ===\n",
    "# Update paths to your Google Drive location\n",
    "base_path = \"/content/drive/My Drive/thesis data/\"\n",
    "\n",
    "AT = pd.read_pickle(f\"{base_path}/AT/AT_combined_with_segments.pkl\")\n",
    "HR = pd.read_pickle(f\"{base_path}/HR/HR_combined_with_segments.pkl\")\n",
    "GB = pd.read_pickle(f\"{base_path}/GB/GB_with_segments.pkl\")\n",
    "\n",
    "print(f\"✅ Datasets loaded: AT ({AT.shape}), HR ({HR.shape}), GB ({GB.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260bfc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EMBEDDING FUNCTIONS ===\n",
    "\n",
    "def embed_long_text(text, model, tokenizer, max_length=8192, overlap=1024):\n",
    "    \"\"\"Handle texts longer than the model's max token length.\"\"\"\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    starts = list(range(0, len(token_ids), max_length - overlap))\n",
    "    for start in starts:\n",
    "        end = min(start + max_length, len(token_ids))\n",
    "        chunk_ids = token_ids[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    chunk_embeddings = model.encode(chunks, batch_size=128, convert_to_tensor=True, show_progress_bar=False)\n",
    "    return torch.mean(chunk_embeddings, dim=0).cpu().numpy()\n",
    "\n",
    "def generate_segment_embeddings(df, text_column, segment_id_column, model, batch_size=128, checkpoint_path=None):\n",
    "    \"\"\"Generate embeddings for concatenated segment texts with checkpointing.\"\"\"\n",
    "    print(f\"🔄 Generating embeddings for {text_column} grouped by {segment_id_column}...\")\n",
    "    \n",
    "    # Concatenate texts within each segment\n",
    "    segment_texts = df.groupby(segment_id_column)[text_column].apply(lambda x: ' '.join(x.astype(str)))\n",
    "    segment_ids = segment_texts.index.tolist()\n",
    "    segment_texts = segment_texts.tolist()\n",
    "    \n",
    "    tokenizer = model.tokenizer\n",
    "    embeddings = []\n",
    "    checkpoint_data = {}\n",
    "    \n",
    "    # Load checkpoint if available\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint_data = torch.load(checkpoint_path)\n",
    "        embeddings = checkpoint_data.get(\"embeddings\", [])\n",
    "        start_idx = checkpoint_data.get(\"start_idx\", 0)\n",
    "        print(f\"⏳ Resuming from checkpoint at index {start_idx}...\")\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    \n",
    "    for i in tqdm(range(start_idx, len(segment_texts), batch_size), desc=\"🚀 Embedding segments\", unit=\"batch\"):\n",
    "        batch_texts = segment_texts[i:i+batch_size]\n",
    "        batch_embeddings = []\n",
    "        for text in batch_texts:\n",
    "            if len(tokenizer.encode(text, add_special_tokens=False)) > 8192:\n",
    "                batch_embeddings.append(embed_long_text(text, model, tokenizer))\n",
    "            else:\n",
    "                batch_embeddings.append(model.encode(text, convert_to_tensor=True).cpu().numpy())\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if checkpoint_path:\n",
    "            checkpoint_data = {\"embeddings\": embeddings, \"start_idx\": i + batch_size}\n",
    "            torch.save(checkpoint_data, checkpoint_path)\n",
    "            print(f\"💾 Checkpoint saved at index {i + batch_size}\")\n",
    "    \n",
    "    # Map embeddings back to the dataframe\n",
    "    embedding_map = dict(zip(segment_ids, embeddings))\n",
    "    return df[segment_id_column].map(embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD MODEL ===\n",
    "model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"✅ SentenceTransformer model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee36903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROCESS DATASETS ===\n",
    "\n",
    "# AT: Generate embeddings for English and German\n",
    "print(\"🔄 Processing Austrian Parliament dataset...\")\n",
    "AT[\"segment_embeddings_english\"] = generate_segment_embeddings(\n",
    "    AT, text_column=\"Text\", segment_id_column=\"Segment_ID_english\", model=model,\n",
    "    checkpoint_path=f\"{base_path}/AT/english_checkpoint.pt\"\n",
    ")\n",
    "AT.to_pickle(f\"{base_path}/AT/AT_with_english_embeddings.pkl\")\n",
    "print(\"✅ Saved Austrian Parliament dataset with English embeddings.\")\n",
    "\n",
    "AT[\"segment_embeddings_native_language\"] = generate_segment_embeddings(\n",
    "    AT, text_column=\"Text_native_language\", segment_id_column=\"Segment_ID_german\", model=model,\n",
    "    checkpoint_path=f\"{base_path}/AT/native_checkpoint.pt\"\n",
    ")\n",
    "AT.to_pickle(f\"{base_path}/AT/AT_with_native_embeddings.pkl\")\n",
    "print(\"✅ Saved Austrian Parliament dataset with native language embeddings.\")\n",
    "\n",
    "# HR: Generate embeddings for English and Croatian\n",
    "print(\"🔄 Processing Croatian Parliament dataset...\")\n",
    "HR[\"segment_embeddings_english\"] = generate_segment_embeddings(\n",
    "    HR, text_column=\"Text\", segment_id_column=\"Segment_ID_english\", model=model,\n",
    "    checkpoint_path=f\"{base_path}/HR/english_checkpoint.pt\"\n",
    ")\n",
    "HR.to_pickle(f\"{base_path}/HR/HR_with_english_embeddings.pkl\")\n",
    "print(\"✅ Saved Croatian Parliament dataset with English embeddings.\")\n",
    "\n",
    "HR[\"segment_embeddings_native_language\"] = generate_segment_embeddings(\n",
    "    HR, text_column=\"Text_native_language\", segment_id_column=\"Segment_ID_croatian\", model=model,\n",
    "    checkpoint_path=f\"{base_path}/HR/native_checkpoint.pt\"\n",
    ")\n",
    "HR.to_pickle(f\"{base_path}/HR/HR_with_native_embeddings.pkl\")\n",
    "print(\"✅ Saved Croatian Parliament dataset with native language embeddings.\")\n",
    "\n",
    "# GB: Generate embeddings for English only\n",
    "print(\"🔄 Processing British Parliament dataset...\")\n",
    "GB[\"segment_embeddings_english\"] = generate_segment_embeddings(\n",
    "    GB, text_column=\"Text\", segment_id_column=\"Segment_ID\", model=model,\n",
    "    checkpoint_path=f\"{base_path}/GB/english_checkpoint.pt\"\n",
    ")\n",
    "GB.to_pickle(f\"{base_path}/GB/GB_with_english_embeddings.pkl\")\n",
    "print(\"✅ Saved British Parliament dataset with English embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
