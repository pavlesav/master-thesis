{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1ca87c",
   "metadata": {},
   "source": [
    "# Topic Modeling with BERTopic on Parliamentary Speeches - Google Colab Version\n",
    "\n",
    "This notebook is optimized for Google Colab with GPU acceleration. It implements a complete pipeline from data loading to topic modeling:\n",
    "\n",
    "1. **Data Loading** - Loads the AT_original_complete.pkl file and processes it\n",
    "2. **Data Filtering** - Creates processed version for topic modeling\n",
    "3. **Dual Embedding** - Speech-level and segment-level embeddings\n",
    "4. **Semantic Segmentation** - Similarity-based boundary detection  \n",
    "5. **Topic Discovery** - BERTopic with custom clustering\n",
    "6. **Topic Naming** - LLM-generated readable names\n",
    "\n",
    "## Key Approach - Dual Embedding Strategy:\n",
    "- **First embedding**: Individual speeches using raw text (for segmentation)\n",
    "- **Second embedding**: Concatenated segment texts (for topic modeling)\n",
    "- **Why twice?** Re-embedding captures full discourse coherence vs. averaging individual embeddings\n",
    "- **Raw text used throughout** for better semantic capture\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Upload only one file** to Google Drive: `AT_original_complete.pkl`\n",
    "   - Place it in a folder like: `MyDrive/thesis_data/AT_original_complete.pkl`\n",
    "2. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "3. **Run the setup cell** below to mount Drive and install packages\n",
    "4. **Update data path** to match your Google Drive structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955eaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GOOGLE COLAB SETUP ===\n",
    "# Mount Google Drive to access your data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install sentence-transformers bertopic umap-learn hdbscan tqdm openai python-dotenv\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU detected - will use CPU (slower)\")\n",
    "\n",
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete! ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340efcc0",
   "metadata": {},
   "source": [
    "## Data Loading and Processing\n",
    "\n",
    "Load the original complete data from Google Drive and create the processed version for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f113b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA LOADING AND PROCESSING ===\n",
    "\n",
    "# IMPORTANT: Update this path to match your Google Drive structure\n",
    "data_folder = '/content/drive/MyDrive/thesis data/'  # Update this path!\n",
    "data_path = f'{data_folder}AT_original_complete.pkl'\n",
    "\n",
    "print(\"üöÄ Loading original complete data...\")\n",
    "\n",
    "\n",
    "# Load ORIGINAL complete data\n",
    "AT_original_df = pd.read_pickle(data_path)\n",
    "print(f\"‚úÖ Loaded original complete data: {AT_original_df.shape}\")\n",
    "    \n",
    "# Display basic info about the loaded data\n",
    "print(f\"\\nüìã Dataset Info:\")\n",
    "print(f\"  üìä Total speeches: {len(AT_original_df):,}\")\n",
    "print(f\"  üìÖ Date range: {AT_original_df['Date'].min()} to {AT_original_df['Date'].max()}\")\n",
    "print(f\"  üìù Columns: {list(AT_original_df.columns)}\")\n",
    "    \n",
    "# Check if filtering columns exist, if not create them\n",
    "if 'Word_Count' not in AT_original_df.columns:\n",
    "    print(\"üîÑ Calculating word counts...\")\n",
    "    AT_original_df['Word_Count'] = AT_original_df['Text'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "if 'Is_Too_Short' not in AT_original_df.columns:\n",
    "    print(\"üîÑ Creating filtering flags...\")\n",
    "    min_word_count = 10\n",
    "    AT_original_df['Is_Too_Short'] = AT_original_df['Word_Count'] < min_word_count\n",
    "    AT_original_df['Is_Filtered'] = AT_original_df['Is_Too_Short']\n",
    "    \n",
    "# Create processed version for topic modeling\n",
    "print(\"\\nüîÑ Creating processed version for topic modeling...\")\n",
    "AT_processed_df = AT_original_df[~AT_original_df['Is_Filtered']].copy()\n",
    "    \n",
    "# Sort by sitting and speech order for consistency\n",
    "if 'Sitting_ID' in AT_processed_df.columns and 'Speech_ID' in AT_processed_df.columns:\n",
    "    AT_processed_df = AT_processed_df.sort_values(['Sitting_ID', 'Speech_ID']).reset_index(drop=True)\n",
    "    print(\"üîÑ Sorted speeches by Sitting_ID and Speech_ID\")\n",
    "elif 'Text_ID' in AT_processed_df.columns and 'ID' in AT_processed_df.columns:\n",
    "    AT_processed_df = AT_processed_df.sort_values(['Text_ID', 'ID']).reset_index(drop=True)\n",
    "    print(\"üîÑ Sorted speeches by Text_ID and ID\")\n",
    "    \n",
    "# Standardize column names for topic modeling pipeline\n",
    "if 'Text_ID' in AT_processed_df.columns and 'Sitting_ID' not in AT_processed_df.columns:\n",
    "    AT_processed_df['Sitting_ID'] = AT_processed_df['Text_ID']\n",
    "    print(\"üîÑ Created Sitting_ID from Text_ID\")\n",
    "\n",
    "if 'ID' in AT_processed_df.columns and 'Speaker_ID' not in AT_processed_df.columns:\n",
    "    AT_processed_df['Speaker_ID'] = AT_processed_df['ID']\n",
    "    print(\"üîÑ Created Speaker_ID from ID\")\n",
    "    \n",
    "# Add tracking column\n",
    "AT_processed_df['Used_For_Topic_Modeling'] = True\n",
    "    \n",
    "print(f\"\\nüìà Filtering summary:\")\n",
    "print(f\"  üìä Original speeches: {len(AT_original_df):,}\")\n",
    "print(f\"  üìä For topic modeling: {len(AT_processed_df):,}\")\n",
    "print(f\"  üóëÔ∏è  Too short speeches: {AT_original_df['Is_Too_Short'].sum():,}\")\n",
    "print(f\"  üìä Filtered out: {len(AT_original_df) - len(AT_processed_df):,}\")\n",
    "    \n",
    "# Save processed version for later use\n",
    "AT_processed_df.to_pickle(f'{data_folder}AT_for_topic_modeling.pkl')\n",
    "print(f\"\\nüíæ Saved processed data to: {data_folder}AT_for_topic_modeling.pkl\")\n",
    "    \n",
    "# Verify required columns for pipeline\n",
    "required_cols = ['Text', 'Sitting_ID']\n",
    "missing_cols = [col for col in required_cols if col not in AT_processed_df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Missing required columns: {missing_cols}\")\n",
    "    print(f\"Available columns: {list(AT_processed_df.columns)}\")\n",
    "else:\n",
    "    print(f\"‚úÖ All required columns present for topic modeling pipeline\")\n",
    "\n",
    "print(\"\\n‚úÖ Data loading and processing complete! Ready for topic modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686f837",
   "metadata": {},
   "source": [
    "## Embedding and Segmentation Functions (GPU Optimized)\n",
    "\n",
    "These functions are optimized for GPU acceleration and handle the dual-embedding approach:\n",
    "1. **Speech-level embeddings** for similarity-based segmentation  \n",
    "2. **Segment-level embeddings** for final topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EMBEDDING FUNCTIONS (GPU OPTIMIZED) ===\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.signal import find_peaks\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_embedding_model(model_name=\"nomic-ai/nomic-embed-text-v1.5\", device=None):\n",
    "    \"\"\"\n",
    "    Load a sentence embedding model optimized for Colab GPU.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(f\"Loading embedding model: {model_name} on {device}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if device == 'cpu':\n",
    "            torch.set_num_threads(2)  # Colab CPU optimization\n",
    "            model = SentenceTransformer(\n",
    "                model_name, \n",
    "                device=device, \n",
    "                trust_remote_code=True,\n",
    "                model_kwargs={'torch_dtype': torch.float32}\n",
    "            )\n",
    "        else:\n",
    "            # GPU optimization for Colab\n",
    "            model = SentenceTransformer(model_name, device=device, trust_remote_code=True)\n",
    "        \n",
    "        print(f\"‚úì Model loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
    "        raise e\n",
    "\n",
    "def generate_speech_embeddings_for_segmentation(df, text_column='Text', model_name=\"nomic-ai/nomic-embed-text-v1.5\", batch_size=16):\n",
    "    \"\"\"\n",
    "    FIRST EMBEDDING: Generate embeddings for individual speeches for segmentation.\n",
    "    Optimized for Colab GPU with larger batch sizes and smart filtering.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FIRST EMBEDDING: Individual speeches for segmentation\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Generating embeddings for {len(df)} speeches using {model_name}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = load_embedding_model(model_name)\n",
    "    \n",
    "    # Use FULL texts\n",
    "    texts = df[text_column].astype(str).tolist()\n",
    "    \n",
    "    # Show text length statistics and identify extremely long texts\n",
    "    text_lengths = [len(text) for text in texts]\n",
    "    print(f\"Text length statistics (characters):\")\n",
    "    print(f\"  Min: {min(text_lengths)}, Max: {max(text_lengths)}, Mean: {np.mean(text_lengths):.0f}\")\n",
    "    \n",
    "    # Calculate 99.9th percentile threshold to filter out top 0.1% longest speeches\n",
    "    length_threshold = np.percentile(text_lengths, 99.9)\n",
    "    extremely_long_mask = np.array(text_lengths) > length_threshold\n",
    "    n_extremely_long = extremely_long_mask.sum()\n",
    "    \n",
    "    print(f\"  99.9th percentile length: {length_threshold:.0f} characters\")\n",
    "    print(f\"  Extremely long speeches (top 0.1%): {n_extremely_long}\")\n",
    "    print(f\"  These will be assigned zero embeddings for memory safety\")\n",
    "    \n",
    "    # Adjust batch size for GPU\n",
    "    if torch.cuda.is_available():\n",
    "        batch_size = 32  # Larger batch for GPU\n",
    "        print(f\"Using GPU batch size: {batch_size}\")\n",
    "    else:\n",
    "        batch_size = 8   # Smaller batch for CPU\n",
    "        print(f\"Using CPU batch size: {batch_size}\")\n",
    "    \n",
    "    def embed_with_fallback(text, speech_index):\n",
    "        \"\"\"Embed text with GPU-optimized fallback strategies.\"\"\"\n",
    "        text_len = len(text)\n",
    "        \n",
    "        # Skip extremely long texts (top 0.1%) - assign zero embedding\n",
    "        if text_len > length_threshold:\n",
    "            return np.zeros(model.get_sentence_embedding_dimension())\n",
    "        \n",
    "        try:\n",
    "            # Try full text first with GPU-optimized settings\n",
    "            embedding = model.encode(\n",
    "                [text], \n",
    "                batch_size=1,  # Process individually for fallback\n",
    "                convert_to_tensor=False,\n",
    "                normalize_embeddings=True,\n",
    "                show_progress_bar=False\n",
    "            )[0]\n",
    "            return embedding\n",
    "            \n",
    "        except Exception:\n",
    "            # Fallback: chunking for very long texts\n",
    "            if len(text) > 10000:\n",
    "                try:\n",
    "                    chunks = []\n",
    "                    chunk_size = 8000\n",
    "                    for i in range(0, len(text), chunk_size):\n",
    "                        chunk = text[i:i + chunk_size]\n",
    "                        if len(chunk.strip()) > 100:\n",
    "                            chunks.append(chunk)\n",
    "                        if len(chunks) >= 3:\n",
    "                            break\n",
    "                    \n",
    "                    if chunks:\n",
    "                        chunk_embeddings = model.encode(\n",
    "                            chunks,\n",
    "                            batch_size=min(len(chunks), 4),\n",
    "                            convert_to_tensor=False,\n",
    "                            normalize_embeddings=True,\n",
    "                            show_progress_bar=False\n",
    "                        )\n",
    "                        return np.mean(chunk_embeddings, axis=0)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            # Final fallback: truncate\n",
    "            try:\n",
    "                truncated_text = text[:5000]\n",
    "                embedding = model.encode(\n",
    "                    [truncated_text],\n",
    "                    batch_size=1,\n",
    "                    convert_to_tensor=False,\n",
    "               \n",
    "                    show_progress_bar=False\n",
    "                )[0]\n",
    "                return embedding\n",
    "            except Exception:\n",
    "                return np.zeros(model.get_sentence_embedding_dimension())\n",
    "    \n",
    "    # Generate embeddings with GPU-optimized progress tracking\n",
    "    print(\"Generating speech-level embeddings...\")\n",
    "    start_time = time.time()\n",
    "    embeddings = []\n",
    "    \n",
    "    # Create progress bar optimized for Colab\n",
    "    progress_bar = tqdm(enumerate(texts), total=len(texts), desc=\"üß† Embedding speeches\", \n",
    "                       unit=\"speech\", ncols=100, \n",
    "                       bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n",
    "    \n",
    "    for i, text in progress_bar:\n",
    "        # GPU memory cleanup every 50 speeches (more frequent for GPU)\n",
    "        if i % 50 == 0 and i > 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        embedding = embed_with_fallback(text, i)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "        # Update progress bar with GPU-optimized info\n",
    "        if i % 10 == 0:\n",
    "            rate = i / (time.time() - start_time)\n",
    "            progress_bar.set_postfix({\n",
    "                'Rate': f'{rate:.1f}/s',\n",
    "                'GPU': '‚úì' if torch.cuda.is_available() else '‚úó',\n",
    "                'Filtered': n_extremely_long\n",
    "            })\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    rate = len(texts) / total_time\n",
    "    print(f\"‚úì Speech embeddings completed in {total_time:.2f} seconds\")\n",
    "    print(f\"‚úì Average rate: {rate:.1f} speeches/second\")\n",
    "    print(f\"‚úì Embedding shape: {np.array(embeddings).shape}\")\n",
    "    print(f\"‚úì Filtered out {n_extremely_long} extremely long speeches\")\n",
    "    \n",
    "    # Final GPU memory cleanup\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Add to dataframe\n",
    "    df_with_embeddings = df.copy()\n",
    "    df_with_embeddings['Speech_Embeddings'] = embeddings\n",
    "    df_with_embeddings['Is_Extremely_Long'] = extremely_long_mask\n",
    "    \n",
    "    return df_with_embeddings\n",
    "\n",
    "# Keep the existing similarity and segmentation functions (they don't need GPU optimization)\n",
    "def calculate_windowed_similarity(embeddings_list, window_size=3):\n",
    "    \"\"\"Calculate cosine similarity between windowed embeddings.\"\"\"\n",
    "    if len(embeddings_list) < 2:\n",
    "        return np.array([])\n",
    "    if window_size < 1:\n",
    "        raise ValueError(\"Window size must be at least 1.\")\n",
    "\n",
    "    num_utterances = len(embeddings_list)\n",
    "    similarities = []\n",
    "\n",
    "    for g in range(num_utterances - 1):\n",
    "        # Window before gap\n",
    "        start_before = max(0, g - window_size + 1)\n",
    "        end_before = g + 1\n",
    "        window_before = embeddings_list[start_before:end_before]\n",
    "\n",
    "        # Window after gap\n",
    "        start_after = g + 1\n",
    "        end_after = min(num_utterances, g + 1 + window_size)\n",
    "        window_after = embeddings_list[start_after:end_after]\n",
    "\n",
    "        if not window_before or not window_after:\n",
    "            similarities.append(0)\n",
    "            continue\n",
    "\n",
    "        # Calculate mean embeddings and similarity\n",
    "        mean_before = np.mean([np.asarray(e) for e in window_before], axis=0)\n",
    "        mean_after = np.mean([np.asarray(e) for e in window_after], axis=0)\n",
    "        \n",
    "        sim = cosine_similarity(mean_before.reshape(1, -1), mean_after.reshape(1, -1))[0][0]\n",
    "        similarities.append(sim)\n",
    "        \n",
    "    return np.array(similarities)\n",
    "\n",
    "def find_topic_boundaries(similarities, height_threshold=0.25, prominence_threshold=0.15, distance_threshold=5):\n",
    "    \"\"\"Find topic boundaries using peak detection on inverted similarity scores.\"\"\"\n",
    "    if len(similarities) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Invert similarities to find valleys (topic boundaries)\n",
    "    inverted_similarities = np.maximum(0, 1 - similarities)\n",
    "    \n",
    "    # Find peaks in inverted similarities\n",
    "    peaks, _ = find_peaks(\n",
    "        inverted_similarities,\n",
    "        height=height_threshold,\n",
    "        prominence=prominence_threshold,\n",
    "        distance=distance_threshold\n",
    "    )\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "def segment_speeches_by_similarity(df, window_size=3, height_threshold=0.25, \n",
    "                                   prominence_threshold=0.15, distance_threshold=5):\n",
    "    \"\"\"Segment speeches within each sitting based on semantic similarity.\"\"\"\n",
    "    print(f\"üîç Segmenting speeches using similarity-based approach\")\n",
    "    print(f\"Parameters: window_size={window_size}, height_threshold={height_threshold}\")\n",
    "    print(f\"           prominence_threshold={prominence_threshold}, distance_threshold={distance_threshold}\")\n",
    "    \n",
    "    df_segmented = df.copy()\n",
    "    segment_ids = []\n",
    "    total_boundaries = 0\n",
    "    \n",
    "    # Process each sitting separately with progress bar\n",
    "    sittings = list(df_segmented.groupby('Sitting_ID'))\n",
    "    \n",
    "    for sitting_id, group in tqdm(sittings, desc=\"üî™ Segmenting sittings\", unit=\"sitting\"):\n",
    "        if len(group) < 2:\n",
    "            segment_ids.extend([f\"{sitting_id}_seg_0\"] * len(group))\n",
    "            continue\n",
    "        \n",
    "        # Use the speech-level embeddings for segmentation\n",
    "        embeddings_list = group['Speech_Embeddings'].tolist()\n",
    "        similarities = calculate_windowed_similarity(embeddings_list, window_size)\n",
    "        \n",
    "        if len(similarities) == 0:\n",
    "            segment_ids.extend([f\"{sitting_id}_seg_0\"] * len(group))\n",
    "            continue\n",
    "        \n",
    "        # Find boundaries\n",
    "        boundaries = find_topic_boundaries(\n",
    "            similarities, height_threshold, prominence_threshold, distance_threshold\n",
    "        )\n",
    "        total_boundaries += len(boundaries)\n",
    "        \n",
    "        # Assign segment IDs\n",
    "        current_segment = 0\n",
    "        sitting_segment_ids = []\n",
    "        \n",
    "        for i in range(len(group)):\n",
    "            if i > 0 and (i - 1) in boundaries:\n",
    "                current_segment += 1\n",
    "            sitting_segment_ids.append(f\"{sitting_id}_seg_{current_segment}\")\n",
    "        \n",
    "        segment_ids.extend(sitting_segment_ids)\n",
    "    \n",
    "    df_segmented['Segment_ID'] = segment_ids\n",
    "    \n",
    "    # Print statistics\n",
    "    total_segments = df_segmented['Segment_ID'].nunique()\n",
    "    avg_segments_per_sitting = df_segmented.groupby('Sitting_ID')['Segment_ID'].nunique().mean()\n",
    "    \n",
    "    print(f\"‚úì Segmentation complete!\")\n",
    "    print(f\"‚úì Total boundaries detected: {total_boundaries}\")\n",
    "    print(f\"‚úì Total segments created: {total_segments}\")\n",
    "    print(f\"‚úì Average segments per sitting: {avg_segments_per_sitting:.2f}\")\n",
    "    \n",
    "    return df_segmented\n",
    "\n",
    "print(\"‚úì Embedding and segmentation functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38b773",
   "metadata": {},
   "source": [
    "## Segment Aggregation and Re-embedding Functions (GPU Optimized)\n",
    "\n",
    "After creating segments, we aggregate the raw text and re-embed for better topic modeling representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEGMENT AGGREGATION AND RE-EMBEDDING FUNCTIONS (GPU OPTIMIZED) ===\n",
    "\n",
    "def aggregate_segments(df, text_column='Text', segment_id_column='Segment_ID'):\n",
    "    \"\"\"\n",
    "    Aggregate segments by concatenating their texts.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input dataframe with segments\n",
    "        text_column (str): Column name for the text data\n",
    "        segment_id_column (str): Column name for the segment IDs\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Aggregated segments with combined texts\n",
    "    \"\"\"\n",
    "    print(f\"üì¶ Aggregating {len(df)} segments...\")\n",
    "    \n",
    "    # Group by segment ID and concatenate texts\n",
    "    aggregated_df = df.groupby(segment_id_column).agg({\n",
    "        text_column: ' '.join,\n",
    "        'Sitting_ID': 'first'  # Keep one sitting ID per segment\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    aggregated_df.rename(columns={text_column: 'Combined_Text'}, inplace=True)\n",
    "    \n",
    "    print(f\"‚úì Aggregated segments: {len(aggregated_df)}\")\n",
    "    return aggregated_df\n",
    "\n",
    "def re_embed_segments(df, text_column='Combined_Text', model_name=\"nomic-ai/nomic-embed-text-v1.5\"):\n",
    "    \"\"\"\n",
    "    Re-embed aggregated segments using the segment-level embedding model.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input dataframe with aggregated segments\n",
    "        text_column (str): Column name for the text data\n",
    "        model_name (str): SentenceTransformer model name\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Segments with updated embeddings\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Re-embedding {len(df)} aggregated segments...\")\n",
    "    \n",
    "    # Load embedding model\n",
    "    model = load_embedding_model(model_name)\n",
    "    \n",
    "    # Use FULL texts for re-embedding\n",
    "    texts = df[text_column].astype(str).tolist()\n",
    "    \n",
    "    # Generate embeddings with GPU-optimized settings\n",
    "    embeddings = model.encode(\n",
    "        texts, \n",
    "        batch_size=32,  # Optimal batch size for GPU\n",
    "        convert_to_tensor=False,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Add embeddings to dataframe\n",
    "    df['Segment_Embeddings'] = embeddings\n",
    "    \n",
    "    print(f\"‚úì Re-embedded segments: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "print(\"‚úì Segment aggregation and re-embedding functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab1765",
   "metadata": {},
   "source": [
    "## Topic Modeling Functions (Colab Optimized)\n",
    "\n",
    "These functions handle BERTopic training using the re-embedded segment representations, optimized for Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TOPIC MODELING FUNCTIONS (COLAB OPTIMIZED) ===\n",
    "from bertopic import BERTopic\n",
    "\n",
    "def train_bertopic_model(df, text_column='Combined_Text', embeddings_column='Segment_Embeddings', \n",
    "                         min_topic_size=10, nr_topics='auto', language='english'):\n",
    "    \"\"\"\n",
    "    Train a BERTopic model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input dataframe with documents\n",
    "        text_column (str): Column name for the text data\n",
    "        embeddings_column (str): Column name for the embeddings\n",
    "        min_topic_size (int): Minimum size of topics\n",
    "        nr_topics (int or str): Number of topics (auto or fixed number)\n",
    "        language (str): Language for stopwords and stemming\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (topic_model, df_with_topics)\n",
    "            - topic_model: Trained BERTopic model\n",
    "            - df_with_topics: Input dataframe with topic assignments\n",
    "    \"\"\"\n",
    "    print(f\"üß† Training BERTopic model on {len(df)} segments...\")\n",
    "    \n",
    "    # Initialize BERTopic model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,  # We will use pre-computed embeddings\n",
    "        min_topic_size=min_topic_size,\n",
    "        nr_topics=nr_topics,\n",
    "        language=language,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    topics, _ = topic_model.fit_transform(df[text_column], df[embeddings_column])\n",
    "    \n",
    "    # Add topic assignments to dataframe\n",
    "    df['Topic'] = topics\n",
    "    \n",
    "    print(f\"‚úì Discovered {len(topic_model.get_topic_info())} topics\")\n",
    "    return topic_model, df\n",
    "\n",
    "def fine_tune_bertopic_model(topic_model, df, text_column='Combined_Text', embeddings_column='Segment_Embeddings'):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained BERTopic model on new data.\n",
    "    \n",
    "    Args:\n",
    "        topic_model (BERTopic): Pre-trained BERTopic model\n",
    "        df (DataFrame): New data for fine-tuning\n",
    "        text_column (str): Column name for the text data\n",
    "        embeddings_column (str): Column name for the embeddings\n",
    "    \"\"\"\n",
    "    print(f\"üîß Fine-tuning BERTopic model on {len(df)} segments...\")\n",
    "    \n",
    "    # Get current topics\n",
    "    current_topics = topic_model.get_topic_info()\n",
    "    current_topic_count = len(current_topics)\n",
    "    \n",
    "    # Fit the model on new data\n",
    "    topic_model.partial_fit(df[text_column], df[embeddings_column])\n",
    "    \n",
    "    # Show updated topic info\n",
    "    updated_topics = topic_model.get_topic_info()\n",
    "    updated_topic_count = len(updated_topics)\n",
    "    \n",
    "    print(f\"‚úì Fine-tuned model: {current_topic_count} -> {updated_topic_count} topics\")\n",
    "    return topic_model\n",
    "\n",
    "print(\"‚úì Topic modeling functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16093f54",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline (Colab Optimized)\n",
    "\n",
    "This is the complete dual-embedding pipeline optimized for Google Colab with GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e331c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAIN PROCESSING PIPELINE (COLAB OPTIMIZED) ===\n",
    "\n",
    "def run_dual_embedding_topic_pipeline(df, save_intermediate=True, data_folder='/content/drive/MyDrive/thesis_data/'):\n",
    "    \"\"\"\n",
    "    Run the complete dual-embedding topic modeling pipeline optimized for Colab.\n",
    "    \n",
    "    DUAL EMBEDDING STRATEGY:\n",
    "    1. First embedding: Individual speeches using RAW TEXT (for segmentation)\n",
    "    2. Second embedding: Aggregated segments using RAW TEXT (for topic modeling)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Verify required columns (flexible column checking)\n",
    "    required_cols = ['Text', 'Sitting_ID']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Try to fix common column name issues\n",
    "        if 'Text_ID' in df.columns and 'Sitting_ID' not in df.columns:\n",
    "            df['Sitting_ID'] = df['Text_ID']\n",
    "            print(\"üîß Fixed: Created Sitting_ID from Text_ID\")\n",
    "        if 'ID' in df.columns and 'Speaker_ID' not in df.columns:\n",
    "            df['Speaker_ID'] = df['ID']\n",
    "            print(\"üîß Fixed: Created Speaker_ID from ID\")\n",
    "        \n",
    "        # Check again\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Still missing required columns after fixes: {missing_cols}\")\n",
    "    \n",
    "    print(\"üöÄ DUAL EMBEDDING PIPELINE USING RAW TEXT (COLAB OPTIMIZED)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìä Input data shape: {df.shape}\")\n",
    "    print(f\"üîß Using RAW text for both embeddings (better semantic quality)\")\n",
    "    print(f\"üíª Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    \n",
    "    ### STEP 1: SPEECH-LEVEL EMBEDDINGS (FIRST EMBEDDING)\n",
    "    print(\"\\nüîÑ Generating speech-level embeddings (first embedding)...\")\n",
    "    df_with_speech_embeddings = generate_speech_embeddings_for_segmentation(df, text_column='Text')\n",
    "    \n",
    "    ### STEP 2: SEGMENTATION BASED ON SIMILARITY\n",
    "    print(\"\\nüîç Segmenting speeches by similarity...\")\n",
    "    df_segmented = segment_speeches_by_similarity(df_with_speech_embeddings, window_size=3, \n",
    "                                                  height_threshold=0.25, prominence_threshold=0.15, \n",
    "                                                  distance_threshold=5)\n",
    "    \n",
    "    ### STEP 3: AGGREGATE SEGMENTS AND RE-EMBEDDING (SECOND EMBEDDING)\n",
    "    print(\"\\nüì¶ Aggregating segments and re-embedding...\")\n",
    "    aggregated_segments = aggregate_segments(df_segmented, text_column='Text', segment_id_column='Segment_ID')\n",
    "    df_with_segment_embeddings = re_embed_segments(aggregated_segments, text_column='Combined_Text')\n",
    "    \n",
    "    ### STEP 4: TOPIC MODELING WITH BERTOPIC\n",
    "    print(\"\\nüß† Running topic modeling with BERTopic...\")\n",
    "    topic_model, df_with_topics = train_bertopic_model(df_with_segment_embeddings, \n",
    "                                                       text_column='Combined_Text', \n",
    "                                                       embeddings_column='Segment_Embeddings', \n",
    "                                                       min_topic_size=10, nr_topics='auto', language='english')\n",
    "    \n",
    "    ### OPTIONAL: FINE-TUNE MODEL (UNCOMMENT TO USE)\n",
    "    # print(\"\\nüîß Fine-tuning BERTopic model...\")\n",
    "    # topic_model = fine_tune_bertopic_model(topic_model, df_with_segment_embeddings, \n",
    "    #                                         text_column='Combined_Text', \n",
    "    #                                         embeddings_column='Segment_Embeddings')\n",
    "    \n",
    "    ### SAVE INTERMEDIATE RESULTS\n",
    "    if save_intermediate:\n",
    "        print(f\"\\nüíæ Saving intermediate results...\")\n",
    "        df_with_topics.to_pickle(f'{data_folder}AT_with_topics_final_colab.pkl')\n",
    "        topic_model.save(f'{data_folder}bertopic_model_colab')\n",
    "        print(f\"‚úì Saved: AT_with_topics_final_colab.pkl, bertopic_model_colab\")\n",
    "    \n",
    "    # Prepare results summary\n",
    "    results['df_with_topics'] = df_with_topics\n",
    "    results['segments_df'] = df_with_segment_embeddings\n",
    "    results['topic_model'] = topic_model\n",
    "    results['topic_info'] = topic_model.get_topic_info()\n",
    "    \n",
    "    # Assign LLM-generated names to topics (if available)\n",
    "    if 'Topic' in df_with_topics.columns and df_with_topics['Topic'].nunique() < 100:\n",
    "        print(f\"üè∑Ô∏è  Assigning LLM-generated names to topics...\")\n",
    "        topic_names = df_with_topics.groupby('Topic').first().reset_index()\n",
    "        topic_names = topic_names[['Topic', 'LLM_Name']]\n",
    "        results['topic_info_with_names'] = topic_names\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Topic naming skipped (too many topics or column missing)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Main processing pipeline loaded and ready for Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f92a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN THE PIPELINE ===\n",
    "print(\"üöÄ Starting topic modeling pipeline on Colab...\")\n",
    "print(f\"üíª Using: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"üìä Processing {len(AT_processed_df)} speeches\")\n",
    "\n",
    "try:\n",
    "    # Run the complete pipeline\n",
    "    results = run_dual_embedding_topic_pipeline(\n",
    "        AT_processed_df, \n",
    "        save_intermediate=True, \n",
    "        data_folder=data_folder  # Use the data_folder variable set earlier\n",
    "    )\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"\\nüéâ Pipeline completed successfully!\")\n",
    "        print(\"üìÅ Results saved to Google Drive\")\n",
    "        \n",
    "        # Quick results summary\n",
    "        print(\"\\nüìã QUICK RESULTS SUMMARY:\")\n",
    "        print(f\"‚úì Speeches processed: {len(results['df_with_topics'])}\")\n",
    "        print(f\"‚úì Segments created: {len(results['segments_df'])}\")\n",
    "        print(f\"‚úì Topics discovered: {len(results['topic_info'])}\")\n",
    "        \n",
    "        # Show topic names\n",
    "        print(f\"\\nüè∑Ô∏è  DISCOVERED TOPICS:\")\n",
    "        for _, row in results['topic_info_with_names'].iterrows():\n",
    "            if row['Topic'] != -1:\n",
    "                print(f\"  Topic {row['Topic']}: {row['LLM_Name']} ({row['Count']} segments)\")\n",
    "    else:\n",
    "        print(\"‚ùå Pipeline failed!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in pipeline: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b949b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DOWNLOAD RESULTS ===\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Define file paths\n",
    "data_folder = '/content/drive/MyDrive/thesis_data/'\n",
    "results_files = [\n",
    "    f\"{data_folder}AT_for_topic_modeling.pkl\",\n",
    "    f\"{data_folder}AT_with_topics_final_colab.pkl\",\n",
    "    f\"{data_folder}topic_info_with_names_colab.csv\",\n",
    "    f\"{data_folder}AT_with_speech_embeddings_colab.pkl\",\n",
    "    f\"{data_folder}AT_segments_with_embeddings_colab.pkl\",\n",
    "    f\"{data_folder}bertopic_model_colab\"\n",
    "]\n",
    "\n",
    "# Zip the results folder\n",
    "shutil.make_archive('/content/results_backup', 'zip', data_folder)\n",
    "\n",
    "print(\"üì¶ Results archived. Downloading...\")\n",
    "files.download('/content/results_backup.zip')\n",
    "\n",
    "print(\"‚úì Download complete! Check your files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c63f3",
   "metadata": {},
   "source": [
    "## Download Results from Colab\n",
    "\n",
    "After the pipeline completes successfully, you can download the results to your local machine:\n",
    "\n",
    "### Option 1: Direct Download from Google Drive\n",
    "1. Navigate to your Google Drive folder\n",
    "2. Download the generated pickle and CSV files\n",
    "3. Use them in your local analysis notebooks\n",
    "\n",
    "### Option 2: Download via Colab\n",
    "Run the cell below to download key results directly from Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DOWNLOAD RESULTS ===\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Define file paths\n",
    "data_folder = '/content/drive/MyDrive/thesis_data/'\n",
    "results_files = [\n",
    "    f\"{data_folder}AT_for_topic_modeling.pkl\",\n",
    "    f\"{data_folder}AT_with_topics_final_colab.pkl\",\n",
    "    f\"{data_folder}topic_info_with_names_colab.csv\",\n",
    "    f\"{data_folder}AT_with_speech_embeddings_colab.pkl\",\n",
    "    f\"{data_folder}AT_segments_with_embeddings_colab.pkl\",\n",
    "    f\"{data_folder}bertopic_model_colab\"\n",
    "]\n",
    "\n",
    "# Zip the results folder\n",
    "shutil.make_archive('/content/results_backup', 'zip', data_folder)\n",
    "\n",
    "print(\"üì¶ Results archived. Downloading...\")\n",
    "files.download('/content/results_backup.zip')\n",
    "\n",
    "print(\"‚úì Download complete! Check your files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b109bb1",
   "metadata": {},
   "source": [
    "## Usage Summary\n",
    "\n",
    "### What This Notebook Does:\n",
    "1. **Loads one file** from Google Drive: `AT_original_complete.pkl`\n",
    "2. **Processes the data** to create topic modeling version\n",
    "3. **Runs dual embedding** approach with GPU acceleration:\n",
    "   - Speech-level embeddings for segmentation\n",
    "   - Segment-level embeddings for topic modeling\n",
    "4. **Discovers topics** using BERTopic with clustering\n",
    "5. **Generates topic names** using LLM (optional)\n",
    "6. **Saves results** back to Google Drive\n",
    "\n",
    "### What You Need to Upload:\n",
    "- **Only one file**: `AT_original_complete.pkl`\n",
    "- **Upload location**: `MyDrive/thesis_data/AT_original_complete.pkl`\n",
    "\n",
    "### Performance on Colab GPU:\n",
    "- **Speech embeddings**: ~30-100 speeches/second\n",
    "- **Segment embeddings**: ~10-30 segments/second  \n",
    "- **Total time**: ~10-30 minutes for large datasets (vs hours on CPU)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download results** to your local machine\n",
    "2. **Use the final dataframe** for further analysis\n",
    "3. **Visualize topics** using the topic information\n",
    "4. **Integrate with LIWC** analysis using the speech-topic mappings\n",
    "\n",
    "### Files Generated:\n",
    "- `AT_for_topic_modeling.pkl` - Processed data used for topic modeling\n",
    "- `AT_with_topics_final_colab.pkl` - Final dataframe with topic assignments\n",
    "- `topic_info_with_names_colab.csv` - Topic information and LLM-generated names\n",
    "- `AT_with_speech_embeddings_colab.pkl` - Intermediate speech embeddings\n",
    "- `AT_segments_with_embeddings_colab.pkl` - Intermediate segment embeddings\n",
    "\n",
    "üéâ **Happy Topic Modeling on Colab!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
