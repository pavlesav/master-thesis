{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85c09fe",
   "metadata": {},
   "source": [
    "# Parliamentary Speech Segmentation Analysis\n",
    "\n",
    "This notebook analyzes parliamentary speech patterns and validates segmentation approaches for **Austrian Parliament** (English/German) and **Croatian Parliament** (English/Croatian) datasets and **British Parliament** (English only).\n",
    "\n",
    "## Key Features:\n",
    "- **Embedding Analysis**: Compare native vs English embeddings\n",
    "- **Enhanced Segmentation**: Multi-signal boundary detection with agenda awareness\n",
    "- **Quality Validation**: Within vs between segment similarity analysis\n",
    "- **Cross-Parliament & Cross-Language Comparison**: Unified analysis across different parliamentary systems and languages\n",
    "\n",
    "## Pipeline:\n",
    "1. Load data with pre-computed embeddings\n",
    "2. Analyze embedding similarities and text length relationships\n",
    "3. Apply sophisticated segmentation algorithm with native language support\n",
    "4. Validate segmentation quality\n",
    "5. Compare English vs native language segmentation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f24ec8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded Austrian data: (231759, 28)\n",
      "‚úÖ Loaded Croatian data: (504338, 28)\n",
      "‚úÖ Loaded British data: (670912, 27)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Load the data with embeddings\n",
    "AT_combined = pd.read_pickle(r\"data folder\\AT\\AT_combined_with_embeddings.pkl\")\n",
    "HR_combined = pd.read_pickle(r\"data folder\\HR\\HR_combined_with_embeddings.pkl\")\n",
    "GB = pd.read_pickle(r\"data folder\\GB\\GB_with_speech_embeddings.pkl\")\n",
    "\n",
    "print(f\"‚úÖ Loaded Austrian data: {AT_combined.shape}\")\n",
    "print(f\"‚úÖ Loaded Croatian data: {HR_combined.shape}\")\n",
    "print(f\"‚úÖ Loaded British data: {GB.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d6d688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ENHANCED MULTI-SIGNAL SEGMENTATION (INCLUDING NATIVE LANGUAGES)\n",
      "======================================================================\n",
      "‚öôÔ∏è Configuration: window_size=10, min_segment_size=15\n",
      "\n",
      "AUSTRIAN (English) PARLIAMENT\n",
      "----------------------------------------\n",
      "üîÑ Creating segments...\n",
      "üî§ Using English keywords for AUSTRIAN (English)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting AUSTRIAN (English): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1221/1221 [03:29<00:00,  5.83it/s]\n",
      "Segmenting AUSTRIAN (English): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1221/1221 [03:29<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Validating segmentation quality...\n",
      "üîç Validating segmentation quality across 1221 sessions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating segments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1221/1221 [02:09<00:00,  9.43session/s]\n",
      "Validating segments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1221/1221 [02:09<00:00,  9.43session/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quality validation complete: 243385 within-segment, 11308 between-segment comparisons\n",
      "üìä Segments: 12,529 (avg size: 18.5)\n",
      "üéØ Break types: 3,371 agenda + 7,937 multi-signal\n",
      "‚úÖ Quality score: 0.070\n",
      "üìà Within-segment similarities: 243,385\n",
      "üìâ Between-segment similarities: 11,308\n",
      "\n",
      "AUSTRIAN (German) PARLIAMENT\n",
      "----------------------------------------\n",
      "üîÑ Creating segments...\n",
      "üåç Using native keywords for AUSTRIAN (German): ['tagesordnung', 'tagesordnungspunkt', 'punkt']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting AUSTRIAN (German): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1221/1221 [04:55<00:00,  4.14it/s]\n",
      "Segmenting AUSTRIAN (German): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1221/1221 [04:55<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Validating segmentation quality...\n",
      "üîç Validating segmentation quality across 1221 sessions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating segments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1221/1221 [02:04<00:00,  9.81session/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quality validation complete: 233965 within-segment, 10837 between-segment comparisons\n",
      "üìä Segments: 12,058 (avg size: 19.2)\n",
      "üéØ Break types: 2,097 agenda + 8,740 multi-signal\n",
      "‚úÖ Quality score: 0.076\n",
      "üìà Within-segment similarities: 233,965\n",
      "üìâ Between-segment similarities: 10,837\n",
      "\n",
      "CROATIAN (English) PARLIAMENT\n",
      "----------------------------------------\n",
      "üîÑ Creating segments...\n",
      "üî§ Using English keywords for CROATIAN (English)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting CROATIAN (English): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1708/1708 [06:23<00:00,  4.45it/s]\n",
      "Segmenting CROATIAN (English): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1708/1708 [06:23<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Validating segmentation quality...\n",
      "üîç Validating segmentation quality across 1708 sessions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating segments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1708/1708 [04:05<00:00,  6.95session/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quality validation complete: 502070 within-segment, 23407 between-segment comparisons\n",
      "üìä Segments: 25,115 (avg size: 20.1)\n",
      "üéØ Break types: 1,269 agenda + 22,138 multi-signal\n",
      "‚úÖ Quality score: 0.096\n",
      "üìà Within-segment similarities: 502,070\n",
      "üìâ Between-segment similarities: 23,407\n",
      "\n",
      "CROATIAN (Croatian) PARLIAMENT\n",
      "----------------------------------------\n",
      "üîÑ Creating segments...\n",
      "üåç Using native keywords for CROATIAN (Croatian): ['dnevni', 'red', 'toƒçka']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting CROATIAN (Croatian): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1708/1708 [07:37<00:00,  3.73it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Validating segmentation quality...\n",
      "üîç Validating segmentation quality across 1708 sessions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating segments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1708/1708 [06:43<00:00,  4.24session/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quality validation complete: 504310 within-segment, 23519 between-segment comparisons\n",
      "üìä Segments: 25,227 (avg size: 20.0)\n",
      "üéØ Break types: 2,812 agenda + 20,707 multi-signal\n",
      "‚úÖ Quality score: 0.087\n",
      "üìà Within-segment similarities: 504,310\n",
      "üìâ Between-segment similarities: 23,519\n",
      "\n",
      "BRITISH (English) PARLIAMENT\n",
      "----------------------------------------\n",
      "üîÑ Creating segments...\n",
      "üî§ Using English keywords for BRITISH (English)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting BRITISH (English): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2209/2209 [14:59<00:00,  2.45it/s]\n",
      "Segmenting BRITISH (English): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2209/2209 [14:59<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Validating segmentation quality...\n",
      "üîç Validating segmentation quality across 2209 sessions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating segments: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2209/2209 [10:08<00:00,  3.63session/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quality validation complete: 667418 within-segment, 31172 between-segment comparisons\n",
      "üìä Segments: 33,381 (avg size: 20.1)\n",
      "üéØ Break types: 120 agenda + 31,052 multi-signal\n",
      "‚úÖ Quality score: 0.024\n",
      "üìà Within-segment similarities: 667,418\n",
      "üìâ Between-segment similarities: 31,172\n",
      "\n",
      "üèÜ SEGMENTATION SUMMARY (ALL LANGUAGES)\n",
      "==================================================\n",
      "AUSTRIAN (English): 12,529 segments, 29.8% agenda-driven\n",
      "AUSTRIAN (German): 12,058 segments, 19.4% agenda-driven\n",
      "CROATIAN (English): 25,115 segments, 5.4% agenda-driven\n",
      "CROATIAN (Croatian): 25,227 segments, 12.0% agenda-driven\n",
      "BRITISH (English): 33,381 segments, 0.4% agenda-driven\n",
      "AUSTRIAN (English): 12,529 segments, 29.8% agenda-driven\n",
      "AUSTRIAN (German): 12,058 segments, 19.4% agenda-driven\n",
      "CROATIAN (English): 25,115 segments, 5.4% agenda-driven\n",
      "CROATIAN (Croatian): 25,227 segments, 12.0% agenda-driven\n",
      "BRITISH (English): 33,381 segments, 0.4% agenda-driven\n"
     ]
    }
   ],
   "source": [
    "# === ENHANCED MULTI-SIGNAL SEGMENTATION WITH NATIVE LANGUAGE SUPPORT ===\n",
    "\n",
    "# Native language agenda keywords for better segmentation\n",
    "native_agenda_keywords = {\n",
    "    \"AUSTRIAN (German)\": [\n",
    "        'tagesordnung', 'tagesordnungspunkt', 'punkt', 'verhandlung', \n",
    "        'behandlung', 'n√§chster', 'weiter', 'fortsetzen'\n",
    "    ],\n",
    "    \"CROATIAN (Croatian)\": [\n",
    "        'dnevni', 'red', 'toƒçka', 'taƒçka', 'sljedeƒái', 'sljedeƒáe',\n",
    "        'prijedlog', 'zakon', 'tema', 'nastavljamo', 'prelazimo',\n",
    "    ]\n",
    "}\n",
    "\n",
    "def create_enhanced_segments_native(dataset, embedding_col='Speech_Embeddings_english', \n",
    "                                  text_col='Text', parliament_name=\"\", \n",
    "                                  text_id_col='Text_ID', window_size=5, min_segment_size=10):\n",
    "    \"\"\"Enhanced segmentation with native language support\"\"\"\n",
    "    \n",
    "    # Use native keywords if available, otherwise default English keywords\n",
    "    if parliament_name in native_agenda_keywords:\n",
    "        agenda_keywords = native_agenda_keywords[parliament_name]\n",
    "        print(f\"üåç Using native keywords for {parliament_name}: {agenda_keywords[:3]}...\")\n",
    "    else:\n",
    "        agenda_keywords = [\n",
    "            'agenda', 'proceed', 'point', 'item',\n",
    "            'topic', 'next', 'following', 'move on'\n",
    "        ]\n",
    "        print(f\"üî§ Using English keywords for {parliament_name}\")\n",
    "    \n",
    "    def boundary_in_signal(boundary, signal_name, similarity_signals, window_size):\n",
    "        \"\"\"Check if boundary corresponds to a signal\"\"\"\n",
    "        signal = similarity_signals.get(signal_name, [])\n",
    "        if len(signal) == 0:\n",
    "            return False\n",
    "        \n",
    "        if signal_name == 'windowed':\n",
    "            return boundary - window_size >= 0 and boundary - window_size < len(signal)\n",
    "        else:\n",
    "            return boundary - 1 >= 0 and boundary - 1 < len(signal)\n",
    "    \n",
    "    all_segments = []\n",
    "    segment_stats = {\n",
    "        'total_segments': 0, 'agenda_breaks': 0, 'multi_signal_breaks': 0,\n",
    "        'similarity_breaks': 0, 'chairperson_breaks': 0\n",
    "    }\n",
    "    \n",
    "    unique_text_ids = dataset[text_id_col].unique()\n",
    "    \n",
    "    for text_id in tqdm(unique_text_ids, desc=f\"Segmenting {parliament_name}\"):\n",
    "        text_data = dataset[dataset[text_id_col] == text_id].reset_index(drop=True)\n",
    "        sitting_length = len(text_data)\n",
    "        \n",
    "        if sitting_length < min_segment_size:\n",
    "            segment = {\n",
    "                'Text_ID': text_id, 'Segment_ID': f\"{text_id}_seg_1\",\n",
    "                'Start_Index': 0, 'End_Index': sitting_length - 1,\n",
    "                'Speech_Count': sitting_length, 'Break_Type': 'single_speech'\n",
    "            }\n",
    "            all_segments.append(segment)\n",
    "            segment_stats['total_segments'] += 1\n",
    "            continue\n",
    "\n",
    "        embeddings = np.array(text_data[embedding_col].tolist())\n",
    "\n",
    "        # === NATIVE LANGUAGE AGENDA DETECTION ===\n",
    "        agenda_boundaries = set()\n",
    "        agenda_signals = []\n",
    "\n",
    "        for i, (_, row) in enumerate(text_data.iterrows()):\n",
    "            agenda_score = 0\n",
    "            \n",
    "            if 'Speaker_role' in row and pd.notna(row['Speaker_role']):\n",
    "                if 'Chairperson' in str(row['Speaker_role']):\n",
    "                    text = str(row[text_col]).lower() if pd.notna(row[text_col]) else \"\"\n",
    "                    \n",
    "                    # Count how many agenda keywords are present\n",
    "                    keyword_count = sum(1 for keyword in agenda_keywords if keyword in text)\n",
    "                    \n",
    "                    # Score based on keyword count (more keywords = higher score)\n",
    "                    if keyword_count > 0:\n",
    "                        agenda_score = min(1.0, keyword_count * 0.3)  # Cap at 1.0, each keyword adds 0.3\n",
    "                    \n",
    "                    # Special handling for session start\n",
    "                    if agenda_score == 0 and i == 0:\n",
    "                        agenda_score = 0.3\n",
    "            \n",
    "            agenda_signals.append(agenda_score)\n",
    "            \n",
    "            # Add as agenda boundary if score is significant enough\n",
    "            if (agenda_score >= 0.6 and i >= min_segment_size and \n",
    "                (sitting_length - i) >= min_segment_size):\n",
    "                agenda_boundaries.add(i)\n",
    "\n",
    "        # === SIMILARITY ANALYSIS ===\n",
    "        similarity_signals = {}\n",
    "        \n",
    "        # Windowed similarity\n",
    "        if len(embeddings) > window_size * 2:\n",
    "            similarities = []\n",
    "            for i in range(len(embeddings) - window_size):\n",
    "                if i + 2 * window_size <= len(embeddings):\n",
    "                    window1 = np.mean(embeddings[i:i + window_size], axis=0)\n",
    "                    window2 = np.mean(embeddings[i + window_size:i + 2*window_size], axis=0)\n",
    "                    sim = cosine_similarity(window1.reshape(1, -1), window2.reshape(1, -1))[0][0]\n",
    "                    similarities.append(sim)\n",
    "            similarity_signals['windowed'] = np.array(similarities)\n",
    "        \n",
    "        # Point-to-point similarity\n",
    "        if len(embeddings) > 6:\n",
    "            point_sims = []\n",
    "            for i in range(len(embeddings) - 1):\n",
    "                sim = cosine_similarity(\n",
    "                    embeddings[i].reshape(1, -1),\n",
    "                    embeddings[i + 1].reshape(1, -1)\n",
    "                )[0][0]\n",
    "                point_sims.append(sim)\n",
    "            similarity_signals['point_to_point'] = np.array(point_sims)\n",
    "\n",
    "        if not similarity_signals:\n",
    "            segment = {\n",
    "                'Text_ID': text_id, 'Segment_ID': f\"{text_id}_seg_1\",\n",
    "                'Start_Index': 0, 'End_Index': sitting_length - 1,\n",
    "                'Speech_Count': sitting_length, 'Break_Type': 'no_signals'\n",
    "            }\n",
    "            all_segments.append(segment)\n",
    "            segment_stats['total_segments'] += 1\n",
    "            continue\n",
    "\n",
    "        # === BOUNDARY DETECTION (Natural Thresholds) ===\n",
    "        candidate_boundaries = set(agenda_boundaries)\n",
    "        \n",
    "        for signal_name, signal in similarity_signals.items():\n",
    "            if len(signal) > 0:\n",
    "                signal_mean = np.mean(signal)\n",
    "                signal_std = np.std(signal)\n",
    "                threshold = signal_mean - signal_std\n",
    "                \n",
    "                if signal_name == 'windowed':\n",
    "                    offset = window_size\n",
    "                else:\n",
    "                    offset = 1\n",
    "                \n",
    "                for i in range(len(signal)):\n",
    "                    boundary_pos = i + offset\n",
    "                    \n",
    "                    if (signal[i] < threshold and \n",
    "                        boundary_pos >= min_segment_size and \n",
    "                        (sitting_length - boundary_pos) >= min_segment_size):\n",
    "                        candidate_boundaries.add(boundary_pos)\n",
    "\n",
    "        candidates = sorted(list(candidate_boundaries))\n",
    "\n",
    "        # === BOUNDARY SELECTION (Quality-Based) ===\n",
    "        if candidates:\n",
    "            candidate_scores = []\n",
    "            for c in candidates:\n",
    "                score = 0\n",
    "                \n",
    "                if c < len(agenda_signals):\n",
    "                    score += agenda_signals[c] * 5.0\n",
    "                \n",
    "                for signal_name, signal in similarity_signals.items():\n",
    "                    signal_mean = np.mean(signal)\n",
    "                    \n",
    "                    if signal_name == 'windowed' and c - window_size >= 0 and c - window_size < len(signal):\n",
    "                        deviation = signal_mean - signal[c - window_size]\n",
    "                        score += max(0, deviation) * 5.0\n",
    "                    elif signal_name == 'point_to_point' and c - 1 >= 0 and c - 1 < len(signal):\n",
    "                        deviation = signal_mean - signal[c - 1]\n",
    "                        score += max(0, deviation) * 3.0\n",
    "                \n",
    "                candidate_scores.append((c, score))\n",
    "            \n",
    "            boundaries = sorted([c for c, score in candidate_scores if score > 0])\n",
    "        else:\n",
    "            boundaries = []\n",
    "\n",
    "        # Validate boundaries (ensure minimum spacing)\n",
    "        validated_boundaries = []\n",
    "        for boundary in boundaries:\n",
    "            if not validated_boundaries or (boundary - validated_boundaries[-1]) >= min_segment_size:\n",
    "                validated_boundaries.append(boundary)\n",
    "\n",
    "        # Create segments\n",
    "        segment_breaks = [0] + validated_boundaries + [sitting_length]\n",
    "        segment_breaks = sorted(list(set(segment_breaks)))\n",
    "        \n",
    "        for seg_idx in range(len(segment_breaks) - 1):\n",
    "            start_idx = segment_breaks[seg_idx]\n",
    "            end_idx = segment_breaks[seg_idx + 1] - 1\n",
    "            \n",
    "            break_type = 'enhanced'\n",
    "            if seg_idx < len(validated_boundaries):\n",
    "                boundary = validated_boundaries[seg_idx]\n",
    "                if boundary in agenda_boundaries:\n",
    "                    break_type = 'agenda'\n",
    "                    segment_stats['agenda_breaks'] += 1\n",
    "                elif sum(boundary_in_signal(boundary, s, similarity_signals, window_size) \n",
    "                        for s in similarity_signals.keys()) > 1:\n",
    "                    break_type = 'multi_signal'\n",
    "                    segment_stats['multi_signal_breaks'] += 1\n",
    "                else:\n",
    "                    break_type = 'similarity'\n",
    "                    segment_stats['similarity_breaks'] += 1\n",
    "            \n",
    "            segment = {\n",
    "                'Text_ID': text_id, 'Segment_ID': f\"{text_id}_seg_{seg_idx + 1}\",\n",
    "                'Start_Index': start_idx, 'End_Index': end_idx,\n",
    "                'Speech_Count': end_idx - start_idx + 1, 'Break_Type': break_type\n",
    "            }\n",
    "            all_segments.append(segment)\n",
    "            segment_stats['total_segments'] += 1\n",
    "\n",
    "    return all_segments, segment_stats\n",
    "\n",
    "def validate_segmentation_quality(dataset, segments, embedding_col):\n",
    "    \"\"\"Validate segmentation by comparing within vs between segment similarities with progress tracking\"\"\"\n",
    "    within_similarities = []\n",
    "    between_similarities = []\n",
    "    \n",
    "    text_id_segments = {}\n",
    "    for segment in segments:\n",
    "        text_id = segment['Text_ID']\n",
    "        if text_id not in text_id_segments:\n",
    "            text_id_segments[text_id] = []\n",
    "        text_id_segments[text_id].append(segment)\n",
    "    \n",
    "    print(f\"üîç Validating segmentation quality across {len(text_id_segments)} sessions...\")\n",
    "    \n",
    "    for text_id in tqdm(text_id_segments.keys(), desc=\"Validating segments\", unit=\"session\"):\n",
    "        text_segments = text_id_segments[text_id]\n",
    "        text_data = dataset[dataset['Text_ID'] == text_id].reset_index(drop=True)\n",
    "        \n",
    "        # Within-segment similarities (sample to avoid explosion)\n",
    "        for segment in text_segments:\n",
    "            start_idx, end_idx = segment['Start_Index'], segment['End_Index']\n",
    "            \n",
    "            if end_idx > start_idx:\n",
    "                max_comparisons = 20  # Limit per segment\n",
    "                comparisons_made = 0\n",
    "                \n",
    "                for i in range(start_idx, min(end_idx + 1, len(text_data))):\n",
    "                    for j in range(i + 1, min(end_idx + 1, len(text_data))):\n",
    "                        if comparisons_made >= max_comparisons:\n",
    "                            break\n",
    "                        \n",
    "                        if i < len(text_data) and j < len(text_data):\n",
    "                            emb1, emb2 = text_data.iloc[i][embedding_col], text_data.iloc[j][embedding_col]\n",
    "                            \n",
    "                            if emb1 is not None and emb2 is not None:\n",
    "                                try:\n",
    "                                    sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "                                    within_similarities.append(sim)\n",
    "                                    comparisons_made += 1\n",
    "                                except:\n",
    "                                    continue\n",
    "                    if comparisons_made >= max_comparisons:\n",
    "                        break\n",
    "        \n",
    "        # Between-segment similarities\n",
    "        for i in range(len(text_segments) - 1):\n",
    "            seg1, seg2 = text_segments[i], text_segments[i + 1]\n",
    "            idx1, idx2 = seg1['End_Index'], seg2['Start_Index']\n",
    "            \n",
    "            if idx1 < len(text_data) and idx2 < len(text_data):\n",
    "                emb1, emb2 = text_data.iloc[idx1][embedding_col], text_data.iloc[idx2][embedding_col]\n",
    "                \n",
    "                if emb1 is not None and emb2 is not None:\n",
    "                    try:\n",
    "                        sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "                        between_similarities.append(sim)\n",
    "                    except:\n",
    "                        continue\n",
    "    \n",
    "    print(f\"‚úÖ Quality validation complete: {len(within_similarities)} within-segment, {len(between_similarities)} between-segment comparisons\")\n",
    "    return within_similarities, between_similarities\n",
    "\n",
    "# === APPLY TO ALL PARLIAMENTS (INCLUDING NATIVE LANGUAGES) ===\n",
    "\n",
    "parliament_configs = {\n",
    "    \"AUSTRIAN (English)\": {\"dataset\": AT_combined, \"embedding_col\": \"Speech_Embeddings_english\", \"text_col\": \"Text\"},\n",
    "    \"AUSTRIAN (German)\": {\"dataset\": AT_combined, \"embedding_col\": \"Speech_Embeddings_native_language\", \"text_col\": \"Text_native_language\"},\n",
    "    \"CROATIAN (English)\": {\"dataset\": HR_combined, \"embedding_col\": \"Speech_Embeddings_english\", \"text_col\": \"Text\"},\n",
    "    \"CROATIAN (Croatian)\": {\"dataset\": HR_combined, \"embedding_col\": \"Speech_Embeddings_native_language\", \"text_col\": \"Text_native_language\"},\n",
    "    \"BRITISH (English)\": {\"dataset\": GB, \"embedding_col\": \"Speech_Embeddings\", \"text_col\": \"Text\"}\n",
    "}\n",
    "\n",
    "print(\"üîß ENHANCED MULTI-SIGNAL SEGMENTATION (INCLUDING NATIVE LANGUAGES)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# CONFIGURABLE PARAMETERS\n",
    "WINDOW_SIZE = 10          \n",
    "MIN_SEGMENT_SIZE = 15    \n",
    "\n",
    "print(f\"‚öôÔ∏è Configuration: window_size={WINDOW_SIZE}, min_segment_size={MIN_SEGMENT_SIZE}\")\n",
    "\n",
    "results = {}\n",
    "for parliament_name, config in parliament_configs.items():\n",
    "    print(f\"\\n{parliament_name} PARLIAMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    dataset = config[\"dataset\"]\n",
    "    embedding_col = config[\"embedding_col\"]\n",
    "    text_col = config[\"text_col\"]\n",
    "    \n",
    "    # Check if the required columns exist\n",
    "    if embedding_col not in dataset.columns:\n",
    "        print(f\"‚ùå Embedding column '{embedding_col}' not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    if text_col not in dataset.columns:\n",
    "        print(f\"‚ùå Text column '{text_col}' not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Create segments with configurable parameters\n",
    "    print(\"üîÑ Creating segments...\")\n",
    "    segments, stats = create_enhanced_segments_native(\n",
    "        dataset, \n",
    "        embedding_col,\n",
    "        text_col,\n",
    "        parliament_name,\n",
    "        window_size=WINDOW_SIZE, \n",
    "        min_segment_size=MIN_SEGMENT_SIZE\n",
    "    )\n",
    "    \n",
    "    # Validate quality with progress tracking\n",
    "    print(\"üîÑ Validating segmentation quality...\")\n",
    "    within_sim, between_sim = validate_segmentation_quality(dataset, segments, embedding_col)\n",
    "    \n",
    "    avg_segment_size = len(dataset) / stats['total_segments']\n",
    "    quality_score = np.mean(within_sim) - np.mean(between_sim) if within_sim and between_sim else 0\n",
    "    \n",
    "    results[parliament_name] = {\n",
    "        'segments': segments, 'stats': stats, 'avg_size': avg_segment_size,\n",
    "        'quality_score': quality_score, 'within_sim': within_sim, 'between_sim': between_sim\n",
    "    }\n",
    "    \n",
    "    print(f\"üìä Segments: {stats['total_segments']:,} (avg size: {avg_segment_size:.1f})\")\n",
    "    print(f\"üéØ Break types: {stats['agenda_breaks']:,} agenda + {stats['multi_signal_breaks']:,} multi-signal\")\n",
    "    if quality_score > 0:\n",
    "        print(f\"‚úÖ Quality score: {quality_score:.3f}\")\n",
    "    print(f\"üìà Within-segment similarities: {len(within_sim):,}\")\n",
    "    print(f\"üìâ Between-segment similarities: {len(between_sim):,}\")\n",
    "\n",
    "print(f\"\\nüèÜ SEGMENTATION SUMMARY (ALL LANGUAGES)\")\n",
    "print(\"=\" * 50)\n",
    "for name, result in results.items():\n",
    "    total_breaks = result['stats']['agenda_breaks'] + result['stats']['multi_signal_breaks']\n",
    "    agenda_pct = result['stats']['agenda_breaks'] / max(total_breaks, 1) * 100\n",
    "    print(f\"{name}: {result['stats']['total_segments']:,} segments, {agenda_pct:.1f}% agenda-driven\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2e4783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç ENGLISH vs NATIVE LANGUAGE SEGMENTATION COMPARISON\n",
      "============================================================\n",
      "\n",
      "üìä AUSTRIAN PARLIAMENT COMPARISON\n",
      "----------------------------------------\n",
      "üî¢ Segment Count:\n",
      "  ‚Ä¢ English: 12,529 segments\n",
      "  ‚Ä¢ German: 12,058 segments\n",
      "  ‚Ä¢ Difference: 471 segments\n",
      "  ‚Ä¢ Similarity: 0.962 (1.0 = identical)\n",
      "\n",
      "‚ö° Quality Score Comparison:\n",
      "  ‚Ä¢ English: 0.070\n",
      "  ‚Ä¢ German: 0.076\n",
      "  ‚Ä¢ Difference: 0.006 (Native better)\n",
      "\n",
      "üìè Average Segment Size:\n",
      "  ‚Ä¢ English: 18.5 speeches/segment\n",
      "  ‚Ä¢ German: 19.2 speeches/segment\n",
      "  ‚Ä¢ Difference: 0.7 speeches\n",
      "\n",
      "üéØ Agenda Boundaries Detected:\n",
      "  ‚Ä¢ English: 3,371 agenda breaks\n",
      "  ‚Ä¢ German: 2,097 agenda breaks\n",
      "  ‚Ä¢ Difference: -1,274 (English found more)\n",
      "\n",
      "üí° Overall Assessment:\n",
      "  ‚Ä¢ Very similar segmentation between English and German\n",
      "\n",
      "üìä CROATIAN PARLIAMENT COMPARISON\n",
      "----------------------------------------\n",
      "üî¢ Segment Count:\n",
      "  ‚Ä¢ English: 25,115 segments\n",
      "  ‚Ä¢ Croatian: 25,227 segments\n",
      "  ‚Ä¢ Difference: 112 segments\n",
      "  ‚Ä¢ Similarity: 0.996 (1.0 = identical)\n",
      "\n",
      "‚ö° Quality Score Comparison:\n",
      "  ‚Ä¢ English: 0.096\n",
      "  ‚Ä¢ Croatian: 0.087\n",
      "  ‚Ä¢ Difference: -0.009 (English better)\n",
      "\n",
      "üìè Average Segment Size:\n",
      "  ‚Ä¢ English: 20.1 speeches/segment\n",
      "  ‚Ä¢ Croatian: 20.0 speeches/segment\n",
      "  ‚Ä¢ Difference: -0.1 speeches\n",
      "\n",
      "üéØ Agenda Boundaries Detected:\n",
      "  ‚Ä¢ English: 1,269 agenda breaks\n",
      "  ‚Ä¢ Croatian: 2,812 agenda breaks\n",
      "  ‚Ä¢ Difference: 1,543 (Native found more)\n",
      "\n",
      "üí° Overall Assessment:\n",
      "  ‚Ä¢ Very similar segmentation between English and Croatian\n",
      "\n",
      "üìä CROSS-LANGUAGE INSIGHTS:\n",
      "========================================\n",
      "üèÜ Segmentation Quality Ranking:\n",
      "  1. CROATIAN (English): 0.096\n",
      "  2. CROATIAN (Croatian): 0.087\n",
      "  3. AUSTRIAN (German): 0.076\n",
      "  4. AUSTRIAN (English): 0.070\n",
      "  5. BRITISH (English): 0.024\n",
      "\n",
      "üîç Language Performance Summary:\n",
      "  ‚Ä¢ Native language better: 0/2 parliaments\n",
      "  ‚Ä¢ English better: 0/2 parliaments\n",
      "  ‚Ä¢ Similar performance: 2/2 parliaments\n",
      "\n",
      "üéØ KEY FINDINGS:\n",
      "‚Ä¢ Cross-language embeddings preserve segmentation structure well\n",
      "‚Ä¢ Native language keywords may provide marginal improvements\n",
      "‚Ä¢ Embedding quality is more important than language-specific optimization\n",
      "‚Ä¢ Multi-signal approach works consistently across languages\n"
     ]
    }
   ],
   "source": [
    "# === LANGUAGE COMPARISON ANALYSIS ===\n",
    "\n",
    "print(\"üåç ENGLISH vs NATIVE LANGUAGE SEGMENTATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare segmentation performance between English and native languages\n",
    "comparison_pairs = [\n",
    "    (\"AUSTRIAN (English)\", \"AUSTRIAN (German)\"),\n",
    "    (\"CROATIAN (English)\", \"CROATIAN (Croatian)\")\n",
    "]\n",
    "\n",
    "for english_key, native_key in comparison_pairs:\n",
    "    if english_key in results and native_key in results:\n",
    "        english_result = results[english_key]\n",
    "        native_result = results[native_key]\n",
    "        \n",
    "        parliament = english_key.split(\" \")[0]\n",
    "        native_lang = native_key.split(\"(\")[1].rstrip(\")\")\n",
    "        \n",
    "        print(f\"\\nüìä {parliament} PARLIAMENT COMPARISON\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Segment count comparison\n",
    "        en_segments = english_result['stats']['total_segments']\n",
    "        native_segments = native_result['stats']['total_segments']\n",
    "        segment_diff = abs(en_segments - native_segments)\n",
    "        segment_similarity = 1 - (segment_diff / max(en_segments, native_segments))\n",
    "        \n",
    "        print(f\"üî¢ Segment Count:\")\n",
    "        print(f\"  ‚Ä¢ English: {en_segments:,} segments\")\n",
    "        print(f\"  ‚Ä¢ {native_lang}: {native_segments:,} segments\")\n",
    "        print(f\"  ‚Ä¢ Difference: {segment_diff:,} segments\")\n",
    "        print(f\"  ‚Ä¢ Similarity: {segment_similarity:.3f} (1.0 = identical)\")\n",
    "        \n",
    "        # Quality score comparison\n",
    "        en_quality = english_result['quality_score']\n",
    "        native_quality = native_result['quality_score']\n",
    "        quality_diff = native_quality - en_quality\n",
    "        \n",
    "        print(f\"\\n‚ö° Quality Score Comparison:\")\n",
    "        print(f\"  ‚Ä¢ English: {en_quality:.3f}\")\n",
    "        print(f\"  ‚Ä¢ {native_lang}: {native_quality:.3f}\")\n",
    "        print(f\"  ‚Ä¢ Difference: {quality_diff:.3f} ({'Native better' if quality_diff > 0 else 'English better' if quality_diff < 0 else 'Equal'})\")\n",
    "        \n",
    "        # Average segment size comparison\n",
    "        en_avg_size = english_result['avg_size']\n",
    "        native_avg_size = native_result['avg_size']\n",
    "        size_diff = native_avg_size - en_avg_size\n",
    "        \n",
    "        print(f\"\\nüìè Average Segment Size:\")\n",
    "        print(f\"  ‚Ä¢ English: {en_avg_size:.1f} speeches/segment\")\n",
    "        print(f\"  ‚Ä¢ {native_lang}: {native_avg_size:.1f} speeches/segment\")\n",
    "        print(f\"  ‚Ä¢ Difference: {size_diff:.1f} speeches\")\n",
    "        \n",
    "        # Agenda boundaries comparison\n",
    "        en_agenda = english_result['stats']['agenda_breaks']\n",
    "        native_agenda = native_result['stats']['agenda_breaks']\n",
    "        agenda_diff = native_agenda - en_agenda\n",
    "        \n",
    "        print(f\"\\nüéØ Agenda Boundaries Detected:\")\n",
    "        print(f\"  ‚Ä¢ English: {en_agenda:,} agenda breaks\")\n",
    "        print(f\"  ‚Ä¢ {native_lang}: {native_agenda:,} agenda breaks\")\n",
    "        print(f\"  ‚Ä¢ Difference: {agenda_diff:,} ({'Native found more' if agenda_diff > 0 else 'English found more' if agenda_diff < 0 else 'Same'})\")\n",
    "        \n",
    "        # Overall assessment\n",
    "        print(f\"\\nüí° Overall Assessment:\")\n",
    "        if abs(quality_diff) < 0.01 and segment_similarity > 0.9:\n",
    "            print(f\"  ‚Ä¢ Very similar segmentation between English and {native_lang}\")\n",
    "        elif quality_diff > 0.02:\n",
    "            print(f\"  ‚Ä¢ {native_lang} shows notably better segmentation quality\")\n",
    "        elif quality_diff < -0.02:\n",
    "            print(f\"  ‚Ä¢ English shows notably better segmentation quality\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ Moderate differences between English and {native_lang}\")\n",
    "\n",
    "print(f\"\\nüìä CROSS-LANGUAGE INSIGHTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Extract quality scores for comparison\n",
    "language_quality = {}\n",
    "for name, result in results.items():\n",
    "    quality = result['quality_score']\n",
    "    language_quality[name] = quality\n",
    "\n",
    "# Sort by quality score\n",
    "sorted_quality = sorted(language_quality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"üèÜ Segmentation Quality Ranking:\")\n",
    "for i, (name, quality) in enumerate(sorted_quality, 1):\n",
    "    print(f\"  {i}. {name}: {quality:.3f}\")\n",
    "\n",
    "# Analyze native vs English performance\n",
    "native_better = 0\n",
    "english_better = 0\n",
    "similar = 0\n",
    "\n",
    "for english_key, native_key in comparison_pairs:\n",
    "    if english_key in results and native_key in results:\n",
    "        en_quality = results[english_key]['quality_score']\n",
    "        native_quality = results[native_key]['quality_score']\n",
    "        diff = native_quality - en_quality\n",
    "        \n",
    "        if diff > 0.01:\n",
    "            native_better += 1\n",
    "        elif diff < -0.01:\n",
    "            english_better += 1\n",
    "        else:\n",
    "            similar += 1\n",
    "\n",
    "print(f\"\\nüîç Language Performance Summary:\")\n",
    "print(f\"  ‚Ä¢ Native language better: {native_better}/2 parliaments\")\n",
    "print(f\"  ‚Ä¢ English better: {english_better}/2 parliaments\") \n",
    "print(f\"  ‚Ä¢ Similar performance: {similar}/2 parliaments\")\n",
    "\n",
    "print(f\"\\nüéØ KEY FINDINGS:\")\n",
    "print(\"‚Ä¢ Cross-language embeddings preserve segmentation structure well\")\n",
    "print(\"‚Ä¢ Native language keywords may provide marginal improvements\")\n",
    "print(\"‚Ä¢ Embedding quality is more important than language-specific optimization\")\n",
    "print(\"‚Ä¢ Multi-signal approach works consistently across languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb89c838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ADDING SEGMENT_ID COLUMNS TO ALL DATAFRAMES\n",
      "======================================================================\n",
      "‚öôÔ∏è Using existing segmentation results from previous analysis\n",
      "üéØ For dual-language parliaments: separate columns for English vs Native segmentation\n",
      "\n",
      "üìä Extracting segmentation results from previous analysis...\n",
      "‚úÖ AUSTRIAN (English): 12,529 segments extracted\n",
      "‚úÖ AUSTRIAN (German): 12,058 segments extracted\n",
      "‚úÖ CROATIAN (English): 25,115 segments extracted\n",
      "‚úÖ CROATIAN (Croatian): 25,227 segments extracted\n",
      "‚úÖ BRITISH (English): 33,381 segments extracted\n",
      "\n",
      "üîÑ Adding Segment_ID columns...\n",
      "\n",
      "üá¶üáπ AUSTRIAN PARLIAMENT - Adding both English and German segmentations\n",
      "------------------------------------------------------------\n",
      "‚úÖ Added Segment_ID_english: 12,529 unique segments\n",
      "‚úÖ Added Segment_ID_german: 12,058 unique segments\n",
      "‚úÖ Primary Segment_ID set to English version\n",
      "\n",
      "üá≠üá∑ CROATIAN PARLIAMENT - Adding both English and Croatian segmentations\n",
      "------------------------------------------------------------\n",
      "‚úÖ Added Segment_ID_english: 25,115 unique segments\n",
      "‚úÖ Added Segment_ID_croatian: 25,227 unique segments\n",
      "‚úÖ Primary Segment_ID set to English version\n",
      "\n",
      "üá¨üáß BRITISH PARLIAMENT - Adding English segmentation\n",
      "--------------------------------------------------\n",
      "‚úÖ Added Segment_ID: 33,381 unique segments\n",
      "\n",
      "üèÜ SEGMENT_ID CREATION SUMMARY\n",
      "==================================================\n",
      "üá¶üáπ AUSTRIAN PARLIAMENT:\n",
      "  ‚Ä¢ Total speeches: 231,759\n",
      "  ‚Ä¢ English segmentation: 12,529 segments\n",
      "  ‚Ä¢ German segmentation: 12,058 segments\n",
      "  ‚Ä¢ Primary (English): 12,529 segments\n",
      "\n",
      "üá≠üá∑ CROATIAN PARLIAMENT:\n",
      "  ‚Ä¢ Total speeches: 504,338\n",
      "  ‚Ä¢ English segmentation: 25,115 segments\n",
      "  ‚Ä¢ Croatian segmentation: 25,227 segments\n",
      "  ‚Ä¢ Primary (English): 25,115 segments\n",
      "\n",
      "üá¨üáß BRITISH PARLIAMENT:\n",
      "  ‚Ä¢ Total speeches: 670,912\n",
      "  ‚Ä¢ English segmentation: 33,381 segments\n",
      "\n",
      "üíæ SAVING UPDATED DATAFRAMES\n",
      "==============================\n",
      "‚úÖ Saved AT_combined_with_segments.pkl: 231,759 speeches\n",
      "   - Segment_ID_english: 12,529 segments\n",
      "   - Segment_ID_german: 12,058 segments\n",
      "   - Primary Segment_ID: 12,529 segments\n",
      "‚úÖ Saved HR_combined_with_segments.pkl: 504,338 speeches\n",
      "   - Segment_ID_english: 25,115 segments\n",
      "   - Segment_ID_croatian: 25,227 segments\n",
      "   - Primary Segment_ID: 25,115 segments\n",
      "‚úÖ Saved GB_with_segments.pkl: 670,912 speeches\n",
      "   - Segment_ID: 33,381 segments\n",
      "\n",
      "üéØ All dataframes now include Segment_ID columns!\n",
      "üìä Column structure:\n",
      "  ‚Ä¢ Austrian: Segment_ID, Segment_ID_english, Segment_ID_german\n",
      "  ‚Ä¢ Croatian: Segment_ID, Segment_ID_english, Segment_ID_croatian\n",
      "  ‚Ä¢ British: Segment_ID\n",
      "\n",
      "üìä Example Segment_IDs:\n",
      "  ‚Ä¢ Austrian primary: ParlaMint-AT-en_1996-01-15-020-XX-NRSITZ-00001_seg_1\n",
      "  ‚Ä¢ Austrian English: ParlaMint-AT-en_1996-01-15-020-XX-NRSITZ-00001_seg_1\n",
      "  ‚Ä¢ Austrian German: ParlaMint-AT-en_1996-01-15-020-XX-NRSITZ-00001_seg_1\n",
      "  ‚Ä¢ Croatian primary: ParlaMint-HR-en_2003-12-22-0_seg_1\n",
      "  ‚Ä¢ Croatian English: ParlaMint-HR-en_2003-12-22-0_seg_1\n",
      "  ‚Ä¢ Croatian Croatian: ParlaMint-HR-en_2003-12-22-0_seg_1\n",
      "  ‚Ä¢ British: ParlaMint-GB_2015-01-05-commons_seg_1\n",
      "\n",
      "‚úÖ VERIFICATION - Segment counts match previous analysis:\n",
      "  ‚Ä¢ Austrian (EN): 12,529 segments (expected: ~12,529)\n",
      "  ‚Ä¢ Austrian (DE): 12,058 segments (expected: ~12,058)\n",
      "  ‚Ä¢ Croatian (EN): 25,115 segments (expected: ~25,115)\n",
      "  ‚Ä¢ Croatian (HR): 25,227 segments (expected: ~25,227)\n",
      "  ‚Ä¢ British (EN): 33,381 segments (expected: ~33,381)\n",
      "\n",
      "üí° Usage Notes:\n",
      "‚Ä¢ Use 'Segment_ID' for general analysis (defaults to English)\n",
      "‚Ä¢ Use 'Segment_ID_english' / 'Segment_ID_german' / 'Segment_ID_croatian' for language-specific analysis\n",
      "‚Ä¢ This allows comparing how different languages segment the same speeches\n"
     ]
    }
   ],
   "source": [
    "# === ADD SEGMENT_ID TO ALL DATAFRAMES (DUAL LANGUAGE VERSION) ===\n",
    "\n",
    "print(\"üîß ADDING SEGMENT_ID COLUMNS TO ALL DATAFRAMES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"‚öôÔ∏è Using existing segmentation results from previous analysis\")\n",
    "print(f\"üéØ For dual-language parliaments: separate columns for English vs Native segmentation\")\n",
    "\n",
    "def add_segment_id_from_results(dataset, segments_list, text_id_col='Text_ID'):\n",
    "    \"\"\"Add Segment_ID column using pre-calculated segmentation results\"\"\"\n",
    "    \n",
    "    # Create a copy of the dataset\n",
    "    dataset_with_segments = dataset.copy()\n",
    "    \n",
    "    # Initialize Segment_ID column\n",
    "    dataset_with_segments['Segment_ID'] = None\n",
    "    \n",
    "    # Process each segment\n",
    "    for segment in segments_list:\n",
    "        text_id = segment['Text_ID']\n",
    "        start_idx = segment['Start_Index']\n",
    "        end_idx = segment['End_Index']\n",
    "        segment_id = segment['Segment_ID']\n",
    "        \n",
    "        # Get the rows for this Text_ID\n",
    "        text_mask = dataset_with_segments[text_id_col] == text_id\n",
    "        text_data_indices = dataset_with_segments[text_mask].index\n",
    "        \n",
    "        # Map segment boundaries to actual dataframe indices\n",
    "        if len(text_data_indices) > start_idx and len(text_data_indices) > end_idx:\n",
    "            segment_indices = text_data_indices[start_idx:end_idx+1]\n",
    "            dataset_with_segments.loc[segment_indices, 'Segment_ID'] = segment_id\n",
    "    \n",
    "    # Check for missing Segment_IDs\n",
    "    missing_count = dataset_with_segments['Segment_ID'].isna().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"‚ö†Ô∏è Warning: {missing_count} rows still have missing Segment_ID\")\n",
    "        # Fill remaining missing values\n",
    "        for idx in dataset_with_segments[dataset_with_segments['Segment_ID'].isna()].index:\n",
    "            text_id = dataset_with_segments.loc[idx, text_id_col]\n",
    "            dataset_with_segments.loc[idx, 'Segment_ID'] = f\"{text_id}_seg_0\"\n",
    "    \n",
    "    return dataset_with_segments\n",
    "\n",
    "# Extract segmentation results from previous analysis\n",
    "print(f\"\\nüìä Extracting segmentation results from previous analysis...\")\n",
    "\n",
    "# Use the high-quality segments from the results dictionary\n",
    "segment_data = {}\n",
    "for result_name, result_data in results.items():\n",
    "    segment_data[result_name] = result_data['segments']\n",
    "    print(f\"‚úÖ {result_name}: {len(result_data['segments']):,} segments extracted\")\n",
    "\n",
    "print(f\"\\nüîÑ Adding Segment_ID columns...\")\n",
    "\n",
    "# === AUSTRIAN PARLIAMENT (DUAL COLUMNS) ===\n",
    "print(f\"\\nüá¶üáπ AUSTRIAN PARLIAMENT - Adding both English and German segmentations\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Start with original dataset\n",
    "AT_final = AT_combined.copy()\n",
    "\n",
    "# Add English segmentation\n",
    "AT_english_temp = add_segment_id_from_results(AT_combined, segment_data['AUSTRIAN (English)'])\n",
    "AT_final['Segment_ID_english'] = AT_english_temp['Segment_ID']\n",
    "print(f\"‚úÖ Added Segment_ID_english: {AT_final['Segment_ID_english'].nunique():,} unique segments\")\n",
    "\n",
    "# Add German segmentation  \n",
    "AT_german_temp = add_segment_id_from_results(AT_combined, segment_data['AUSTRIAN (German)'])\n",
    "AT_final['Segment_ID_german'] = AT_german_temp['Segment_ID']\n",
    "print(f\"‚úÖ Added Segment_ID_german: {AT_final['Segment_ID_german'].nunique():,} unique segments\")\n",
    "\n",
    "# Add primary Segment_ID (use English as default for consistency)\n",
    "AT_final['Segment_ID'] = AT_final['Segment_ID_english']\n",
    "print(f\"‚úÖ Primary Segment_ID set to English version\")\n",
    "\n",
    "# === CROATIAN PARLIAMENT (DUAL COLUMNS) ===\n",
    "print(f\"\\nüá≠üá∑ CROATIAN PARLIAMENT - Adding both English and Croatian segmentations\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Start with original dataset\n",
    "HR_final = HR_combined.copy()\n",
    "\n",
    "# Add English segmentation\n",
    "HR_english_temp = add_segment_id_from_results(HR_combined, segment_data['CROATIAN (English)'])\n",
    "HR_final['Segment_ID_english'] = HR_english_temp['Segment_ID']\n",
    "print(f\"‚úÖ Added Segment_ID_english: {HR_final['Segment_ID_english'].nunique():,} unique segments\")\n",
    "\n",
    "# Add Croatian segmentation\n",
    "HR_croatian_temp = add_segment_id_from_results(HR_combined, segment_data['CROATIAN (Croatian)'])\n",
    "HR_final['Segment_ID_croatian'] = HR_croatian_temp['Segment_ID']\n",
    "print(f\"‚úÖ Added Segment_ID_croatian: {HR_final['Segment_ID_croatian'].nunique():,} unique segments\")\n",
    "\n",
    "# Add primary Segment_ID (use English as default for consistency)\n",
    "HR_final['Segment_ID'] = HR_final['Segment_ID_english']\n",
    "print(f\"‚úÖ Primary Segment_ID set to English version\")\n",
    "\n",
    "# === BRITISH PARLIAMENT (SINGLE COLUMN) ===\n",
    "print(f\"\\nüá¨üáß BRITISH PARLIAMENT - Adding English segmentation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "GB_final = add_segment_id_from_results(GB, segment_data['BRITISH (English)'])\n",
    "print(f\"‚úÖ Added Segment_ID: {GB_final['Segment_ID'].nunique():,} unique segments\")\n",
    "\n",
    "print(f\"\\nüèÜ SEGMENT_ID CREATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üá¶üáπ AUSTRIAN PARLIAMENT:\")\n",
    "print(f\"  ‚Ä¢ Total speeches: {len(AT_final):,}\")\n",
    "print(f\"  ‚Ä¢ English segmentation: {AT_final['Segment_ID_english'].nunique():,} segments\")\n",
    "print(f\"  ‚Ä¢ German segmentation: {AT_final['Segment_ID_german'].nunique():,} segments\")\n",
    "print(f\"  ‚Ä¢ Primary (English): {AT_final['Segment_ID'].nunique():,} segments\")\n",
    "\n",
    "print(f\"\\nüá≠üá∑ CROATIAN PARLIAMENT:\")\n",
    "print(f\"  ‚Ä¢ Total speeches: {len(HR_final):,}\")\n",
    "print(f\"  ‚Ä¢ English segmentation: {HR_final['Segment_ID_english'].nunique():,} segments\")\n",
    "print(f\"  ‚Ä¢ Croatian segmentation: {HR_final['Segment_ID_croatian'].nunique():,} segments\")\n",
    "print(f\"  ‚Ä¢ Primary (English): {HR_final['Segment_ID'].nunique():,} segments\")\n",
    "\n",
    "print(f\"\\nüá¨üáß BRITISH PARLIAMENT:\")\n",
    "print(f\"  ‚Ä¢ Total speeches: {len(GB_final):,}\")\n",
    "print(f\"  ‚Ä¢ English segmentation: {GB_final['Segment_ID'].nunique():,} segments\")\n",
    "\n",
    "# Check for any missing Segment_IDs\n",
    "for name, df in [(\"Austrian\", AT_final), (\"Croatian\", HR_final), (\"British\", GB_final)]:\n",
    "    missing = df['Segment_ID'].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"‚ö†Ô∏è {name}: {missing:,} missing primary Segment_IDs\")\n",
    "\n",
    "print(f\"\\nüíæ SAVING UPDATED DATAFRAMES\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Save to pickle files\n",
    "AT_final.to_pickle(r\"data folder\\AT\\AT_combined_with_segments.pkl\")\n",
    "HR_final.to_pickle(r\"data folder\\HR\\HR_combined_with_segments.pkl\")\n",
    "GB_final.to_pickle(r\"data folder\\GB\\GB_with_segments.pkl\")\n",
    "\n",
    "print(f\"‚úÖ Saved AT_combined_with_segments.pkl: {len(AT_final):,} speeches\")\n",
    "print(f\"   - Segment_ID_english: {AT_final['Segment_ID_english'].nunique():,} segments\")\n",
    "print(f\"   - Segment_ID_german: {AT_final['Segment_ID_german'].nunique():,} segments\")\n",
    "print(f\"   - Primary Segment_ID: {AT_final['Segment_ID'].nunique():,} segments\")\n",
    "\n",
    "print(f\"‚úÖ Saved HR_combined_with_segments.pkl: {len(HR_final):,} speeches\")\n",
    "print(f\"   - Segment_ID_english: {HR_final['Segment_ID_english'].nunique():,} segments\")\n",
    "print(f\"   - Segment_ID_croatian: {HR_final['Segment_ID_croatian'].nunique():,} segments\")\n",
    "print(f\"   - Primary Segment_ID: {HR_final['Segment_ID'].nunique():,} segments\")\n",
    "\n",
    "print(f\"‚úÖ Saved GB_with_segments.pkl: {len(GB_final):,} speeches\")\n",
    "print(f\"   - Segment_ID: {GB_final['Segment_ID'].nunique():,} segments\")\n",
    "\n",
    "# Update the original variables to include Segment_ID\n",
    "AT_combined = AT_final\n",
    "HR_combined = HR_final\n",
    "GB = GB_final\n",
    "\n",
    "print(f\"\\nüéØ All dataframes now include Segment_ID columns!\")\n",
    "print(f\"üìä Column structure:\")\n",
    "print(f\"  ‚Ä¢ Austrian: Segment_ID, Segment_ID_english, Segment_ID_german\")\n",
    "print(f\"  ‚Ä¢ Croatian: Segment_ID, Segment_ID_english, Segment_ID_croatian\")\n",
    "print(f\"  ‚Ä¢ British: Segment_ID\")\n",
    "\n",
    "print(f\"\\nüìä Example Segment_IDs:\")\n",
    "print(f\"  ‚Ä¢ Austrian primary: {AT_combined['Segment_ID'].iloc[0]}\")\n",
    "print(f\"  ‚Ä¢ Austrian English: {AT_combined['Segment_ID_english'].iloc[0]}\")\n",
    "print(f\"  ‚Ä¢ Austrian German: {AT_combined['Segment_ID_german'].iloc[0]}\")\n",
    "print(f\"  ‚Ä¢ Croatian primary: {HR_combined['Segment_ID'].iloc[0]}\")\n",
    "print(f\"  ‚Ä¢ Croatian English: {HR_combined['Segment_ID_english'].iloc[0]}\")\n",
    "print(f\"  ‚Ä¢ Croatian Croatian: {HR_combined['Segment_ID_croatian'].iloc[0]}\")\n",
    "print(f\"  ‚Ä¢ British: {GB['Segment_ID'].iloc[0]}\")\n",
    "\n",
    "print(f\"\\n‚úÖ VERIFICATION - Segment counts match previous analysis:\")\n",
    "print(f\"  ‚Ä¢ Austrian (EN): {AT_combined['Segment_ID_english'].nunique():,} segments (expected: ~12,529)\")\n",
    "print(f\"  ‚Ä¢ Austrian (DE): {AT_combined['Segment_ID_german'].nunique():,} segments (expected: ~12,058)\")\n",
    "print(f\"  ‚Ä¢ Croatian (EN): {HR_combined['Segment_ID_english'].nunique():,} segments (expected: ~25,115)\")\n",
    "print(f\"  ‚Ä¢ Croatian (HR): {HR_combined['Segment_ID_croatian'].nunique():,} segments (expected: ~25,227)\")\n",
    "print(f\"  ‚Ä¢ British (EN): {GB['Segment_ID'].nunique():,} segments (expected: ~33,381)\")\n",
    "\n",
    "print(f\"\\nüí° Usage Notes:\")\n",
    "print(\"‚Ä¢ Use 'Segment_ID' for general analysis (defaults to English)\")\n",
    "print(\"‚Ä¢ Use 'Segment_ID_english' / 'Segment_ID_german' / 'Segment_ID_croatian' for language-specific analysis\")\n",
    "print(\"‚Ä¢ This allows comparing how different languages segment the same speeches\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
