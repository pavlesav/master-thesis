{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1ca87c",
   "metadata": {},
   "source": [
    "# Topic Modeling with BERTopic on Parliamentary Speeches - Google Colab Version\n",
    "\n",
    "This notebook is optimized for Google Colab with GPU acceleration. It implements a complete pipeline from data loading to topic modeling:\n",
    "\n",
    "1. **Data Loading** - Loads the AT_original_complete.pkl file and processes it\n",
    "2. **Data Filtering** - Creates processed version for topic modeling\n",
    "3. **Dual Embedding** - Speech-level and segment-level embeddings\n",
    "4. **Semantic Segmentation** - Similarity-based boundary detection  \n",
    "\n",
    "## Key Approach - Dual Embedding Strategy:\n",
    "- **First embedding**: Individual speeches using raw text (for segmentation)\n",
    "- **Second embedding**: Concatenated segment texts (for topic modeling)\n",
    "- **Why twice?** Re-embedding captures full discourse coherence vs. averaging individual embeddings\n",
    "- **Raw text used throughout** for better semantic capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955eaf10",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === GOOGLE COLAB SETUP ===\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Mount Google Drive to access your data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      4\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Install required packages\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# === GOOGLE COLAB SETUP ===\n",
    "# Mount Google Drive to access your data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install sentence-transformers bertopic umap-learn hdbscan tqdm openai python-dotenv\n",
    "\n",
    "# Check GPU availability and optimize for A100\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    # Optimize for A100\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"No GPU detected - will use CPU (slower)\")\n",
    "\n",
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"Setup complete! ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340efcc0",
   "metadata": {},
   "source": [
    "## Data Loading and Processing\n",
    "\n",
    "Load the original complete data from Google Drive and create the processed version for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f113b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded original complete data: (231752, 27)\n",
      "Columns: ['Sitting_ID', 'Speech_ID', 'Title', 'Date', 'Body', 'Term', 'Session', 'Meeting', 'Sitting', 'Agenda', 'Subcorpus', 'Lang', 'Speaker_role', 'Speaker_MP', 'Speaker_minister', 'Speaker_party', 'Speaker_party_name', 'Party_status', 'Party_orientation', 'Speaker_ID', 'Speaker_name', 'Speaker_gender', 'Speaker_birth', 'Text', 'Word_Count', 'Is_Too_Short', 'Is_Filtered']\n",
      "\n",
      "üìà Ready for topic modeling: 231,752 speeches\n",
      "\n",
      "üìà Ready for topic modeling: 231,752 speeches\n"
     ]
    }
   ],
   "source": [
    "# === DATA LOADING AND PROCESSING FOR GOOGLE COLAB ===\n",
    "\n",
    "# Path to your file in Google Drive (update if needed)\n",
    "data_folder = '/content/drive/MyDrive/data folder/data/'\n",
    "data_path = f'{data_folder}AT_original_complete.pkl'\n",
    "\n",
    "AT_original_df = pd.read_pickle(data_path)\n",
    "print(f\"‚úÖ Loaded original complete data: {AT_original_df.shape}\")\n",
    "print(f\"Columns: {list(AT_original_df.columns)}\")\n",
    "\n",
    "# Filter out short speeches for segmentation and embedding\n",
    "long_df = AT_original_df[~AT_original_df['Is_Too_Short']].copy()\n",
    "short_df = AT_original_df[AT_original_df['Is_Too_Short']].copy()\n",
    "print(f\"Long speeches for segmentation: {len(long_df):,}\")\n",
    "print(f\"Short speeches to assign after segmentation: {len(short_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686f837",
   "metadata": {},
   "source": [
    "## Embedding and Segmentation Functions (GPU Optimized)\n",
    "\n",
    "These functions are optimized for GPU acceleration and handle the dual-embedding approach:\n",
    "1. **Speech-level embeddings** for similarity-based segmentation  \n",
    "2. **Segment-level embeddings** for final topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f5f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embedding and segmentation functions loaded\n"
     ]
    }
   ],
   "source": [
    "# === EMBEDDING FUNCTIONS (A100 OPTIMIZED) ===\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.signal import find_peaks\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_embedding_model(model_name=\"BAAI/bge-m3\", device=None):\n",
    "    \"\"\"Load embedding model optimized for A100 GPU.\"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print(f\"Loading embedding model: {model_name} on {device}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        model = SentenceTransformer(\n",
    "            model_name, \n",
    "            device=device, \n",
    "            trust_remote_code=True,\n",
    "            model_kwargs={'torch_dtype': torch.float16}  # Use FP16 for A100\n",
    "        )\n",
    "        # Optimize for A100\n",
    "        if device == 'cuda':\n",
    "            model.half()  # Use FP16 for faster inference on A100\n",
    "        print(f\"‚úì Model loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
    "        raise e\n",
    "\n",
    "def chunk_text_tokenwise(text, tokenizer, chunk_size=4096, overlap=1024):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    starts = list(range(0, len(token_ids), chunk_size - overlap))\n",
    "    for start in starts:\n",
    "        end = min(start + chunk_size, len(token_ids))\n",
    "        chunk_ids = token_ids[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        chunks.append((chunk_text, len(chunk_ids)))\n",
    "    return chunks\n",
    "\n",
    "def weighted_mean(embeddings, weights):\n",
    "    embeddings = np.stack(embeddings)\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    return np.average(embeddings, axis=0, weights=weights)\n",
    "\n",
    "def embed_text_bge(text, model, tokenizer):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(token_ids) <= 8192:\n",
    "        return model.encode(text, convert_to_tensor=False, show_progress_bar=False)\n",
    "    else:\n",
    "        chunks = chunk_text_tokenwise(text, tokenizer, chunk_size=4096, overlap=1024)\n",
    "        chunk_texts, chunk_lengths = zip(*chunks)\n",
    "        # Use larger batch size for A100 and suppress progress bar\n",
    "        chunk_embeddings = model.encode(\n",
    "            list(chunk_texts), \n",
    "            batch_size=128, \n",
    "            convert_to_tensor=False,\n",
    "            show_progress_bar=False  # Suppress internal progress bar\n",
    "        )\n",
    "        return weighted_mean(chunk_embeddings, chunk_lengths)\n",
    "\n",
    "def save_checkpoint(data, checkpoint_path):\n",
    "    \"\"\"Save checkpoint for resuming.\"\"\"\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    \"\"\"Load checkpoint for resuming.\"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"üìÇ Checkpoint loaded: {checkpoint_path}\")\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "def generate_speech_embeddings_for_segmentation(\n",
    "    df, text_column='Text', model_name=\"BAAI/bge-m3\", \n",
    "    batch_size=64, checkpoint_freq=5000\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate BGE-m3 embeddings with A100 optimization and checkpointing.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SPEECH EMBEDDINGS: BGE-m3 optimized for A100\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Processing {len(df)} speeches with batch_size={batch_size}\")\n",
    "\n",
    "    # Setup checkpointing\n",
    "    checkpoint_dir = '/content/drive/MyDrive/checkpoints/'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = f'{checkpoint_dir}speech_embeddings_checkpoint.pkl'\n",
    "    \n",
    "    # Try to load existing checkpoint\n",
    "    checkpoint_data = load_checkpoint(checkpoint_path)\n",
    "    if checkpoint_data:\n",
    "        start_idx = checkpoint_data['last_processed_idx'] + 1\n",
    "        embeddings = checkpoint_data['embeddings']\n",
    "        print(f\"üîÑ Resuming from index {start_idx}\")\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        embeddings = []\n",
    "\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        model.half()  # FP16 for A100\n",
    "    tokenizer = model.tokenizer\n",
    "    model.max_seq_length = 8192\n",
    "\n",
    "    texts = df[text_column].astype(str).values\n",
    "    \n",
    "    # Process in batches with larger batch size for A100\n",
    "    total_batches = (len(texts) - start_idx + batch_size - 1) // batch_size\n",
    "    \n",
    "    with tqdm(total=len(texts), initial=start_idx, desc=\"üöÄ Embedding speeches\", unit=\"speech\") as pbar:\n",
    "        for i in range(start_idx, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Process batch - handle long texts individually but in batches where possible\n",
    "            batch_embeddings = []\n",
    "            short_texts = []\n",
    "            short_indices = []\n",
    "            \n",
    "            for j, text in enumerate(batch_texts):\n",
    "                token_count = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "                if token_count <= 8192:\n",
    "                    short_texts.append(text)\n",
    "                    short_indices.append(j)\n",
    "                else:\n",
    "                    # Handle long text individually - suppress any internal progress\n",
    "                    emb = embed_text_bge(text, model, tokenizer)\n",
    "                    batch_embeddings.append((j, emb))\n",
    "            \n",
    "            # Batch process short texts with larger batch size\n",
    "            if short_texts:\n",
    "                short_embeddings = model.encode(\n",
    "                    short_texts, \n",
    "                    batch_size=min(128, len(short_texts)),  # A100 optimized batch size\n",
    "                    convert_to_tensor=False,\n",
    "                    show_progress_bar=False  # Suppress internal progress bar\n",
    "                )\n",
    "                for idx, emb in zip(short_indices, short_embeddings):\n",
    "                    batch_embeddings.append((idx, emb))\n",
    "            \n",
    "            # Sort by original order and add to results\n",
    "            batch_embeddings.sort(key=lambda x: x[0])\n",
    "            embeddings.extend([emb for _, emb in batch_embeddings])\n",
    "            \n",
    "            # Update progress (only once per batch)\n",
    "            pbar.update(len(batch_texts))\n",
    "            \n",
    "            # Save checkpoint periodically (suppress checkpoint messages)\n",
    "            if (i + batch_size) % checkpoint_freq == 0:\n",
    "                checkpoint_data = {\n",
    "                    'embeddings': embeddings,\n",
    "                    'last_processed_idx': i + len(batch_texts) - 1\n",
    "                }\n",
    "                with open(checkpoint_path, 'wb') as f:\n",
    "                    pickle.dump(checkpoint_data, f)\n",
    "                # Only show checkpoint message every 20k speeches\n",
    "                if (i + batch_size) % (checkpoint_freq * 4) == 0:\n",
    "                    print(f\"\\nüíæ Checkpoint: {i + batch_size:,}/{len(texts):,} speeches processed\")\n",
    "                \n",
    "            # Clear GPU cache periodically\n",
    "            if (i + batch_size) % (checkpoint_freq * 2) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "    # Clean up checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "        print(\"üßπ Checkpoint cleaned up\")\n",
    "\n",
    "    df_with_embeddings = df.copy()\n",
    "    df_with_embeddings['Speech_Embeddings'] = embeddings\n",
    "    return df_with_embeddings\n",
    "\n",
    "# === IMPROVED PARLIAMENTARY SEGMENTATION FUNCTIONS ===\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def parliamentary_segment_speeches(df, window_size=3, min_segment_size=3):\n",
    "    \"\"\"\n",
    "    Parliamentary segmentation with multi-scale analysis and chairperson agenda detection\n",
    "    \"\"\"\n",
    "    segment_ids = []\n",
    "    segmentation_metrics = []\n",
    "    \n",
    "    # Get unique sittings for progress tracking\n",
    "    unique_sittings = df['Sitting_ID'].unique()\n",
    "    print(f\"üîÑ Processing {len(unique_sittings)} sittings...\")\n",
    "    \n",
    "    for sitting_id in tqdm(unique_sittings, desc=\"Segmenting sittings\", unit=\"sitting\"):\n",
    "        group = df[df['Sitting_ID'] == sitting_id]\n",
    "        sitting_length = len(group)\n",
    "        \n",
    "        if sitting_length < min_segment_size:\n",
    "            # Very small sitting - one segment\n",
    "            sitting_segments = [f\"{sitting_id}_seg_0\"] * len(group)\n",
    "            segment_ids.extend(sitting_segments)\n",
    "            segmentation_metrics.append({\n",
    "                'sitting_id': sitting_id,\n",
    "                'sitting_length': sitting_length,\n",
    "                'num_segments': 1,\n",
    "                'avg_segment_size': sitting_length,\n",
    "                'boundaries_found': 0,\n",
    "                'agenda_boundaries': 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        embeddings = np.array(group['Speech_Embeddings'].tolist())\n",
    "        \n",
    "        # --- NEW: Flexible target_segments formula ---\n",
    "        target_segments = max(2, int(np.ceil(sitting_length / 25)))\n",
    "        threshold_percentile = 40\n",
    "        \n",
    "        # === CHAIRPERSON AGENDA DETECTION ===\n",
    "        agenda_boundaries = set()\n",
    "        agenda_signals = []\n",
    "        \n",
    "        for i, (idx, row) in enumerate(group.iterrows()):\n",
    "            agenda_score = 0\n",
    "            \n",
    "            # Strong signal for chairperson with agenda mentions\n",
    "            if row['Speaker_role'] == 'Chairperson':\n",
    "                text = str(row['Text']).lower()\n",
    "                \n",
    "                if 'agenda item' in text:\n",
    "                    agenda_score = 1.0  # Strongest signal\n",
    "                elif 'agenda' in text:\n",
    "                    agenda_score = 0.7  # Strong signal\n",
    "                elif i == 0:  # First speech by chairperson (session start)\n",
    "                    agenda_score = 0.3  # Mild signal\n",
    "            \n",
    "            agenda_signals.append(agenda_score)\n",
    "            \n",
    "            # Add strong agenda boundaries\n",
    "            if agenda_score >= 0.7 and i >= min_segment_size and (sitting_length - i) >= min_segment_size:\n",
    "                agenda_boundaries.add(i)\n",
    "        \n",
    "        # === MULTI-SCALE SIMILARITY ANALYSIS ===\n",
    "        similarity_signals = {}\n",
    "        \n",
    "        # 1. Primary windowed similarity\n",
    "        similarities = []\n",
    "        for i in range(len(embeddings) - window_size):\n",
    "            window1 = np.mean(embeddings[i:i + window_size], axis=0)\n",
    "            window2 = np.mean(embeddings[i + window_size:i + 2*window_size], axis=0)\n",
    "            \n",
    "            sim = cosine_similarity(\n",
    "                window1.reshape(1, -1),\n",
    "                window2.reshape(1, -1)\n",
    "            )[0][0]\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        similarity_signals['primary'] = np.array(similarities)\n",
    "        \n",
    "        # 2. Point-to-point similarity for fine-grained detection\n",
    "        if len(embeddings) > 6:\n",
    "            point_sims = []\n",
    "            for i in range(len(embeddings) - 1):\n",
    "                sim = cosine_similarity(\n",
    "                    embeddings[i].reshape(1, -1),\n",
    "                    embeddings[i + 1].reshape(1, -1)\n",
    "                )[0][0]\n",
    "                point_sims.append(sim)\n",
    "            \n",
    "            # Align with primary signal\n",
    "            point_sims = np.array(point_sims)\n",
    "            if len(point_sims) > len(similarities):\n",
    "                point_sims = point_sims[:len(similarities)]\n",
    "            elif len(point_sims) < len(similarities):\n",
    "                padding = len(similarities) - len(point_sims)\n",
    "                point_sims = np.pad(point_sims, (0, padding), mode='edge')\n",
    "            \n",
    "            similarity_signals['point'] = point_sims\n",
    "        \n",
    "        # 3. Gradient-based change detection\n",
    "        if len(embeddings) > 10:\n",
    "            trajectory = []\n",
    "            for i in range(1, len(embeddings)):\n",
    "                displacement = np.linalg.norm(embeddings[i] - embeddings[i-1])\n",
    "                trajectory.append(float(displacement))\n",
    "            \n",
    "            trajectory = np.array(trajectory, dtype=np.float64)\n",
    "            if len(trajectory) > 3:\n",
    "                try:\n",
    "                    from scipy.ndimage import uniform_filter1d\n",
    "                    smoothed = uniform_filter1d(trajectory.astype(np.float64), size=3)\n",
    "                    gradient = np.gradient(smoothed)\n",
    "                    \n",
    "                    # Align with similarities\n",
    "                    if len(gradient) > len(similarities):\n",
    "                        gradient = gradient[:len(similarities)]\n",
    "                    elif len(gradient) < len(similarities):\n",
    "                        padding = len(similarities) - len(gradient)\n",
    "                        gradient = np.pad(gradient, (0, padding), mode='edge')\n",
    "                    \n",
    "                    similarity_signals['gradient'] = gradient\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if len(similarity_signals['primary']) == 0:\n",
    "            sitting_segments = [f\"{sitting_id}_seg_0\"] * len(group)\n",
    "            segment_ids.extend(sitting_segments)\n",
    "            segmentation_metrics.append({\n",
    "                'sitting_id': sitting_id,\n",
    "                'sitting_length': sitting_length,\n",
    "                'num_segments': 1,\n",
    "                'avg_segment_size': sitting_length,\n",
    "                'boundaries_found': 0,\n",
    "                'agenda_boundaries': 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # === BOUNDARY DETECTION ===\n",
    "        candidate_boundaries = set()\n",
    "        \n",
    "        # 1. Add agenda boundaries (highest priority)\n",
    "        candidate_boundaries.update(agenda_boundaries)\n",
    "        \n",
    "        # 2. Find boundaries from primary similarity drops\n",
    "        primary_sims = similarity_signals['primary']\n",
    "        threshold = np.percentile(primary_sims, threshold_percentile)\n",
    "        \n",
    "        for i in range(len(primary_sims)):\n",
    "            if (primary_sims[i] < threshold and \n",
    "                i >= min_segment_size and \n",
    "                (len(group) - i - window_size) >= min_segment_size):\n",
    "                candidate_boundaries.add(i + window_size)\n",
    "        \n",
    "        # 3. Add from point-to-point analysis\n",
    "        if 'point' in similarity_signals:\n",
    "            point_threshold = np.percentile(similarity_signals['point'], threshold_percentile - 10)\n",
    "            for i in range(len(similarity_signals['point'])):\n",
    "                if (similarity_signals['point'][i] < point_threshold and \n",
    "                    i >= min_segment_size and \n",
    "                    (len(group) - i) >= min_segment_size):\n",
    "                    candidate_boundaries.add(i)\n",
    "        \n",
    "        # 4. Add from gradient analysis\n",
    "        if 'gradient' in similarity_signals:\n",
    "            gradient = similarity_signals['gradient']\n",
    "            gradient_threshold = np.percentile(np.abs(gradient), 75)\n",
    "            for i in range(len(gradient)):\n",
    "                if (np.abs(gradient[i]) > gradient_threshold and \n",
    "                    i >= min_segment_size and \n",
    "                    (len(group) - i) >= min_segment_size):\n",
    "                    candidate_boundaries.add(i)\n",
    "        \n",
    "        candidates = sorted(list(candidate_boundaries))\n",
    "        \n",
    "        # === BOUNDARY SELECTION WITH AGENDA PRIORITIZATION ===\n",
    "        boundaries = []\n",
    "        if candidates:\n",
    "            if len(candidates) <= target_segments - 1:\n",
    "                boundaries = candidates\n",
    "            else:\n",
    "                # Score candidates with agenda boost\n",
    "                candidate_scores = []\n",
    "                for c in candidates:\n",
    "                    score = 0\n",
    "                    \n",
    "                    # Agenda boost (highest priority)\n",
    "                    if c < len(agenda_signals):\n",
    "                        score += agenda_signals[c] * 5.0  # Very high weight for agenda\n",
    "                    \n",
    "                    # Primary similarity score\n",
    "                    if c - window_size >= 0 and c - window_size < len(primary_sims):\n",
    "                        score += (1 - primary_sims[c - window_size]) * 2.0\n",
    "                    \n",
    "                    # Point similarity score\n",
    "                    if 'point' in similarity_signals and c < len(similarity_signals['point']):\n",
    "                        score += (1 - similarity_signals['point'][c]) * 1.5\n",
    "                    \n",
    "                    # Gradient score\n",
    "                    if 'gradient' in similarity_signals and c < len(similarity_signals['gradient']):\n",
    "                        score += np.abs(similarity_signals['gradient'][c]) * 1.0\n",
    "                    \n",
    "                    candidate_scores.append((c, score))\n",
    "                \n",
    "                # Select top scoring boundaries\n",
    "                candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                boundaries = sorted([c for c, _ in candidate_scores[:target_segments-1]])\n",
    "        \n",
    "        # === BOUNDARY VALIDATION ===\n",
    "        validated_boundaries = []\n",
    "        for boundary in boundaries:\n",
    "            if not validated_boundaries or (boundary - validated_boundaries[-1]) >= min_segment_size:\n",
    "                validated_boundaries.append(boundary)\n",
    "        \n",
    "        boundaries = validated_boundaries\n",
    "        \n",
    "        # Assign segment IDs\n",
    "        current_segment = 0\n",
    "        sitting_segments = []\n",
    "        \n",
    "        for i in range(len(group)):\n",
    "            if i > 0 and (i - 1) in boundaries:\n",
    "                current_segment += 1\n",
    "            sitting_segments.append(f\"{sitting_id}_seg_{current_segment}\")\n",
    "        \n",
    "        segment_ids.extend(sitting_segments)\n",
    "        \n",
    "        # Store metrics\n",
    "        num_segments = len(set(sitting_segments))\n",
    "        agenda_bound_count = len([b for b in boundaries if b in agenda_boundaries])\n",
    "        \n",
    "        segmentation_metrics.append({\n",
    "            'sitting_id': sitting_id,\n",
    "            'sitting_length': sitting_length,\n",
    "            'num_segments': num_segments,\n",
    "            'avg_segment_size': sitting_length / num_segments,\n",
    "            'boundaries_found': len(boundaries),\n",
    "            'agenda_boundaries': agenda_bound_count,\n",
    "            'target_segments': target_segments,\n",
    "            'candidate_boundaries': len(candidates),\n",
    "            'signals_used': len(similarity_signals) + 1  # +1 for agenda signals\n",
    "        })\n",
    "    \n",
    "    df['Segment_ID'] = segment_ids\n",
    "    return df, segmentation_metrics\n",
    "\n",
    "print(\"‚úì Enhanced parliamentary segmentation with agenda detection loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98114534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN THE ENHANCED PIPELINE ===\n",
    "\n",
    "print(\"üöÄ Starting A100-optimized embedding pipeline with enhanced segmentation...\")\n",
    "print(f\"üíª Using: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"üìä Processing {len(long_df)} speeches for segmentation\")\n",
    "\n",
    "# Clear any existing cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    # STEP 1: Generate speech-level embeddings with A100 optimization\n",
    "    print(\"\\nüîÑ Generating speech-level embeddings...\")\n",
    "    df_with_speech_embeddings = generate_speech_embeddings_for_segmentation(\n",
    "        long_df, \n",
    "        text_column='Text',\n",
    "        batch_size=64,  # A100 optimized batch size\n",
    "        checkpoint_freq=10000  # Checkpoint every 10k speeches\n",
    "    )\n",
    "    print(\"‚úÖ Speech-level embeddings generated!\")\n",
    "    \n",
    "    # Clear cache before segmentation\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # STEP 2: Enhanced parliamentary segmentation with agenda detection\n",
    "    print(\"\\nüèõÔ∏è Running enhanced parliamentary segmentation...\")\n",
    "    df_segmented, seg_metrics = parliamentary_segment_speeches(\n",
    "        df_with_speech_embeddings, \n",
    "        window_size=5,        \n",
    "        min_segment_size=3    # Smaller minimum for more segments\n",
    "    )\n",
    "    \n",
    "    # Display segmentation results\n",
    "    metrics_df = pd.DataFrame(seg_metrics)\n",
    "    print(f\"\\n‚úÖ Enhanced segmentation complete!\")\n",
    "    print(f\"üìä Results:\")\n",
    "    print(f\"  ‚Ä¢ Total speeches processed: {len(df_segmented):,}\")\n",
    "    print(f\"  ‚Ä¢ Unique segments created: {df_segmented['Segment_ID'].nunique():,}\")\n",
    "    print(f\"  ‚Ä¢ Average speeches per segment: {len(df_segmented) / df_segmented['Segment_ID'].nunique():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Average segments per sitting: {metrics_df['num_segments'].mean():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Agenda boundaries used: {metrics_df['agenda_boundaries'].sum()}\")\n",
    "\n",
    "    # STEP 3: Assign short speeches to nearest segment (FIXED)\n",
    "    print(\"\\nüîÑ Assigning short speeches to segments...\")\n",
    "    def assign_short_speeches(short_df, segmented_df):\n",
    "        \"\"\"Assign short speeches to segments based on their original order within sittings.\"\"\"\n",
    "        assigned = []\n",
    "        for sitting_id, group in short_df.groupby('Sitting_ID'):\n",
    "            seg_group = segmented_df[segmented_df['Sitting_ID'] == sitting_id]\n",
    "            if seg_group.empty:\n",
    "                # If no segments in this sitting, create a default segment\n",
    "                default_segment = f\"{sitting_id}_seg_0\"\n",
    "                assigned.extend([default_segment] * len(group))\n",
    "                continue\n",
    "            \n",
    "            # Get unique segments for this sitting in order\n",
    "            segments_in_sitting = seg_group['Segment_ID'].unique()\n",
    "            \n",
    "            # For each short speech, assign to the first available segment\n",
    "            # This is a simple strategy - you could make it more sophisticated\n",
    "            for idx, row in group.iterrows():\n",
    "                # Assign to first segment (could be improved with better logic)\n",
    "                assigned.append(segments_in_sitting[0])\n",
    "        \n",
    "        short_df = short_df.copy()\n",
    "        short_df['Segment_ID'] = assigned\n",
    "        return short_df\n",
    "\n",
    "    short_df_assigned = assign_short_speeches(short_df, df_segmented)\n",
    "    df_all = pd.concat([df_segmented, short_df_assigned], ignore_index=True)\n",
    "\n",
    "    # STEP 4: Generate segment-level embeddings with A100 optimization\n",
    "    print(\"\\nüîÑ Generating segment-level embeddings...\")\n",
    "    df_final = generate_segment_embeddings(\n",
    "        df_all, \n",
    "        text_column='Text', \n",
    "        segment_id_column='Segment_ID',\n",
    "        batch_size=32  # A100 optimized for segments\n",
    "    )\n",
    "    print(\"‚úÖ Segment-level embeddings mapped!\")\n",
    "\n",
    "    # STEP 5: Save final output\n",
    "    output_path = f\"{data_folder}AT_with_embeddings_final.pkl\"\n",
    "    df_final.to_pickle(output_path)\n",
    "    print(f\"\\nüíæ Saved final dataframe: {output_path}\")\n",
    "    print(f\"üìä Final shape: {df_final.shape}\")\n",
    "    print(f\"üéØ Segments created: {df_final['Segment_ID'].nunique()}\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ Memory cleaned up\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in pipeline: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fcc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVE RESULTS (OPTIONAL) ===\n",
    "# Uncomment to save the final results\n",
    "\n",
    "print(\"üíæ Saving final results...\")\n",
    "final_embeddings.to_pickle('data folder/data/AT_with_22_topics_final.pkl')\n",
    "final_topic_info.to_pickle('data folder/data/topic_info_22_categories.pkl')\n",
    "category_stats.to_pickle('data folder/data/category_statistics.pkl')\n",
    "print(\"‚úÖ Results saved!\")\n",
    "\n",
    "print(\"\\nüéâ Analysis complete! Ready to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
