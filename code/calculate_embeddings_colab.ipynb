{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1ca87c",
   "metadata": {},
   "source": [
    "# Topic Modeling with BERTopic on Parliamentary Speeches - Google Colab Version\n",
    "\n",
    "This notebook is optimized for Google Colab with GPU acceleration. It implements a complete pipeline from data loading to topic modeling:\n",
    "\n",
    "1. **Data Loading** - Loads the AT_original_complete.pkl file and processes it\n",
    "2. **Data Filtering** - Creates processed version for topic modeling\n",
    "3. **Dual Embedding** - Speech-level and segment-level embeddings\n",
    "4. **Semantic Segmentation** - Similarity-based boundary detection  \n",
    "\n",
    "## Key Approach - Dual Embedding Strategy:\n",
    "- **First embedding**: Individual speeches using raw text (for segmentation)\n",
    "- **Second embedding**: Concatenated segment texts (for topic modeling)\n",
    "- **Why twice?** Re-embedding captures full discourse coherence vs. averaging individual embeddings\n",
    "- **Raw text used throughout** for better semantic capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955eaf10",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === GOOGLE COLAB SETUP ===\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Mount Google Drive to access your data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      4\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Install required packages\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# === GOOGLE COLAB SETUP ===\n",
    "# Mount Google Drive to access your data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install required packages\n",
    "!pip install sentence-transformers bertopic umap-learn hdbscan tqdm openai python-dotenv\n",
    "\n",
    "# Check GPU availability and optimize for A100\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    # Optimize for A100\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"No GPU detected - will use CPU (slower)\")\n",
    "\n",
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"Setup complete! ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340efcc0",
   "metadata": {},
   "source": [
    "## Data Loading and Processing\n",
    "\n",
    "Load the original complete data from Google Drive and create the processed version for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f113b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded original complete data: (231752, 27)\n",
      "Columns: ['Sitting_ID', 'Speech_ID', 'Title', 'Date', 'Body', 'Term', 'Session', 'Meeting', 'Sitting', 'Agenda', 'Subcorpus', 'Lang', 'Speaker_role', 'Speaker_MP', 'Speaker_minister', 'Speaker_party', 'Speaker_party_name', 'Party_status', 'Party_orientation', 'Speaker_ID', 'Speaker_name', 'Speaker_gender', 'Speaker_birth', 'Text', 'Word_Count', 'Is_Too_Short', 'Is_Filtered']\n",
      "\n",
      "üìà Ready for topic modeling: 231,752 speeches\n",
      "\n",
      "üìà Ready for topic modeling: 231,752 speeches\n"
     ]
    }
   ],
   "source": [
    "# === DATA LOADING AND PROCESSING FOR GOOGLE COLAB ===\n",
    "\n",
    "# Path to your file in Google Drive (update if needed)\n",
    "data_folder = '/content/drive/MyDrive/data folder/data/'\n",
    "data_path = f'{data_folder}AT_original_complete.pkl'\n",
    "\n",
    "AT_original_df = pd.read_pickle(data_path)\n",
    "print(f\"‚úÖ Loaded original complete data: {AT_original_df.shape}\")\n",
    "print(f\"Columns: {list(AT_original_df.columns)}\")\n",
    "\n",
    "# Filter out short speeches for segmentation and embedding\n",
    "long_df = AT_original_df[~AT_original_df['Is_Too_Short']].copy()\n",
    "short_df = AT_original_df[AT_original_df['Is_Too_Short']].copy()\n",
    "print(f\"Long speeches for segmentation: {len(long_df):,}\")\n",
    "print(f\"Short speeches to assign after segmentation: {len(short_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686f837",
   "metadata": {},
   "source": [
    "## Embedding and Segmentation Functions (GPU Optimized)\n",
    "\n",
    "These functions are optimized for GPU acceleration and handle the dual-embedding approach:\n",
    "1. **Speech-level embeddings** for similarity-based segmentation  \n",
    "2. **Segment-level embeddings** for final topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f5f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embedding and segmentation functions loaded\n"
     ]
    }
   ],
   "source": [
    "# === EMBEDDING FUNCTIONS (A100 OPTIMIZED) ===\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.signal import find_peaks\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_embedding_model(model_name=\"BAAI/bge-m3\", device=None):\n",
    "    \"\"\"Load embedding model optimized for A100 GPU.\"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print(f\"Loading embedding model: {model_name} on {device}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        model = SentenceTransformer(\n",
    "            model_name, \n",
    "            device=device, \n",
    "            trust_remote_code=True,\n",
    "            model_kwargs={'torch_dtype': torch.float16}  # Use FP16 for A100\n",
    "        )\n",
    "        # Optimize for A100\n",
    "        if device == 'cuda':\n",
    "            model.half()  # Use FP16 for faster inference on A100\n",
    "        print(f\"‚úì Model loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {model_name}: {e}\")\n",
    "        raise e\n",
    "\n",
    "def chunk_text_tokenwise(text, tokenizer, chunk_size=4096, overlap=1024):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    starts = list(range(0, len(token_ids), chunk_size - overlap))\n",
    "    for start in starts:\n",
    "        end = min(start + chunk_size, len(token_ids))\n",
    "        chunk_ids = token_ids[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        chunks.append((chunk_text, len(chunk_ids)))\n",
    "    return chunks\n",
    "\n",
    "def weighted_mean(embeddings, weights):\n",
    "    embeddings = np.stack(embeddings)\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    return np.average(embeddings, axis=0, weights=weights)\n",
    "\n",
    "def embed_text_bge(text, model, tokenizer):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(token_ids) <= 8192:\n",
    "        return model.encode(text, convert_to_tensor=False, show_progress_bar=False)\n",
    "    else:\n",
    "        chunks = chunk_text_tokenwise(text, tokenizer, chunk_size=4096, overlap=1024)\n",
    "        chunk_texts, chunk_lengths = zip(*chunks)\n",
    "        # Use larger batch size for A100 and suppress progress bar\n",
    "        chunk_embeddings = model.encode(\n",
    "            list(chunk_texts), \n",
    "            batch_size=128, \n",
    "            convert_to_tensor=False,\n",
    "            show_progress_bar=False  # Suppress internal progress bar\n",
    "        )\n",
    "        return weighted_mean(chunk_embeddings, chunk_lengths)\n",
    "\n",
    "def save_checkpoint(data, checkpoint_path):\n",
    "    \"\"\"Save checkpoint for resuming.\"\"\"\n",
    "    with open(checkpoint_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    \"\"\"Load checkpoint for resuming.\"\"\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"üìÇ Checkpoint loaded: {checkpoint_path}\")\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "def generate_speech_embeddings_for_segmentation(\n",
    "    df, text_column='Text', model_name=\"BAAI/bge-m3\", \n",
    "    batch_size=64, checkpoint_freq=5000\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate BGE-m3 embeddings with A100 optimization and checkpointing.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SPEECH EMBEDDINGS: BGE-m3 optimized for A100\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Processing {len(df)} speeches with batch_size={batch_size}\")\n",
    "\n",
    "    # Setup checkpointing\n",
    "    checkpoint_dir = '/content/drive/MyDrive/checkpoints/'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = f'{checkpoint_dir}speech_embeddings_checkpoint.pkl'\n",
    "    \n",
    "    # Try to load existing checkpoint\n",
    "    checkpoint_data = load_checkpoint(checkpoint_path)\n",
    "    if checkpoint_data:\n",
    "        start_idx = checkpoint_data['last_processed_idx'] + 1\n",
    "        embeddings = checkpoint_data['embeddings']\n",
    "        print(f\"üîÑ Resuming from index {start_idx}\")\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        embeddings = []\n",
    "\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        model.half()  # FP16 for A100\n",
    "    tokenizer = model.tokenizer\n",
    "    model.max_seq_length = 8192\n",
    "\n",
    "    texts = df[text_column].astype(str).values\n",
    "    \n",
    "    # Process in batches with larger batch size for A100\n",
    "    total_batches = (len(texts) - start_idx + batch_size - 1) // batch_size\n",
    "    \n",
    "    with tqdm(total=len(texts), initial=start_idx, desc=\"üöÄ Embedding speeches\", unit=\"speech\") as pbar:\n",
    "        for i in range(start_idx, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Process batch - handle long texts individually but in batches where possible\n",
    "            batch_embeddings = []\n",
    "            short_texts = []\n",
    "            short_indices = []\n",
    "            \n",
    "            for j, text in enumerate(batch_texts):\n",
    "                token_count = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "                if token_count <= 8192:\n",
    "                    short_texts.append(text)\n",
    "                    short_indices.append(j)\n",
    "                else:\n",
    "                    # Handle long text individually - suppress any internal progress\n",
    "                    emb = embed_text_bge(text, model, tokenizer)\n",
    "                    batch_embeddings.append((j, emb))\n",
    "            \n",
    "            # Batch process short texts with larger batch size\n",
    "            if short_texts:\n",
    "                short_embeddings = model.encode(\n",
    "                    short_texts, \n",
    "                    batch_size=min(128, len(short_texts)),  # A100 optimized batch size\n",
    "                    convert_to_tensor=False,\n",
    "                    show_progress_bar=False  # Suppress internal progress bar\n",
    "                )\n",
    "                for idx, emb in zip(short_indices, short_embeddings):\n",
    "                    batch_embeddings.append((idx, emb))\n",
    "            \n",
    "            # Sort by original order and add to results\n",
    "            batch_embeddings.sort(key=lambda x: x[0])\n",
    "            embeddings.extend([emb for _, emb in batch_embeddings])\n",
    "            \n",
    "            # Update progress (only once per batch)\n",
    "            pbar.update(len(batch_texts))\n",
    "            \n",
    "            # Save checkpoint periodically (suppress checkpoint messages)\n",
    "            if (i + batch_size) % checkpoint_freq == 0:\n",
    "                checkpoint_data = {\n",
    "                    'embeddings': embeddings,\n",
    "                    'last_processed_idx': i + len(batch_texts) - 1\n",
    "                }\n",
    "                with open(checkpoint_path, 'wb') as f:\n",
    "                    pickle.dump(checkpoint_data, f)\n",
    "                # Only show checkpoint message every 20k speeches\n",
    "                if (i + batch_size) % (checkpoint_freq * 4) == 0:\n",
    "                    print(f\"\\nüíæ Checkpoint: {i + batch_size:,}/{len(texts):,} speeches processed\")\n",
    "                \n",
    "            # Clear GPU cache periodically\n",
    "            if (i + batch_size) % (checkpoint_freq * 2) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "    # Clean up checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "        print(\"üßπ Checkpoint cleaned up\")\n",
    "\n",
    "    df_with_embeddings = df.copy()\n",
    "    df_with_embeddings['Speech_Embeddings'] = embeddings\n",
    "    return df_with_embeddings\n",
    "\n",
    "# Keep the existing similarity and segmentation functions (they don't need GPU optimization)\n",
    "def calculate_windowed_similarity(embeddings_list, window_size=3):\n",
    "    \"\"\"Calculate cosine similarity between windowed embeddings.\"\"\"\n",
    "    if len(embeddings_list) < 2:\n",
    "        return np.array([])\n",
    "    if window_size < 1:\n",
    "        raise ValueError(\"Window size must be at least 1.\")\n",
    "\n",
    "    num_utterances = len(embeddings_list)\n",
    "    similarities = []\n",
    "\n",
    "    for g in range(num_utterances - 1):\n",
    "        # Window before gap\n",
    "        start_before = max(0, g - window_size + 1)\n",
    "        end_before = g + 1\n",
    "        window_before = embeddings_list[start_before:end_before]\n",
    "\n",
    "        # Window after gap\n",
    "        start_after = g + 1\n",
    "        end_after = min(num_utterances, g + 1 + window_size)\n",
    "        window_after = embeddings_list[start_after:end_after]\n",
    "\n",
    "        if not window_before or not window_after:\n",
    "            similarities.append(0)\n",
    "            continue\n",
    "\n",
    "        # Calculate mean embeddings and similarity\n",
    "        mean_before = np.mean([np.asarray(e) for e in window_before], axis=0)\n",
    "        mean_after = np.mean([np.asarray(e) for e in window_after], axis=0)\n",
    "        \n",
    "        sim = cosine_similarity(mean_before.reshape(1, -1), mean_after.reshape(1, -1))[0][0]\n",
    "        similarities.append(sim)\n",
    "        \n",
    "    return np.array(similarities)\n",
    "\n",
    "def find_topic_boundaries(similarities, height_threshold=0.3, prominence_threshold=0.2, distance_threshold=5):\n",
    "    \"\"\"Find topic boundaries using peak detection on inverted similarity scores.\"\"\"\n",
    "    if len(similarities) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Invert similarities to find valleys (topic boundaries)\n",
    "    inverted_similarities = np.maximum(0, 1 - similarities)\n",
    "    \n",
    "    # Find peaks in inverted similarities\n",
    "    peaks, _ = find_peaks(\n",
    "        inverted_similarities,\n",
    "        height=height_threshold,\n",
    "        prominence=prominence_threshold,\n",
    "        distance=distance_threshold\n",
    "    )\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "def segment_speeches_by_similarity(df, window_size=3, height_threshold=0.3, \n",
    "                                   prominence_threshold=0.2, distance_threshold=5):\n",
    "    \"\"\"Segment speeches within each sitting based on semantic similarity.\"\"\"\n",
    "    print(f\"üîç Segmenting speeches using similarity-based approach\")\n",
    "    print(f\"Parameters: window_size={window_size}, height_threshold={height_threshold}\")\n",
    "    print(f\"           prominence_threshold={prominence_threshold}, distance_threshold={distance_threshold}\")\n",
    "    \n",
    "    df_segmented = df.copy()\n",
    "    segment_ids = []\n",
    "    total_boundaries = 0\n",
    "    \n",
    "    # Process each sitting separately with progress bar\n",
    "    sittings = list(df_segmented.groupby('Sitting_ID'))\n",
    "    \n",
    "    for sitting_id, group in tqdm(sittings, desc=\"üî™ Segmenting sittings\", unit=\"sitting\"):\n",
    "        if len(group) < 2:\n",
    "            segment_ids.extend([f\"{sitting_id}_seg_0\"] * len(group))\n",
    "            continue\n",
    "        \n",
    "        # Use the speech-level embeddings for segmentation\n",
    "        embeddings_list = group['Speech_Embeddings'].tolist()\n",
    "        similarities = calculate_windowed_similarity(embeddings_list, window_size)\n",
    "        \n",
    "        if len(similarities) == 0:\n",
    "            segment_ids.extend([f\"{sitting_id}_seg_0\"] * len(group))\n",
    "            continue\n",
    "        \n",
    "        # Find boundaries\n",
    "        boundaries = find_topic_boundaries(\n",
    "            similarities, height_threshold, prominence_threshold, distance_threshold\n",
    "        )\n",
    "        total_boundaries += len(boundaries)\n",
    "        \n",
    "        # Assign segment IDs\n",
    "        current_segment = 0\n",
    "        sitting_segment_ids = []\n",
    "        \n",
    "        for i in range(len(group)):\n",
    "            if i > 0 and (i - 1) in boundaries:\n",
    "                current_segment += 1\n",
    "            sitting_segment_ids.append(f\"{sitting_id}_seg_{current_segment}\")\n",
    "        \n",
    "        segment_ids.extend(sitting_segment_ids)\n",
    "    \n",
    "    df_segmented['Segment_ID'] = segment_ids\n",
    "    \n",
    "    # Print statistics\n",
    "    total_segments = df_segmented['Segment_ID'].nunique()\n",
    "    avg_segments_per_sitting = df_segmented.groupby('Sitting_ID')['Segment_ID'].nunique().mean()\n",
    "    \n",
    "    print(f\"‚úì Segmentation complete!\")\n",
    "    print(f\"‚úì Total boundaries detected: {total_boundaries}\")\n",
    "    print(f\"‚úì Total segments created: {total_segments}\")\n",
    "    print(f\"‚úì Average segments per sitting: {avg_segments_per_sitting:.2f}\")\n",
    "    \n",
    "    return df_segmented\n",
    "\n",
    "print(\"‚úì Embedding and segmentation functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38b773",
   "metadata": {},
   "source": [
    "## Segment Aggregation and Re-embedding Functions (GPU Optimized)\n",
    "\n",
    "After creating segments, we aggregate the raw text and re-embed for better topic modeling representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEGMENT AGGREGATION AND RE-EMBEDDING FUNCTIONS (A100 OPTIMIZED) ===\n",
    "\n",
    "BGE_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "TOKEN_LIMIT = 8192\n",
    "CHUNK_SIZE = 4096\n",
    "CHUNK_OVERLAP = 1024\n",
    "BGE_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def generate_segment_embeddings(df, text_column='Text', segment_id_column='Segment_ID', batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate segment embeddings with A100 optimization and batching.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Generating segment embeddings for {df[segment_id_column].nunique()} segments...\")\n",
    "    \n",
    "    model = SentenceTransformer(BGE_MODEL_NAME, device=BGE_DEVICE)\n",
    "    if torch.cuda.is_available():\n",
    "        model.half()  # FP16 for A100\n",
    "    tokenizer = model.tokenizer\n",
    "    model.max_seq_length = TOKEN_LIMIT\n",
    "\n",
    "    # Aggregate texts by segment\n",
    "    segment_texts = df.groupby(segment_id_column)[text_column].apply(\n",
    "        lambda x: ' '.join(x.astype(str))\n",
    "    ).to_dict()\n",
    "    \n",
    "    # Process segments in batches\n",
    "    segment_ids = list(segment_texts.keys())\n",
    "    segment_embeddings = {}\n",
    "    \n",
    "    with tqdm(total=len(segment_ids), desc=\"üöÄ Embedding segments\", unit=\"segment\") as pbar:\n",
    "        for i in range(0, len(segment_ids), batch_size):\n",
    "            batch_ids = segment_ids[i:i+batch_size]\n",
    "            batch_texts = [segment_texts[seg_id] for seg_id in batch_ids]\n",
    "            \n",
    "            # Check which texts need chunking\n",
    "            short_texts = []\n",
    "            short_ids = []\n",
    "            long_ids = []\n",
    "            \n",
    "            for seg_id, text in zip(batch_ids, batch_texts):\n",
    "                token_count = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "                if token_count <= TOKEN_LIMIT:\n",
    "                    short_texts.append(text)\n",
    "                    short_ids.append(seg_id)\n",
    "                else:\n",
    "                    long_ids.append(seg_id)\n",
    "            \n",
    "            # Batch process short texts\n",
    "            if short_texts:\n",
    "                batch_embeddings = model.encode(\n",
    "                    short_texts,\n",
    "                    batch_size=min(64, len(short_texts)),  # A100 optimized\n",
    "                    convert_to_tensor=False,\n",
    "                    show_progress_bar=False  # Suppress internal progress bar\n",
    "                )\n",
    "                for seg_id, emb in zip(short_ids, batch_embeddings):\n",
    "                    segment_embeddings[seg_id] = emb\n",
    "            \n",
    "            # Process long texts individually\n",
    "            for seg_id in long_ids:\n",
    "                text = segment_texts[seg_id]\n",
    "                emb = embed_text_bge(text, model, tokenizer)\n",
    "                segment_embeddings[seg_id] = emb\n",
    "            \n",
    "            pbar.update(len(batch_ids))\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if i % (batch_size * 10) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "    # Map embeddings back to dataframe\n",
    "    df['Segment_Embeddings'] = df[segment_id_column].map(segment_embeddings)\n",
    "    print(f\"‚úì Segment embeddings mapped to all speeches.\")\n",
    "    return df\n",
    "\n",
    "print(\"‚úì A100-optimized embedding functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab1765",
   "metadata": {},
   "source": [
    "## Usage Summary\n",
    "\n",
    "### What This Notebook Does:\n",
    "1. **Loads one file** from Google Drive: `AT_original_complete.pkl`\n",
    "2. **Processes the data** to create topic modeling version\n",
    "3. **Runs dual embedding** approach with GPU acceleration:\n",
    "   - Speech-level embeddings for segmentation (`Speech_Embeddings`)\n",
    "   - Segment-level embeddings for each speech (`Segment_Embeddings`)\n",
    "   - Segment ID for each speech (`Segment_ID`)\n",
    "4. **Saves results** back to Google Drive as a single file\n",
    "\n",
    "### What You Need to Upload:\n",
    "- **Only one file**: `AT_original_complete.pkl`\n",
    "- **Upload location**: `MyDrive/thesis_data/AT_original_complete.pkl`\n",
    "\n",
    "### Performance on Colab GPU:\n",
    "- **Speech embeddings**: ~30-100 speeches/second\n",
    "- **Segment embeddings**: ~10-30 segments/second  \n",
    "- **Total time**: ~10-30 minutes for large datasets (vs hours on CPU)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download results** to your local machine\n",
    "2. **Use the final dataframe** for further analysis\n",
    "\n",
    "### Output File:\n",
    "- `AT_with_embeddings_final.pkl` - Original dataframe with three new columns:\n",
    "  - `Speech_Embeddings`\n",
    "  - `Segment_ID`\n",
    "  - `Segment_Embeddings`\n",
    "\n",
    "üéâ **Happy Embedding on Colab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98114534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN THE OPTIMIZED PIPELINE ===\n",
    "\n",
    "print(\"üöÄ Starting A100-optimized embedding pipeline...\")\n",
    "print(f\"üíª Using: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"üìä Processing {len(long_df)} speeches for segmentation\")\n",
    "\n",
    "# Clear any existing cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    # STEP 1: Generate speech-level embeddings with A100 optimization\n",
    "    print(\"\\nüîÑ Generating speech-level embeddings...\")\n",
    "    df_with_speech_embeddings = generate_speech_embeddings_for_segmentation(\n",
    "        long_df, \n",
    "        text_column='Text',\n",
    "        batch_size=64,  # A100 optimized batch size\n",
    "        checkpoint_freq=10000  # Checkpoint every 10k speeches\n",
    "    )\n",
    "    print(\"‚úÖ Speech-level embeddings generated!\")\n",
    "    \n",
    "    # Clear cache before segmentation\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # STEP 2: Segment speeches by similarity\n",
    "    print(\"\\nüîç Segmenting speeches by similarity...\")\n",
    "    df_segmented = segment_speeches_by_similarity(\n",
    "        df_with_speech_embeddings, window_size=3,\n",
    "        height_threshold=0.3, prominence_threshold=0.2,\n",
    "        distance_threshold=5\n",
    "    )\n",
    "    print(\"‚úÖ Segmentation complete!\")\n",
    "\n",
    "    # STEP 3: Assign short speeches to nearest segment (FIXED)\n",
    "    print(\"\\nüîÑ Assigning short speeches to segments...\")\n",
    "    def assign_short_speeches(short_df, segmented_df):\n",
    "        \"\"\"Assign short speeches to segments based on their original order within sittings.\"\"\"\n",
    "        assigned = []\n",
    "        for sitting_id, group in short_df.groupby('Sitting_ID'):\n",
    "            seg_group = segmented_df[segmented_df['Sitting_ID'] == sitting_id]\n",
    "            if seg_group.empty:\n",
    "                # If no segments in this sitting, create a default segment\n",
    "                default_segment = f\"{sitting_id}_seg_0\"\n",
    "                assigned.extend([default_segment] * len(group))\n",
    "                continue\n",
    "            \n",
    "            # Get unique segments for this sitting in order\n",
    "            segments_in_sitting = seg_group['Segment_ID'].unique()\n",
    "            \n",
    "            # For each short speech, assign to the first available segment\n",
    "            # This is a simple strategy - you could make it more sophisticated\n",
    "            for idx, row in group.iterrows():\n",
    "                # Assign to first segment (could be improved with better logic)\n",
    "                assigned.append(segments_in_sitting[0])\n",
    "        \n",
    "        short_df = short_df.copy()\n",
    "        short_df['Segment_ID'] = assigned\n",
    "        return short_df\n",
    "\n",
    "    short_df_assigned = assign_short_speeches(short_df, df_segmented)\n",
    "    df_all = pd.concat([df_segmented, short_df_assigned], ignore_index=True)\n",
    "\n",
    "    # STEP 4: Generate segment-level embeddings with A100 optimization\n",
    "    print(\"\\nüîÑ Generating segment-level embeddings...\")\n",
    "    df_final = generate_segment_embeddings(\n",
    "        df_all, \n",
    "        text_column='Text', \n",
    "        segment_id_column='Segment_ID',\n",
    "        batch_size=32  # A100 optimized for segments\n",
    "    )\n",
    "    print(\"‚úÖ Segment-level embeddings mapped!\")\n",
    "\n",
    "    # STEP 5: Save final output\n",
    "    output_path = f\"{data_folder}AT_with_embeddings_final.pkl\"\n",
    "    df_final.to_pickle(output_path)\n",
    "    print(f\"\\nüíæ Saved final dataframe: {output_path}\")\n",
    "    print(f\"üìä Final shape: {df_final.shape}\")\n",
    "    print(f\"üéØ Segments created: {df_final['Segment_ID'].nunique()}\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ Memory cleaned up\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in pipeline: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
