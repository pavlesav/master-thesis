{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c1b3f39",
      "metadata": {
        "id": "8c1b3f39"
      },
      "source": [
        "# Parlamint Data Processing Pipeline (Google Colab Version)\n",
        "\n",
        "‚ö° **Optimized for GPU execution on Google Colab**\n",
        "\n",
        "Automated pipeline for processing ParlaMint 5.0 data with GPU acceleration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cac5fa3",
      "metadata": {
        "id": "7cac5fa3"
      },
      "source": [
        "## Setup: Mount Google Drive & Install Packages\n",
        "\n",
        "**Before running:**\n",
        "1. Upload your data folder to Google Drive\n",
        "2. Update `BASE_DATA_DIR` path below to match your Drive location\n",
        "3. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9580821a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9580821a",
        "outputId": "57d26880-00e1-47d9-8aed-657057d27b5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted at /content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff46ea52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff46ea52",
        "outputId": "9201f525-3c21-43be-8d2c-41df392ca9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Packages installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q sentence-transformers scikit-learn\n",
        "\n",
        "print(\"‚úÖ Packages installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29710e55",
      "metadata": {
        "id": "29710e55"
      },
      "outputs": [],
      "source": [
        "# Verify GPU availability\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"‚úÖ GPU available: {gpu_name}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: No GPU detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3cbcab",
      "metadata": {
        "id": "4c3cbcab"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import warnings\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "BASE_DATA_DIR = \"/content/drive/MyDrive/thesis/data\"\n",
        "CHECKPOINT_DIR = os.path.join(BASE_DATA_DIR, \"checkpoints\")\n",
        "OUTPUT_DIR = os.path.join(BASE_DATA_DIR, \"processed\")\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "CONFIG = {\n",
        "    'AT': {\n",
        "        'name': 'Austria',\n",
        "        'bilingual': True,\n",
        "        'english_path': os.path.join(BASE_DATA_DIR, \"AT\", \"ParlaMint-AT-en.ana\", \"ParlaMint-AT-en.txt\"),\n",
        "        'native_path': os.path.join(BASE_DATA_DIR, \"AT\", \"ParlaMint-AT\", \"ParlaMint-AT.txt\"),\n",
        "        'native_keywords': ['tagesordnung', 'tagesordnungspunkt', 'punkt', 'verhandlung', 'behandlung', 'n√§chster', 'weiter', 'fortsetzen']\n",
        "    },\n",
        "    'HR': {\n",
        "        'name': 'Croatia',\n",
        "        'bilingual': True,\n",
        "        'english_path': os.path.join(BASE_DATA_DIR, \"HR\", \"ParlaMint-HR-en.ana\", \"ParlaMint-HR-en.txt\"),\n",
        "        'native_path': os.path.join(BASE_DATA_DIR, \"HR\", \"ParlaMint-HR\", \"ParlaMint-HR.txt\"),\n",
        "        'native_keywords': ['dnevni', 'red', 'toƒçka', 'taƒçka', 'sljedeƒái', 'sljedeƒáe', 'prijedlog', 'zakon', 'tema', 'nastavljamo', 'prelazimo']\n",
        "    },\n",
        "    'GB': {\n",
        "        'name': 'Great Britain',\n",
        "        'bilingual': False,\n",
        "        'english_path': os.path.join(BASE_DATA_DIR, \"GB\", \"ParlaMint-GB\", \"ParlaMint-GB.txt\"),\n",
        "        'native_keywords': None\n",
        "    }\n",
        "}\n",
        "\n",
        "ENGLISH_KEYWORDS = ['agenda', 'proceed', 'point', 'item', 'topic', 'next', 'following', 'move on']\n",
        "\n",
        "print(f\"‚úÖ Config loaded | Data: {BASE_DATA_DIR} | GPU batch: 128\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b6701b3",
      "metadata": {
        "id": "1b6701b3"
      },
      "source": [
        "## Step 1: Data Loading\n",
        "\n",
        "Load parliamentary speeches from year-based folder structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1569fddc",
      "metadata": {
        "id": "1569fddc"
      },
      "outputs": [],
      "source": [
        "def load_parlamint_data(parent_folder):\n",
        "    \"\"\"Load ParlaMint data from year folders.\"\"\"\n",
        "    if not os.path.exists(parent_folder):\n",
        "        return None\n",
        "    \n",
        "    df_list = []\n",
        "    year_folders = sorted([f for f in os.listdir(parent_folder) if os.path.isdir(os.path.join(parent_folder, f))])\n",
        "    \n",
        "    if not year_folders:\n",
        "        return None\n",
        "    \n",
        "    print(f\"  Loading {len(year_folders)} years: {year_folders[0]}-{year_folders[-1]}\")\n",
        "    \n",
        "    for year_folder in year_folders:\n",
        "        folder_path = os.path.join(parent_folder, year_folder)\n",
        "        meta_files = [f for f in os.listdir(folder_path) if f.endswith('-meta.tsv') and not f.endswith('-ana-meta.tsv')]\n",
        "        \n",
        "        for meta_file in meta_files:\n",
        "            base = meta_file.replace('-meta.tsv', '')\n",
        "            txt_path = os.path.join(folder_path, base + '.txt')\n",
        "            \n",
        "            if not os.path.exists(txt_path):\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                df_meta = pd.read_csv(os.path.join(folder_path, meta_file), sep='\\t', encoding='utf-8', index_col=False)\n",
        "                \n",
        "                text_map = {}\n",
        "                with open(txt_path, encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split('\\t', 1)\n",
        "                        if len(parts) == 2:\n",
        "                            text_map[parts[0]] = parts[1]\n",
        "                \n",
        "                df_meta['Text'] = df_meta['ID'].map(text_map)\n",
        "                df_meta = df_meta[df_meta['Text'].notnull() & (df_meta['Text'].str.strip() != '')]\n",
        "                \n",
        "                if len(df_meta) > 0:\n",
        "                    df_list.append(df_meta)\n",
        "            except Exception as e:\n",
        "                print(f\"    Error {meta_file}: {e}\")\n",
        "    \n",
        "    if not df_list:\n",
        "        return None\n",
        "    \n",
        "    df_all = pd.concat(df_list, ignore_index=True)\n",
        "    print(f\"  ‚úÖ {len(df_all):,} speeches\")\n",
        "    return df_all\n",
        "\n",
        "\n",
        "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step1_raw_data.pkl')\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    print(\"üìÇ Loading checkpoint...\")\n",
        "    with open(checkpoint_file, 'rb') as f:\n",
        "        raw_data = pickle.load(f)\n",
        "    \n",
        "    for code, df in raw_data.items():\n",
        "        has_en = 'Text_English' in df.columns and df['Text_English'].notna().any()\n",
        "        has_nat = 'Text_Native' in df.columns and df['Text_Native'].notna().any()\n",
        "        CONFIG[code]['mode'] = 'bilingual' if (has_en and has_nat) else ('english_only' if has_en else 'native_only')\n",
        "else:\n",
        "    print(\"üîÑ Loading data...\")\n",
        "    raw_data = {}\n",
        "    \n",
        "    for code, config in CONFIG.items():\n",
        "        print(f\"\\n{config['name']} ({code})\")\n",
        "        \n",
        "        df_english = load_parlamint_data(config['english_path'])\n",
        "        df_native = load_parlamint_data(config['native_path']) if config['bilingual'] else None\n",
        "        \n",
        "        if df_english is not None:\n",
        "            df = df_english.copy().rename(columns={'Text': 'Text_English'})\n",
        "            has_english = True\n",
        "        elif df_native is not None:\n",
        "            df = df_native.copy().rename(columns={'Text': 'Text_Native'})\n",
        "            has_english = False\n",
        "        else:\n",
        "            print(\"  ‚ùå No data\")\n",
        "            continue\n",
        "        \n",
        "        if has_english and df_native is not None:\n",
        "            df = df.merge(df_native[['ID', 'Text']].rename(columns={'Text': 'Text_Native'}), on='ID', how='left')\n",
        "            config['mode'] = 'bilingual'\n",
        "        elif has_english:\n",
        "            df['Text_Native'] = None\n",
        "            config['mode'] = 'english_only'\n",
        "        else:\n",
        "            df['Text_English'] = None\n",
        "            config['mode'] = 'native_only'\n",
        "        \n",
        "        raw_data[code] = df\n",
        "        print(f\"  ‚úÖ {config['mode'].upper()}: {len(df):,} speeches\")\n",
        "    \n",
        "    with open(checkpoint_file, 'wb') as f:\n",
        "        pickle.dump(raw_data, f)\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded: {list(raw_data.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c45b77",
      "metadata": {
        "id": "74c45b77"
      },
      "source": [
        "## Step 2: Speech Embeddings (GPU Optimized)\n",
        "\n",
        "Generate BGE-m3 embeddings with GPU acceleration.\n",
        "\n",
        "**GPU optimizations:**\n",
        "- Batch size: 128 (vs 16 on CPU)\n",
        "- Reduced garbage collection frequency\n",
        "- Checkpoint every 10% (optional - can disable for speed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ca2fda0",
      "metadata": {
        "id": "7ca2fda0"
      },
      "outputs": [],
      "source": [
        "def add_speech_embeddings(df, text_column, checkpoint_prefix=''):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    batch_size = 128 if device == \"cuda\" else 16\n",
        "    \n",
        "    model = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
        "    tokenizer = model.tokenizer\n",
        "    \n",
        "    MAX_TOKENS, CHUNK_SIZE, STRIDE = 8192, 8000, 6000\n",
        "    texts = df[text_column].astype(str).values\n",
        "    \n",
        "    # Check for partial checkpoint\n",
        "    partial_checkpoint = os.path.join(CHECKPOINT_DIR, f'{checkpoint_prefix}_partial.pkl')\n",
        "    if os.path.exists(partial_checkpoint):\n",
        "        with open(partial_checkpoint, 'rb') as f:\n",
        "            embeddings = pickle.load(f)\n",
        "        start_idx = len(embeddings)\n",
        "        print(f\"  üìÇ Resuming from {start_idx:,}/{len(texts):,} ({start_idx/len(texts)*100:.1f}%)\")\n",
        "    else:\n",
        "        embeddings = []\n",
        "        start_idx = 0\n",
        "    \n",
        "    checkpoint_interval = 5000  # Save every 5k speeches\n",
        "    last_checkpoint = start_idx\n",
        "    \n",
        "    with tqdm(total=len(texts), initial=start_idx, desc=f\"Embed {text_column}\", unit=\"speech\") as pbar:\n",
        "        for i in range(start_idx, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            batch_embeddings = []\n",
        "            \n",
        "            for text in batch_texts:\n",
        "                token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "                \n",
        "                if len(token_ids) <= MAX_TOKENS:\n",
        "                    emb = model.encode([text], convert_to_tensor=False, show_progress_bar=False)[0]\n",
        "                else:\n",
        "                    chunks = [tokenizer.decode(token_ids[start:min(start + CHUNK_SIZE, len(token_ids))], skip_special_tokens=True)\n",
        "                             for start in range(0, len(token_ids), STRIDE)]\n",
        "                    emb = np.mean(model.encode(chunks, convert_to_tensor=False, show_progress_bar=False), axis=0)\n",
        "                \n",
        "                batch_embeddings.append(emb)\n",
        "            \n",
        "            embeddings.extend(batch_embeddings)\n",
        "            pbar.update(len(batch_texts))\n",
        "            \n",
        "            # Save checkpoint every 5k speeches\n",
        "            if checkpoint_prefix and len(embeddings) - last_checkpoint >= checkpoint_interval:\n",
        "                with open(partial_checkpoint, 'wb') as f:\n",
        "                    pickle.dump(embeddings, f)\n",
        "                last_checkpoint = len(embeddings)\n",
        "    \n",
        "    # Clean up partial checkpoint when done\n",
        "    if checkpoint_prefix and os.path.exists(partial_checkpoint):\n",
        "        os.remove(partial_checkpoint)\n",
        "    \n",
        "    df_result = df.copy()\n",
        "    df_result['Speech_Embeddings'] = embeddings\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    return df_result\n",
        "\n",
        "\n",
        "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step2_speech_embeddings.pkl')\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    print(\"üìÇ Loading checkpoint...\")\n",
        "    with open(checkpoint_file, 'rb') as f:\n",
        "        processed_data = pickle.load(f)\n",
        "else:\n",
        "    processed_data = {}\n",
        "\n",
        "for idx, (code, df) in enumerate(raw_data.items(), 1):\n",
        "    config = CONFIG[code]\n",
        "    print(f\"\\n[{idx}/{len(raw_data)}] {config['name']} ({code})\")\n",
        "    \n",
        "    df_emb = processed_data.get(code, df.copy())\n",
        "    \n",
        "    # English embeddings\n",
        "    if 'Text_English' in df.columns and df['Text_English'].notna().any():\n",
        "        if not ('Speech_Embeddings_English' in df_emb.columns and df_emb['Speech_Embeddings_English'].notna().any()):\n",
        "            df_temp = add_speech_embeddings(df, 'Text_English', f'step2_{code}_en')\n",
        "            df_emb['Speech_Embeddings_English'] = df_temp['Speech_Embeddings']\n",
        "            \n",
        "            # Save immediately after English completes\n",
        "            processed_data[code] = df_emb\n",
        "            with open(checkpoint_file, 'wb') as f:\n",
        "                pickle.dump(processed_data, f)\n",
        "            print(f\"  ‚úÖ English: {df_emb['Speech_Embeddings_English'].notna().sum():,} (saved)\")\n",
        "        else:\n",
        "            print(f\"  ‚úÖ English: cached\")\n",
        "    else:\n",
        "        df_emb['Speech_Embeddings_English'] = None\n",
        "    \n",
        "    # Native embeddings\n",
        "    if 'Text_Native' in df.columns and df['Text_Native'].notna().any():\n",
        "        if not ('Speech_Embeddings_Native' in df_emb.columns and df_emb['Speech_Embeddings_Native'].notna().any()):\n",
        "            df_temp = add_speech_embeddings(df, 'Text_Native', f'step2_{code}_native')\n",
        "            df_emb['Speech_Embeddings_Native'] = df_temp['Speech_Embeddings']\n",
        "            \n",
        "            # Save immediately after Native completes\n",
        "            processed_data[code] = df_emb\n",
        "            with open(checkpoint_file, 'wb') as f:\n",
        "                pickle.dump(processed_data, f)\n",
        "            print(f\"  ‚úÖ Native: {df_emb['Speech_Embeddings_Native'].notna().sum():,} (saved)\")\n",
        "        else:\n",
        "            print(f\"  ‚úÖ Native: cached\")\n",
        "    else:\n",
        "        df_emb['Speech_Embeddings_Native'] = None\n",
        "    \n",
        "    processed_data[code] = df_emb\n",
        "\n",
        "# Final cleanup\n",
        "if len(processed_data) == len(raw_data):\n",
        "    step1_checkpoint = os.path.join(CHECKPOINT_DIR, 'step1_raw_data.pkl')\n",
        "    if os.path.exists(step1_checkpoint):\n",
        "        os.remove(step1_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cffc6398",
      "metadata": {
        "id": "cffc6398"
      },
      "source": [
        "## Step 3: Segmentation & Segment IDs\n",
        "\n",
        "Find segment boundaries using automatic parameter optimization and multi-signal detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f09a0bd7",
      "metadata": {
        "id": "f09a0bd7"
      },
      "outputs": [],
      "source": [
        "def detect_boundaries_by_keywords(texts, roles, keywords):\n",
        "    return [i for i, (text, role) in enumerate(zip(texts, roles))\n",
        "            if 'Chairperson' in str(role) and any(kw in str(text).lower() for kw in keywords)]\n",
        "\n",
        "\n",
        "def detect_boundaries_by_similarity(embeddings, window_size=1):\n",
        "    n = len(embeddings)\n",
        "    if n < window_size * 2 + 1:\n",
        "        return []\n",
        "    \n",
        "    similarity_drops = []\n",
        "    for i in range(window_size, n - window_size + 1):\n",
        "        window_before = embeddings[max(0, i - window_size):i]\n",
        "        window_after = embeddings[i:min(n, i + window_size)]\n",
        "        \n",
        "        if len(window_before) == 0 or len(window_after) == 0:\n",
        "            continue\n",
        "        \n",
        "        mean_before = np.mean(window_before, axis=0)\n",
        "        mean_after = np.mean(window_after, axis=0)\n",
        "        sim = cosine_similarity(mean_before.reshape(1, -1), mean_after.reshape(1, -1))[0][0]\n",
        "        similarity_drops.append((i, 1 - sim))\n",
        "    \n",
        "    if not similarity_drops:\n",
        "        return []\n",
        "    \n",
        "    threshold = np.percentile([d[1] for d in similarity_drops], 75)\n",
        "    return [pos for pos, drop in similarity_drops if drop >= threshold]\n",
        "\n",
        "\n",
        "def combine_boundaries(keyword_boundaries, similarity_boundaries, min_distance=3):\n",
        "    all_boundaries = set(keyword_boundaries)\n",
        "    for sim_b in similarity_boundaries:\n",
        "        if not any(abs(sim_b - kb) < min_distance for kb in keyword_boundaries):\n",
        "            all_boundaries.add(sim_b)\n",
        "    return sorted(all_boundaries)\n",
        "\n",
        "\n",
        "def evaluate_segmentation(embeddings, boundaries, texts, roles, keywords):\n",
        "    if len(boundaries) == 0:\n",
        "        return 0.0, {'error': 'No boundaries'}\n",
        "    \n",
        "    keyword_boundaries = detect_boundaries_by_keywords(texts, roles, keywords)\n",
        "    keyword_score = sum(1 for b in boundaries if b in keyword_boundaries) / len(boundaries)\n",
        "    \n",
        "    breaks = [0] + boundaries + [len(embeddings)]\n",
        "    coherence_scores, separation_scores = [], []\n",
        "    \n",
        "    for i in range(len(breaks) - 1):\n",
        "        segment = embeddings[breaks[i]:breaks[i+1]]\n",
        "        if len(segment) > 1:\n",
        "            coherence_scores.append(cosine_similarity(segment).mean())\n",
        "            if i < len(breaks) - 2:\n",
        "                next_segment = embeddings[breaks[i+1]:breaks[i+2]]\n",
        "                if len(next_segment) > 0:\n",
        "                    separation_scores.append(1 - cosine_similarity(segment, next_segment).mean())\n",
        "    \n",
        "    coherence = np.mean(coherence_scores) if coherence_scores else 0\n",
        "    separation = np.mean(separation_scores) if separation_scores else 0\n",
        "    semantic_score = (coherence + separation) / 2\n",
        "    \n",
        "    segment_lengths = [breaks[i+1] - breaks[i] for i in range(len(breaks) - 1)]\n",
        "    \n",
        "    return (keyword_score * 0.5 + semantic_score * 0.5), {\n",
        "        'keyword_score': keyword_score,\n",
        "        'semantic_score': semantic_score,\n",
        "        'coherence': coherence,\n",
        "        'separation': separation,\n",
        "        'avg_length': np.mean(segment_lengths),\n",
        "        'num_segments': len(segment_lengths),\n",
        "        'num_boundaries': len(boundaries)\n",
        "    }\n",
        "\n",
        "\n",
        "def optimize_window_size(embeddings, texts, roles, keywords):\n",
        "    results = []\n",
        "    keyword_boundaries = detect_boundaries_by_keywords(texts, roles, keywords)\n",
        "    \n",
        "    for window in range(1, 11):\n",
        "        similarity_boundaries = detect_boundaries_by_similarity(embeddings, window)\n",
        "        combined_boundaries = combine_boundaries(keyword_boundaries, similarity_boundaries)\n",
        "        \n",
        "        if len(combined_boundaries) == 0:\n",
        "            results.append({'window': window, 'score': 0.0})\n",
        "            continue\n",
        "        \n",
        "        score, stats = evaluate_segmentation(embeddings, combined_boundaries, texts, roles, keywords)\n",
        "        results.append({'window': window, 'score': score, **stats})\n",
        "    \n",
        "    valid_results = [r for r in results if r['score'] > 0]\n",
        "    if not valid_results:\n",
        "        return 5, 0.0, results\n",
        "    \n",
        "    best = max(valid_results, key=lambda x: x['score'])\n",
        "    print(f\"  üîç Optimal window={best['window']} (score={best['score']:.3f})\")\n",
        "    return best['window'], best['score'], results\n",
        "\n",
        "\n",
        "def create_segments(dataset, embedding_col, text_col, keywords, text_id_col='Text_ID'):\n",
        "    print(f\"\\n{'='*60}\\nSEGMENTING: {text_col}\\n{'='*60}\")\n",
        "    \n",
        "    # Optimize window size\n",
        "    all_sessions = dataset[text_id_col].unique()\n",
        "    sample_size = min(max(10, int(len(all_sessions) * 0.2)), 50)\n",
        "    sample_sessions = np.random.choice(all_sessions, sample_size, replace=False)\n",
        "    sample_data = dataset[dataset[text_id_col].isin(sample_sessions)]\n",
        "    \n",
        "    if len(sample_data) >= 50:\n",
        "        optimal_window, _, _ = optimize_window_size(\n",
        "            np.array(sample_data[embedding_col].tolist()),\n",
        "            sample_data[text_col].values,\n",
        "            sample_data['Speaker_role'].values,\n",
        "            keywords\n",
        "        )\n",
        "    else:\n",
        "        optimal_window = 5\n",
        "        print(f\"  ‚ö†Ô∏è Using default window=5\")\n",
        "    \n",
        "    # Segment all sessions\n",
        "    all_segments = []\n",
        "    stats = {'optimal_window': optimal_window, 'total_segments': 0, 'segment_lengths': []}\n",
        "    \n",
        "    for session_id in tqdm(dataset[text_id_col].unique(), desc=\"Segmenting\"):\n",
        "        session = dataset[dataset[text_id_col] == session_id].reset_index(drop=True)\n",
        "        \n",
        "        if len(session) < 5:\n",
        "            all_segments.append({\n",
        "                'Text_ID': session_id,\n",
        "                'Segment_ID': f\"{session_id}_seg_1\",\n",
        "                'Start_Index': 0,\n",
        "                'End_Index': len(session) - 1\n",
        "            })\n",
        "            stats['total_segments'] += 1\n",
        "            stats['segment_lengths'].append(len(session))\n",
        "            continue\n",
        "        \n",
        "        embeddings = np.array(session[embedding_col].tolist())\n",
        "        keyword_boundaries = detect_boundaries_by_keywords(session[text_col].values, session['Speaker_role'].values, keywords)\n",
        "        similarity_boundaries = detect_boundaries_by_similarity(embeddings, optimal_window)\n",
        "        combined_boundaries = combine_boundaries(keyword_boundaries, similarity_boundaries)\n",
        "        \n",
        "        breaks = [0] + combined_boundaries + [len(session)]\n",
        "        for seg_idx in range(len(breaks) - 1):\n",
        "            start, end = breaks[seg_idx], breaks[seg_idx + 1] - 1\n",
        "            stats['segment_lengths'].append(end - start + 1)\n",
        "            all_segments.append({\n",
        "                'Text_ID': session_id,\n",
        "                'Segment_ID': f\"{session_id}_seg_{seg_idx + 1}\",\n",
        "                'Start_Index': start,\n",
        "                'End_Index': end\n",
        "            })\n",
        "            stats['total_segments'] += 1\n",
        "    \n",
        "    stats['avg_length'] = np.mean(stats['segment_lengths'])\n",
        "    print(f\"  ‚úÖ {stats['total_segments']:,} segments | avg={stats['avg_length']:.1f} speeches/segment\")\n",
        "    \n",
        "    return all_segments, stats\n",
        "\n",
        "\n",
        "def add_segment_ids_to_df(df, segments, text_id_col='Text_ID'):\n",
        "    df = df.copy()\n",
        "    df['Segment_ID'] = None\n",
        "    \n",
        "    for seg in segments:\n",
        "        mask = df[text_id_col] == seg['Text_ID']\n",
        "        indices = df[mask].index\n",
        "        if len(indices) > seg['Start_Index']:\n",
        "            df.loc[indices[seg['Start_Index']:seg['End_Index']+1], 'Segment_ID'] = seg['Segment_ID']\n",
        "    \n",
        "    missing_mask = df['Segment_ID'].isna()\n",
        "    if missing_mask.any():\n",
        "        df.loc[missing_mask, 'Segment_ID'] = df.loc[missing_mask, text_id_col] + '_seg_0'\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step3_segmentation.pkl')\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    print(\"üìÇ Loading checkpoint...\")\n",
        "    with open(checkpoint_file, 'rb') as f:\n",
        "        final_data = pickle.load(f)\n",
        "else:\n",
        "    with open(os.path.join(CHECKPOINT_DIR, 'step2_speech_embeddings.pkl'), 'rb') as f:\n",
        "        processed_data = pickle.load(f)\n",
        "    \n",
        "    final_data = {}\n",
        "    \n",
        "    for idx, (code, df) in enumerate(processed_data.items(), 1):\n",
        "        config = CONFIG[code]\n",
        "        print(f\"\\n[{idx}/{len(processed_data)}] {config['name']} ({code})\")\n",
        "        \n",
        "        df_final = df.copy()\n",
        "        \n",
        "        if 'Speech_Embeddings_English' in df.columns and df['Speech_Embeddings_English'].notna().any():\n",
        "            segments_en, stats_en = create_segments(df, 'Speech_Embeddings_English', 'Text_English', ENGLISH_KEYWORDS)\n",
        "            df_final['Segment_ID_English'] = add_segment_ids_to_df(df, segments_en)['Segment_ID']\n",
        "            optimal_window = stats_en['optimal_window']\n",
        "        else:\n",
        "            df_final['Segment_ID_English'] = None\n",
        "            optimal_window = 5\n",
        "        \n",
        "        if 'Speech_Embeddings_Native' in df.columns and df['Speech_Embeddings_Native'].notna().any():\n",
        "            print(f\"  Using window={optimal_window} for Native\")\n",
        "            segments_native, _ = create_segments(df, 'Speech_Embeddings_Native', 'Text_Native', config.get('native_keywords', ENGLISH_KEYWORDS))\n",
        "            df_final['Segment_ID_Native'] = add_segment_ids_to_df(df, segments_native)['Segment_ID']\n",
        "        else:\n",
        "            df_final['Segment_ID_Native'] = None\n",
        "        \n",
        "        final_data[code] = df_final\n",
        "        \n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump(final_data, f)\n",
        "    \n",
        "    step2_checkpoint = os.path.join(CHECKPOINT_DIR, 'step2_speech_embeddings.pkl')\n",
        "    if os.path.exists(step2_checkpoint):\n",
        "        os.remove(step2_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18da15c1",
      "metadata": {
        "id": "18da15c1"
      },
      "source": [
        "## Step 4: Segment Embeddings (GPU Optimized)\n",
        "\n",
        "Generate embeddings for segments with GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cebd8725",
      "metadata": {
        "id": "cebd8725"
      },
      "outputs": [],
      "source": [
        "def add_segment_embeddings(df, text_col, segment_col):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    batch_size = 128 if device == \"cuda\" else 16\n",
        "    \n",
        "    model = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
        "    tokenizer = model.tokenizer\n",
        "    MAX_TOKENS, CHUNK_SIZE, STRIDE = 8192, 8000, 6000\n",
        "    \n",
        "    segment_texts = df.groupby(segment_col)[text_col].apply(lambda x: ' '.join(x.astype(str)))\n",
        "    texts = segment_texts.tolist()\n",
        "    embeddings = []\n",
        "    \n",
        "    with tqdm(total=len(texts), desc=f\"Segment Embed\", unit=\"seg\") as pbar:\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            batch_emb = []\n",
        "            \n",
        "            for text in batch:\n",
        "                tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "                if len(tokens) <= MAX_TOKENS:\n",
        "                    emb = model.encode([text], convert_to_tensor=False, show_progress_bar=False)[0]\n",
        "                else:\n",
        "                    chunks = [tokenizer.decode(tokens[start:min(start + CHUNK_SIZE, len(tokens))], skip_special_tokens=True)\n",
        "                             for start in range(0, len(tokens), STRIDE)]\n",
        "                    emb = np.mean(model.encode(chunks, convert_to_tensor=False, show_progress_bar=False), axis=0)\n",
        "                batch_emb.append(emb)\n",
        "            \n",
        "            embeddings.extend(batch_emb)\n",
        "            pbar.update(len(batch))\n",
        "    \n",
        "    emb_map = dict(zip(segment_texts.index.tolist(), embeddings))\n",
        "    df = df.copy()\n",
        "    df[f'Segment_Embeddings_{text_col}'] = df[segment_col].map(emb_map)\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step4_segment_embeddings.pkl')\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    print(\"üìÇ Loading checkpoint...\")\n",
        "    with open(checkpoint_file, 'rb') as f:\n",
        "        final_data = pickle.load(f)\n",
        "else:\n",
        "    with open(os.path.join(CHECKPOINT_DIR, 'step3_segmentation.pkl'), 'rb') as f:\n",
        "        final_data = pickle.load(f)\n",
        "\n",
        "for idx, (code, df) in enumerate(final_data.items(), 1):\n",
        "    config = CONFIG[code]\n",
        "    print(f\"\\n[{idx}/{len(final_data)}] {config['name']} ({code})\")\n",
        "    \n",
        "    # English segment embeddings\n",
        "    if 'Segment_ID_English' in df.columns and df['Segment_ID_English'].notna().any():\n",
        "        if not ('Segment_Embeddings_English' in df.columns and df['Segment_Embeddings_English'].notna().any()):\n",
        "            df = add_segment_embeddings(df, 'Text_English', 'Segment_ID_English')\n",
        "            df = df.rename(columns={'Segment_Embeddings_Text_English': 'Segment_Embeddings_English'})\n",
        "            print(f\"  ‚úÖ English: {df['Segment_Embeddings_English'].notna().sum():,}\")\n",
        "        else:\n",
        "            print(f\"  ‚úÖ English: cached\")\n",
        "    \n",
        "    # Native segment embeddings\n",
        "    if 'Segment_ID_Native' in df.columns and df['Segment_ID_Native'].notna().any():\n",
        "        if not ('Segment_Embeddings_Native' in df.columns and df['Segment_Embeddings_Native'].notna().any()):\n",
        "            df = add_segment_embeddings(df, 'Text_Native', 'Segment_ID_Native')\n",
        "            df = df.rename(columns={'Segment_Embeddings_Text_Native': 'Segment_Embeddings_Native'})\n",
        "            print(f\"  ‚úÖ Native: {df['Segment_Embeddings_Native'].notna().sum():,}\")\n",
        "        else:\n",
        "            print(f\"  ‚úÖ Native: cached\")\n",
        "    \n",
        "    final_data[code] = df\n",
        "    \n",
        "    with open(checkpoint_file, 'wb') as f:\n",
        "        pickle.dump(final_data, f)\n",
        "\n",
        "step3_checkpoint = os.path.join(CHECKPOINT_DIR, 'step3_segmentation.pkl')\n",
        "if os.path.exists(step3_checkpoint):\n",
        "    os.remove(step3_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a9bda5",
      "metadata": {
        "id": "96a9bda5"
      },
      "source": [
        "## Final Verification & Save\n",
        "\n",
        "Verify all processed data and save to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21926f14",
      "metadata": {
        "id": "21926f14"
      },
      "outputs": [],
      "source": [
        "print(\"üìä FINAL VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for code, df in final_data.items():\n",
        "    config = CONFIG[code]\n",
        "    mode = config.get('mode', 'unknown').upper()\n",
        "    print(f\"\\n{config['name']} ({code}) - {mode}:\")\n",
        "    print(f\"  Speeches: {len(df):,}\")\n",
        "    print(f\"  Sessions: {df['Text_ID'].nunique():,}\")\n",
        "\n",
        "    has_english_emb = 'Speech_Embeddings_English' in df.columns and df['Speech_Embeddings_English'].notna().any()\n",
        "    has_native_emb = 'Speech_Embeddings_Native' in df.columns and df['Speech_Embeddings_Native'].notna().any()\n",
        "\n",
        "    if has_english_emb:\n",
        "        sample_emb = df[df['Speech_Embeddings_English'].notna()]['Speech_Embeddings_English'].iloc[0]\n",
        "        print(f\"  ‚úÖ English speech embeddings: {sample_emb.shape}\")\n",
        "    if has_native_emb:\n",
        "        sample_emb = df[df['Speech_Embeddings_Native'].notna()]['Speech_Embeddings_Native'].iloc[0]\n",
        "        print(f\"  ‚úÖ Native speech embeddings: {sample_emb.shape}\")\n",
        "\n",
        "    if 'Segment_ID_English' in df.columns and df['Segment_ID_English'].notna().any():\n",
        "        print(f\"  ‚úÖ English segments: {df['Segment_ID_English'].nunique():,}\")\n",
        "    if 'Segment_ID_Native' in df.columns and df['Segment_ID_Native'].notna().any():\n",
        "        print(f\"  ‚úÖ Native segments: {df['Segment_ID_Native'].nunique():,}\")\n",
        "\n",
        "print(f\"\\n‚úÖ All processing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "764ff3fd",
      "metadata": {
        "id": "764ff3fd"
      },
      "outputs": [],
      "source": [
        "print(\"üíæ SAVING FINAL DATA\\n\" + \"=\"*60)\n",
        "\n",
        "for code, df in final_data.items():\n",
        "    output_path = os.path.join(OUTPUT_DIR, f\"{code}_speeches_processed.pkl\")\n",
        "    df.to_pickle(output_path)\n",
        "    \n",
        "    n_seg = df['Segment_ID_English'].nunique() if 'Segment_ID_English' in df.columns else 0\n",
        "    print(f\"‚úÖ {CONFIG[code]['name']}: {len(df):,} speeches | {n_seg:,} segments\")\n",
        "\n",
        "step4_checkpoint = os.path.join(CHECKPOINT_DIR, 'step4_segment_embeddings.pkl')\n",
        "if os.path.exists(step4_checkpoint):\n",
        "    os.remove(step4_checkpoint)\n",
        "\n",
        "print(f\"\\n{'='*60}\\n‚úÖ COMPLETE\\n{'='*60}\\nüìÅ {OUTPUT_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
