{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e08b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded data: (231752, 30)\n",
      "Columns: ['Sitting_ID', 'Speech_ID', 'Title', 'Date', 'Body', 'Term', 'Session', 'Meeting', 'Sitting', 'Agenda', 'Subcorpus', 'Lang', 'Speaker_role', 'Speaker_MP', 'Speaker_minister', 'Speaker_party', 'Speaker_party_name', 'Party_status', 'Party_orientation', 'Speaker_ID', 'Speaker_name', 'Speaker_gender', 'Speaker_birth', 'Text', 'Word_Count', 'Is_Too_Short', 'Is_Filtered', 'Speech_Embeddings', 'Segment_ID', 'Segment_Embeddings']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Load the data with embeddings\n",
    "embeddings = pd.read_pickle(r\"data folder\\data\\AT_with_embeddings_final.pkl\")\n",
    "\n",
    "print(f\"‚úÖ Loaded data: {embeddings.shape}\")\n",
    "print(f\"Columns: {list(embeddings.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "737eceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data Overview:\n",
      "  ‚Ä¢ Total speeches: 231,752\n",
      "  ‚Ä¢ Speech embedding shape: (1024,)\n",
      "  ‚Ä¢ Segment embedding shape: (1024,)\n",
      "  ‚Ä¢ Unique segments: 5,728\n",
      "  ‚Ä¢ Average speeches per segment: 40.5\n",
      "\n",
      "üîç Missing values:\n",
      "  ‚Ä¢ Segment_ID: 0\n",
      "  ‚Ä¢ Speech_Embeddings: 41119\n",
      "  ‚Ä¢ Segment_Embeddings: 0\n",
      "\n",
      "üìà Sitting length distribution:\n",
      "  ‚Ä¢ Min speeches per sitting: 1\n",
      "  ‚Ä¢ Max speeches per sitting: 1378\n",
      "  ‚Ä¢ Average speeches per sitting: 189.8\n",
      "  ‚Ä¢ Sittings with <50 speeches: 474\n",
      "  ‚Ä¢ Sittings with >200 speeches: 542\n",
      "\n",
      "üìà Sitting length distribution:\n",
      "  ‚Ä¢ Min speeches per sitting: 1\n",
      "  ‚Ä¢ Max speeches per sitting: 1378\n",
      "  ‚Ä¢ Average speeches per sitting: 189.8\n",
      "  ‚Ä¢ Sittings with <50 speeches: 474\n",
      "  ‚Ä¢ Sittings with >200 speeches: 542\n"
     ]
    }
   ],
   "source": [
    "# === DATA OVERVIEW ===\n",
    "print(\"üìä Data Overview:\")\n",
    "print(f\"  ‚Ä¢ Total speeches: {embeddings.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Speech embedding shape: {embeddings['Speech_Embeddings'][0].shape}\")\n",
    "print(f\"  ‚Ä¢ Segment embedding shape: {embeddings['Segment_Embeddings'][0].shape}\")\n",
    "print(f\"  ‚Ä¢ Unique segments: {embeddings['Segment_ID'].nunique():,}\")\n",
    "print(f\"  ‚Ä¢ Average speeches per segment: {embeddings.shape[0] / embeddings['Segment_ID'].nunique():.1f}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nüîç Missing values:\")\n",
    "print(f\"  ‚Ä¢ Segment_ID: {embeddings['Segment_ID'].isna().sum()}\")\n",
    "print(f\"  ‚Ä¢ Speech_Embeddings: {embeddings['Speech_Embeddings'].isna().sum()}\")\n",
    "print(f\"  ‚Ä¢ Segment_Embeddings: {embeddings['Segment_Embeddings'].isna().sum()}\")\n",
    "\n",
    "# Check sitting length distribution\n",
    "sitting_lengths = embeddings.groupby('Sitting_ID').size()\n",
    "print(f\"\\nüìà Sitting length distribution:\")\n",
    "print(f\"  ‚Ä¢ Min speeches per sitting: {sitting_lengths.min()}\")\n",
    "print(f\"  ‚Ä¢ Max speeches per sitting: {sitting_lengths.max()}\")\n",
    "print(f\"  ‚Ä¢ Average speeches per sitting: {sitting_lengths.mean():.1f}\")\n",
    "print(f\"  ‚Ä¢ Sittings with <50 speeches: {(sitting_lengths < 50).sum()}\")\n",
    "print(f\"  ‚Ä¢ Sittings with >200 speeches: {(sitting_lengths > 200).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d46b6c",
   "metadata": {},
   "source": [
    "## Enhanced Segmentation with Domain Knowledge\n",
    "\n",
    "Improved segmentation considering that Austrian parliament sittings can vary from 1-10 agendas (average 5-6).\n",
    "We'll implement adaptive segmentation that adjusts to sitting length and uses domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7faf133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Enhanced segmentation functions loaded\n"
     ]
    }
   ],
   "source": [
    "# === ENHANCED SEGMENTATION FUNCTIONS ===\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def calculate_adaptive_windowed_similarity(embeddings_list, base_window=3, speaker_roles=None):\n",
    "    \"\"\"Enhanced similarity calculation with adaptive windowing.\"\"\"\n",
    "    if len(embeddings_list) < 2:\n",
    "        return np.array([])\n",
    "    \n",
    "    num_utterances = len(embeddings_list)\n",
    "    similarities = []\n",
    "    \n",
    "    for g in range(num_utterances - 1):\n",
    "        # Adaptive window based on speaker role\n",
    "        if speaker_roles and g < len(speaker_roles):\n",
    "            window_size = max(1, base_window - 1) if speaker_roles[g] == 'chairperson' else base_window\n",
    "        else:\n",
    "            window_size = base_window\n",
    "            \n",
    "        # Calculate windows\n",
    "        start_before = max(0, g - window_size + 1)\n",
    "        end_before = g + 1\n",
    "        window_before = embeddings_list[start_before:end_before]\n",
    "        \n",
    "        start_after = g + 1\n",
    "        end_after = min(num_utterances, g + 1 + window_size)\n",
    "        window_after = embeddings_list[start_after:end_after]\n",
    "        \n",
    "        if not window_before or not window_after:\n",
    "            similarities.append(0)\n",
    "            continue\n",
    "            \n",
    "        # Calculate similarity\n",
    "        mean_before = np.mean([np.asarray(e) for e in window_before], axis=0)\n",
    "        mean_after = np.mean([np.asarray(e) for e in window_after], axis=0)\n",
    "        sim = cosine_similarity(mean_before.reshape(1, -1), mean_after.reshape(1, -1))[0][0]\n",
    "        similarities.append(sim)\n",
    "        \n",
    "    return np.array(similarities)\n",
    "\n",
    "def find_adaptive_boundaries(similarities, sitting_length, min_segments=1, max_segments=10):\n",
    "    \"\"\"\n",
    "    Find boundaries with adaptive target based on sitting length.\n",
    "    Short sittings (1-2 agendas), medium (3-8), long (9-10+).\n",
    "    \"\"\"\n",
    "    if len(similarities) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Adaptive target based on sitting length\n",
    "    if sitting_length < 50:\n",
    "        target_segments = max(1, min(3, sitting_length // 15))  # 1-3 segments for short sittings\n",
    "        height_range = (0.2, 0.4)\n",
    "        prominence_range = (0.15, 0.3)\n",
    "    elif sitting_length < 150:\n",
    "        target_segments = max(3, min(7, sitting_length // 25))  # 3-7 segments for medium sittings  \n",
    "        height_range = (0.15, 0.35)\n",
    "        prominence_range = (0.1, 0.25)\n",
    "    else:\n",
    "        target_segments = max(5, min(10, sitting_length // 30))  # 5-10 segments for long sittings\n",
    "        height_range = (0.1, 0.3)\n",
    "        prominence_range = (0.08, 0.2)\n",
    "    \n",
    "    target_segments = max(min_segments, min(target_segments, max_segments))\n",
    "    \n",
    "    inverted_similarities = np.maximum(0, 1 - similarities)\n",
    "    best_boundaries = np.array([])\n",
    "    best_score = float('inf')\n",
    "    \n",
    "    # Grid search for optimal parameters\n",
    "    for height in np.linspace(height_range[0], height_range[1], 4):\n",
    "        for prominence in np.linspace(prominence_range[0], prominence_range[1], 4):\n",
    "            for distance in [2, 3, 5, 8]:\n",
    "                peaks, _ = find_peaks(\n",
    "                    inverted_similarities,\n",
    "                    height=height,\n",
    "                    prominence=prominence,\n",
    "                    distance=distance\n",
    "                )\n",
    "                \n",
    "                num_segments = len(peaks) + 1\n",
    "                segment_score = abs(num_segments - target_segments)\n",
    "                \n",
    "                if segment_score < best_score:\n",
    "                    best_score = segment_score;\n",
    "                    best_boundaries = peaks;\n",
    "                    \n",
    "    return best_boundaries\n",
    "\n",
    "def calculate_segmentation_coherence(embeddings_list, segment_boundaries):\n",
    "    \"\"\"Calculate coherence metrics for segmentation quality.\"\"\"\n",
    "    if len(embeddings_list) < 4 or len(segment_boundaries) == 0:\n",
    "        return {'intra_similarity': 0, 'inter_similarity': 0, 'silhouette': 0, 'coherence_ratio': 0}\n",
    "    \n",
    "    # Create segment labels\n",
    "    segment_labels = np.zeros(len(embeddings_list))\n",
    "    current_segment = 0\n",
    "    \n",
    "    for i in range(len(embeddings_list)):\n",
    "        if i > 0 and (i - 1) in segment_boundaries:\n",
    "            current_segment += 1\n",
    "        segment_labels[i] = current_segment\n",
    "    \n",
    "    embeddings_array = np.array([np.asarray(e) for e in embeddings_list])\n",
    "    \n",
    "    # Intra-segment similarity (higher is better)\n",
    "    intra_similarities = []\n",
    "    for seg_id in np.unique(segment_labels):\n",
    "        seg_embeddings = embeddings_array[segment_labels == seg_id]\n",
    "        if len(seg_embeddings) > 1:\n",
    "            seg_sim_matrix = cosine_similarity(seg_embeddings)\n",
    "            mask = np.triu(np.ones_like(seg_sim_matrix), k=1) == 1\n",
    "            intra_similarities.append(seg_sim_matrix[mask].mean())\n",
    "    \n",
    "    avg_intra = np.mean(intra_similarities) if intra_similarities else 0\n",
    "    \n",
    "    # Inter-segment similarity (lower is better)\n",
    "    inter_similarities = []\n",
    "    unique_segments = np.unique(segment_labels)\n",
    "    for i in range(len(unique_segments)):\n",
    "        for j in range(i + 1, len(unique_segments)):\n",
    "            seg1 = embeddings_array[segment_labels == unique_segments[i]]\n",
    "            seg2 = embeddings_array[segment_labels == unique_segments[j]]\n",
    "            \n",
    "            if len(seg1) > 0 and len(seg2) > 0:\n",
    "                seg1_center = np.mean(seg1, axis=0)\n",
    "                seg2_center = np.mean(seg2, axis=0)\n",
    "                inter_sim = cosine_similarity(seg1_center.reshape(1, -1), \n",
    "                                            seg2_center.reshape(1, -1))[0][0]\n",
    "                inter_similarities.append(inter_sim)\n",
    "    \n",
    "    avg_inter = np.mean(inter_similarities) if inter_similarities else 0\n",
    "    \n",
    "    # Silhouette score\n",
    "    silhouette = 0\n",
    "    if len(unique_segments) > 1 and len(embeddings_array) > len(unique_segments):\n",
    "        try:\n",
    "            silhouette = silhouette_score(embeddings_array, segment_labels, metric='cosine')\n",
    "        except:\n",
    "            silhouette = 0\n",
    "    \n",
    "    return {\n",
    "        'intra_similarity': avg_intra,\n",
    "        'inter_similarity': avg_inter,\n",
    "        'silhouette': silhouette,\n",
    "        'coherence_ratio': avg_intra - avg_inter if avg_inter > 0 else 0\n",
    "    }\n",
    "\n",
    "print(\"‚úì Enhanced segmentation functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "645662bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running adaptive segmentation...\n",
      "üèõÔ∏è Running adaptive segmentation for Austrian Parliament\n",
      "üìä Targeting 1-10 segments per sitting based on length\n",
      "üèõÔ∏è Running adaptive segmentation for Austrian Parliament\n",
      "üìä Targeting 1-10 segments per sitting based on length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Adaptive segmenting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1221/1221 [03:00<00:00,  6.75sitting/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Adaptive segmentation complete!\n",
      "üìä Overall Statistics:\n",
      "  ‚Ä¢ Total segments: 5952\n",
      "  ‚Ä¢ Avg segments per sitting: 4.87\n",
      "  ‚Ä¢ Coherence ratio: -0.202\n",
      "\n",
      "üìà By Sitting Category:\n",
      "  ‚Ä¢ Short sittings (<50, 50-150, >150 speeches):\n",
      "    - Count: 454\n",
      "    - Avg segments: 1.0\n",
      "    - Avg length: 5 speeches\n",
      "  ‚Ä¢ Medium sittings (<50, 50-150, >150 speeches):\n",
      "    - Count: 180\n",
      "    - Avg segments: 3.4\n",
      "    - Avg length: 83 speeches\n",
      "  ‚Ä¢ Long sittings (<50, 50-150, >150 speeches):\n",
      "    - Count: 556\n",
      "    - Avg segments: 8.7\n",
      "    - Avg length: 311 speeches\n"
     ]
    }
   ],
   "source": [
    "# === ADAPTIVE SEGMENTATION IMPLEMENTATION ===\n",
    "\n",
    "def segment_speeches_adaptive(df, base_window=3):\n",
    "    \"\"\"\n",
    "    Adaptive segmentation that adjusts to sitting length (1-10 agendas expected).\n",
    "    \"\"\"\n",
    "    print(\"üèõÔ∏è Running adaptive segmentation for Austrian Parliament\")\n",
    "    print(\"üìä Targeting 1-10 segments per sitting based on length\")\n",
    "    \n",
    "    df_segmented = df.copy()\n",
    "    segment_ids = []\n",
    "    all_coherence_metrics = []\n",
    "    boundary_stats = []\n",
    "    \n",
    "    sittings = list(df_segmented.groupby('Sitting_ID'))\n",
    "    \n",
    "    for sitting_id, group in tqdm(sittings, desc=\"üîç Adaptive segmenting\", unit=\"sitting\"):\n",
    "        sitting_length = len(group)\n",
    "        \n",
    "        if sitting_length < 3:  # Very short sittings\n",
    "            segment_ids.extend([f\"{sitting_id}_seg_0\"] * len(group))\n",
    "            boundary_stats.append({\n",
    "                'sitting_id': sitting_id,\n",
    "                'sitting_length': sitting_length,\n",
    "                'boundaries_found': 0,\n",
    "                'segments_created': 1,\n",
    "                'category': 'very_short'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Prepare speaker information\n",
    "        speaker_roles = []\n",
    "        for _, row in group.iterrows():\n",
    "            role = 'chairperson' if not row.get('Speaker_MP', True) else 'mp'\n",
    "            speaker_roles.append(role)\n",
    "        \n",
    "        embeddings_list = group['Speech_Embeddings'].tolist()\n",
    "        \n",
    "        # Calculate similarities with speaker awareness\n",
    "        similarities = calculate_adaptive_windowed_similarity(\n",
    "            embeddings_list, \n",
    "            base_window=base_window,\n",
    "            speaker_roles=speaker_roles\n",
    "        )\n",
    "        \n",
    "        if len(similarities) == 0:\n",
    "            segment_ids.extend([f\"{sitting_id}_seg_0\"] * len(group))\n",
    "            continue\n",
    "        \n",
    "        # Find adaptive boundaries\n",
    "        boundaries = find_adaptive_boundaries(similarities, sitting_length)\n",
    "        \n",
    "        # Calculate coherence metrics\n",
    "        coherence = calculate_segmentation_coherence(embeddings_list, boundaries)\n",
    "        all_coherence_metrics.append(coherence)\n",
    "        \n",
    "        # Assign segment IDs\n",
    "        current_segment = 0\n",
    "        sitting_segment_ids = []\n",
    "        \n",
    "        for i in range(len(group)):\n",
    "            if i > 0 and (i - 1) in boundaries:\n",
    "                current_segment += 1\n",
    "            sitting_segment_ids.append(f\"{sitting_id}_seg_{current_segment}\")\n",
    "        \n",
    "        segment_ids.extend(sitting_segment_ids)\n",
    "        \n",
    "        # Categorize sitting by length\n",
    "        if sitting_length < 50:\n",
    "            category = 'short'\n",
    "        elif sitting_length < 150:\n",
    "            category = 'medium'\n",
    "        else:\n",
    "            category = 'long'\n",
    "            \n",
    "        boundary_stats.append({\n",
    "            'sitting_id': sitting_id,\n",
    "            'sitting_length': sitting_length,\n",
    "            'boundaries_found': len(boundaries),\n",
    "            'segments_created': len(set(sitting_segment_ids)),\n",
    "            'category': category\n",
    "        })\n",
    "    \n",
    "    df_segmented['Segment_ID_Enhanced'] = segment_ids\n",
    "    \n",
    "    # Print comprehensive statistics\n",
    "    stats_df = pd.DataFrame(boundary_stats)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Adaptive segmentation complete!\")\n",
    "    print(f\"üìä Overall Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total segments: {df_segmented['Segment_ID_Enhanced'].nunique()}\")\n",
    "    print(f\"  ‚Ä¢ Avg segments per sitting: {stats_df['segments_created'].mean():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Coherence ratio: {np.mean([m['coherence_ratio'] for m in all_coherence_metrics]):.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìà By Sitting Category:\")\n",
    "    for category in ['short', 'medium', 'long']:\n",
    "        cat_stats = stats_df[stats_df['category'] == category]\n",
    "        if len(cat_stats) > 0:\n",
    "            print(f\"  ‚Ä¢ {category.capitalize()} sittings (<50, 50-150, >150 speeches):\")\n",
    "            print(f\"    - Count: {len(cat_stats)}\")\n",
    "            print(f\"    - Avg segments: {cat_stats['segments_created'].mean():.1f}\")\n",
    "            print(f\"    - Avg length: {cat_stats['sitting_length'].mean():.0f} speeches\")\n",
    "    \n",
    "    return df_segmented, all_coherence_metrics, boundary_stats\n",
    "\n",
    "# Apply adaptive segmentation to long speeches only\n",
    "print(\"üöÄ Running adaptive segmentation...\")\n",
    "long_speeches_df = embeddings[~embeddings['Is_Too_Short']].copy()\n",
    "segmented_df, coherence_metrics, boundary_stats = segment_speeches_adaptive(long_speeches_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dda7f9",
   "metadata": {},
   "source": [
    "## BERTopic Configuration and Training\n",
    "\n",
    "Configure BERTopic with Austrian parliament-specific stop words and optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fb852a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 12:26:04,081 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Topic modeling data prepared:\n",
      "  ‚Ä¢ Segments for modeling: 5952\n",
      "  ‚Ä¢ Embedding dimension: 1024\n",
      "üöÄ Training BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 12:26:24,933 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2025-09-19 12:26:24,935 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-19 12:26:24,935 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-09-19 12:26:25,089 - BERTopic - Cluster - Completed ‚úì\n",
      "2025-09-19 12:26:25,090 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-09-19 12:26:25,089 - BERTopic - Cluster - Completed ‚úì\n",
      "2025-09-19 12:26:25,090 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-09-19 12:29:50,017 - BERTopic - Representation - Completed ‚úì\n",
      "2025-09-19 12:29:50,179 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-09-19 12:29:50,184 - BERTopic - Topic reduction - Number of topics (15) is equal or higher than the clustered topics(15).\n",
      "2025-09-19 12:29:50,186 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-19 12:29:50,017 - BERTopic - Representation - Completed ‚úì\n",
      "2025-09-19 12:29:50,179 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-09-19 12:29:50,184 - BERTopic - Topic reduction - Number of topics (15) is equal or higher than the clustered topics(15).\n",
      "2025-09-19 12:29:50,186 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-09-19 12:33:41,984 - BERTopic - Representation - Completed ‚úì\n",
      "2025-09-19 12:33:41,984 - BERTopic - Representation - Completed ‚úì\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Topics created: 15\n"
     ]
    }
   ],
   "source": [
    "# === BERTOPIC SETUP ===\n",
    "from bertopic import BERTopic\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from umap import UMAP\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Austrian parliament-specific stop words\n",
    "custom_stopwords = [\n",
    "    'mr', 'mrs', 'ms', 'dr', 'madam', 'honourable', 'member', 'members', 'vp', 'sp', 'fp', \n",
    "    'minister', 'speaker', 'deputy', 'president', 'chairman', 'chair', 'schilling', \n",
    "    'secretary', 'lord', 'lady', 'question', 'order', 'point', 'debate', 'motion', 'amendment',\n",
    "    'congratulations', 'congratulate', 'thanks', 'thank', 'say', 'one', 'want', 'know', 'think', \n",
    "    'believe', 'see', 'go', 'come', 'give', 'take', 'people', 'federal', 'government', 'austria', \n",
    "    'austrian', 'committee', 'call', 'said', 'already', 'please', 'request', 'proceed', 'reading',\n",
    "    'course', 'welcome', 'council', 'open', 'written', 'contain', 'items', 'item', 'yes', 'no', \n",
    "    'following', 'next', 'speech', 'year', 'years', 'state', 'also', 'would', 'like', 'may', 'must', \n",
    "    'upon', 'indeed', 'session', 'meeting', 'report', 'commission', 'behalf', 'gentleman', 'gentlemen', \n",
    "    'ladies', 'applause', 'group', 'colleague', 'colleagues', 'issue', 'issues', 'chancellor', 'court', \n",
    "    'ask', 'answer', 'reply', 'regard', 'regarding', 'regards', 'respect', 'respectfully', 'sign', \n",
    "    'shall', 'procedure', 'declare', 'hear', 'minutes', 'speaking', 'close', 'abg', 'mag', 'orf', 'wait'\n",
    "]\n",
    "\n",
    "all_stopwords = list(ENGLISH_STOP_WORDS) + custom_stopwords\n",
    "\n",
    "# Use enhanced segmentation for topic modeling\n",
    "segment_texts = segmented_df.groupby('Segment_ID_Enhanced')['Text'].apply(lambda x: ' '.join(x)).tolist()\n",
    "segment_embeddings = np.array(segmented_df.groupby('Segment_ID_Enhanced')['Segment_Embeddings'].first().tolist())\n",
    "\n",
    "print(f\"üìä Topic modeling data prepared:\")\n",
    "print(f\"  ‚Ä¢ Segments for modeling: {len(segment_texts)}\")\n",
    "print(f\"  ‚Ä¢ Embedding dimension: {segment_embeddings.shape[1]}\")\n",
    "\n",
    "# Configure BERTopic\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=all_stopwords,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "n_clusters = 15\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine', random_state=42)\n",
    "clustering_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=None,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=clustering_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    nr_topics=n_clusters,\n",
    "    min_topic_size=15,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training BERTopic model...\")\n",
    "topics, probs = topic_model.fit_transform(segment_texts, embeddings=segment_embeddings)\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(f\"\\nüéØ Topics created: {len(topic_info[topic_info['Topic'] != -1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca133f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded .env file from: ..\\.env\n",
      "üìù Generating topic names with LLM...\n",
      "\n",
      "üéØ Topic Summary:\n",
      "   Topic  Count                                               Name  \\\n",
      "0      0    438  0_removed national_jakob schwarz_gabriela schw...   \n",
      "1      1    343                1_euroteam_leikam_lassing_bhmdorfer   \n",
      "2      2    283  2_road pricing_driving licence_semmering base_...   \n",
      "3      3    368           3_elga_nonsmokers_pharmacies_common care   \n",
      "4      4    403   4_esm_debt brake_financial transaction_euro zone   \n",
      "5      5    537                  5_ceta_euratom_antiatomic_temelin   \n",
      "6      6    452  6_childcare allowance_child rearing_rearing_mi...   \n",
      "7      7    423  7_vaccinated_corona crisis_intrusion belakovic...   \n",
      "8      8    418  8_schwarzenberger_feurstein_active labour_semp...   \n",
      "9      9    482         9_custom fonts_fonts_volksanwalt_addiction   \n",
      "\n",
      "                                 LLM_Name  \n",
      "0        Parliamentary Leadership Changes  \n",
      "1              Euroteam and Liberal Forum  \n",
      "2  Transport Infrastructure and Licensing  \n",
      "3            Healthcare and Public Health  \n",
      "4                     ESM and Fiscal Pact  \n",
      "5              CETA and Antiatomic Policy  \n",
      "6            Childcare and Family Support  \n",
      "7         Vaccination and Crisis Response  \n",
      "8         Labour Initiatives and Industry  \n",
      "9        Custom Fonts and Social Concerns  \n",
      "\n",
      "üéØ Topic Summary:\n",
      "   Topic  Count                                               Name  \\\n",
      "0      0    438  0_removed national_jakob schwarz_gabriela schw...   \n",
      "1      1    343                1_euroteam_leikam_lassing_bhmdorfer   \n",
      "2      2    283  2_road pricing_driving licence_semmering base_...   \n",
      "3      3    368           3_elga_nonsmokers_pharmacies_common care   \n",
      "4      4    403   4_esm_debt brake_financial transaction_euro zone   \n",
      "5      5    537                  5_ceta_euratom_antiatomic_temelin   \n",
      "6      6    452  6_childcare allowance_child rearing_rearing_mi...   \n",
      "7      7    423  7_vaccinated_corona crisis_intrusion belakovic...   \n",
      "8      8    418  8_schwarzenberger_feurstein_active labour_semp...   \n",
      "9      9    482         9_custom fonts_fonts_volksanwalt_addiction   \n",
      "\n",
      "                                 LLM_Name  \n",
      "0        Parliamentary Leadership Changes  \n",
      "1              Euroteam and Liberal Forum  \n",
      "2  Transport Infrastructure and Licensing  \n",
      "3            Healthcare and Public Health  \n",
      "4                     ESM and Fiscal Pact  \n",
      "5              CETA and Antiatomic Policy  \n",
      "6            Childcare and Family Support  \n",
      "7         Vaccination and Crisis Response  \n",
      "8         Labour Initiatives and Industry  \n",
      "9        Custom Fonts and Social Concerns  \n"
     ]
    }
   ],
   "source": [
    "# === LLM TOPIC NAME GENERATION ===\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = os.path.join(os.pardir, '.env')\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path)\n",
    "    print(f\"‚úÖ Loaded .env file from: {dotenv_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è .env file not found. Ensure OPENAI_API_KEY is set in environment.\")\n",
    "\n",
    "def generate_topic_name_with_llm(keywords_list):\n",
    "    \"\"\"Generate concise topic names using OpenAI API.\"\"\"\n",
    "    if not isinstance(keywords_list, list) or not keywords_list:\n",
    "        return \"\"\n",
    "    \n",
    "    keyword_string = ', '.join(keywords_list)\n",
    "    prompt = f\"\"\"Keywords: {keyword_string}\n",
    "\n",
    "Generate a concise topic name (1-3 words, not counting 'and') for Austrian parliamentary debate using these keywords.\n",
    "Give slightly more weight to earlier keywords. Avoid words: reform, security, allocation, strategy.\n",
    "Output only the name.\"\"\"\n",
    "\n",
    "    try:\n",
    "        if not os.getenv('OPENAI_API_KEY'):\n",
    "            print(\"Error: OPENAI_API_KEY not set.\")\n",
    "            return \"\"\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant skilled at summarizing Austrian parliamentary topics from keywords.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            max_tokens=25\n",
    "        )\n",
    "        return response.choices[0].message.content.strip().replace('\"', '').replace('\\n', ' ').strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Generate LLM names\n",
    "print(\"üìù Generating topic names with LLM...\")\n",
    "topic_info['LLM_Name'] = topic_info.apply(\n",
    "    lambda row: generate_topic_name_with_llm(row['Representation']) if row['Topic'] != -1 else \"Outliers\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Topic Summary:\")\n",
    "print(topic_info[['Topic', 'Count', 'Name', 'LLM_Name']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d845ca2",
   "metadata": {},
   "source": [
    "## Temporal Analysis and Visualization\n",
    "\n",
    "Analyze how topics evolve over time and create comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793356e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Preparing temporal analysis...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot insert Segment_ID_Enhanced, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_12088\\998354083.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m print(\u001b[33m\"üìÖ Preparing temporal analysis...\"\u001b[39m)\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Map topics back to original dataframe\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m segment_topic_map = segmented_df.groupby(\u001b[33m'Segment_ID_Enhanced'\u001b[39m)[\u001b[33m'Segment_ID_Enhanced'\u001b[39m].first().reset_index()\n\u001b[32m     11\u001b[39m segment_topic_map[\u001b[33m'Topic_ID'\u001b[39m] = topics\n\u001b[32m     12\u001b[39m embeddings_with_topics = segmented_df.merge(\n\u001b[32m     13\u001b[39m     segment_topic_map.rename(columns={\u001b[33m'Segment_ID_Enhanced'\u001b[39m: \u001b[33m'Segment_ID_temp'\u001b[39m, \u001b[33m'Topic_ID'\u001b[39m: \u001b[33m'Topic_ID'\u001b[39m}),\n",
      "\u001b[32mc:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\series.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, level, drop, name, inplace, allow_duplicates)\u001b[39m\n\u001b[32m   1766\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1767\u001b[39m                     name = self.name\n\u001b[32m   1768\u001b[39m \n\u001b[32m   1769\u001b[39m             df = self.to_frame(name)\n\u001b[32m-> \u001b[39m\u001b[32m1770\u001b[39m             return df.reset_index(\n\u001b[32m   1771\u001b[39m                 level=level, drop=drop, allow_duplicates=allow_duplicates\n\u001b[32m   1772\u001b[39m             )\n\u001b[32m   1773\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[39m\n\u001b[32m   6468\u001b[39m                     level_values = algorithms.take(\n\u001b[32m   6469\u001b[39m                         level_values, lab, allow_fill=\u001b[38;5;28;01mTrue\u001b[39;00m, fill_value=lev._na_value\n\u001b[32m   6470\u001b[39m                     )\n\u001b[32m   6471\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m6472\u001b[39m                 new_obj.insert(\n\u001b[32m   6473\u001b[39m                     \u001b[32m0\u001b[39m,\n\u001b[32m   6474\u001b[39m                     name,\n\u001b[32m   6475\u001b[39m                     level_values,\n",
      "\u001b[32mc:\\Users\\pavle\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, loc, column, value, allow_duplicates)\u001b[39m\n\u001b[32m   5154\u001b[39m                 \u001b[33m\"'self.flags.allows_duplicate_labels' is False.\"\u001b[39m\n\u001b[32m   5155\u001b[39m             )\n\u001b[32m   5156\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m allow_duplicates \u001b[38;5;28;01mand\u001b[39;00m column \u001b[38;5;28;01min\u001b[39;00m self.columns:\n\u001b[32m   5157\u001b[39m             \u001b[38;5;66;03m# Should this be a different kind of error??\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5158\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"cannot insert {column}, already exists\")\n\u001b[32m   5159\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_integer(loc):\n\u001b[32m   5160\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\u001b[33m\"loc must be int\"\u001b[39m)\n\u001b[32m   5161\u001b[39m         \u001b[38;5;66;03m# convert non stdlib ints to satisfy typing checks\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: cannot insert Segment_ID_Enhanced, already exists"
     ]
    }
   ],
   "source": [
    "# === TEMPORAL ANALYSIS ===\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import math\n",
    "\n",
    "print(\"üìÖ Preparing temporal analysis...\")\n",
    "\n",
    "# Map topics back to original dataframe - FIXED\n",
    "# Create a mapping from segment ID to topic ID\n",
    "segment_topic_map = pd.DataFrame({\n",
    "    'Segment_ID_Enhanced': segmented_df['Segment_ID_Enhanced'].unique(),\n",
    "    'Topic_ID': topics\n",
    "})\n",
    "\n",
    "# Merge with segmented dataframe\n",
    "embeddings_with_topics = segmented_df.merge(\n",
    "    segment_topic_map, \n",
    "    on='Segment_ID_Enhanced', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Prepare temporal data\n",
    "embeddings_with_topics['Timestamp'] = pd.to_datetime(embeddings_with_topics['Date'], errors='coerce')\n",
    "embeddings_with_topics.dropna(subset=['Timestamp', 'Topic_ID'], inplace=True)\n",
    "embeddings_with_topics['Year'] = embeddings_with_topics['Timestamp'].dt.year\n",
    "\n",
    "# Create topics over time\n",
    "docs_for_temporal = embeddings_with_topics['Text'].tolist()\n",
    "topics_for_temporal = embeddings_with_topics['Topic_ID'].tolist()\n",
    "timestamps_for_temporal = embeddings_with_topics['Year'].tolist()\n",
    "\n",
    "print(\"üïê Computing topics over time...\")\n",
    "topics_over_time = topic_model.topics_over_time(\n",
    "    docs=docs_for_temporal,\n",
    "    topics=topics_for_temporal,\n",
    "    timestamps=timestamps_for_temporal,\n",
    "    global_tuning=True,\n",
    "    evolution_tuning=True,\n",
    "    nr_bins=20\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Temporal analysis complete! Prepared {len(docs_for_temporal)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56599eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPREHENSIVE VISUALIZATIONS ===\n",
    "\n",
    "# Create topic name mapping\n",
    "topic_id_to_name_map = pd.Series(topic_info.LLM_Name.values, index=topic_info.Topic).to_dict()\n",
    "\n",
    "print(\"üìä Creating visualizations...\")\n",
    "\n",
    "# 1. Topic evolution over time\n",
    "valid_topics = sorted([t for t in topics_over_time['Topic'].unique() if t != -1])\n",
    "speech_counts = embeddings_with_topics.groupby('Topic_ID').size()\n",
    "\n",
    "cols = 3\n",
    "rows = math.ceil(len(valid_topics) / cols) if valid_topics else 1\n",
    "\n",
    "subplot_titles = []\n",
    "for topic_id in valid_topics:\n",
    "    topic_name = topic_id_to_name_map.get(topic_id, f\"Topic {topic_id}\")\n",
    "    speech_count = speech_counts.get(topic_id, 0)\n",
    "    subplot_titles.append(f\"{topic_name}<br>(Speeches: {speech_count})\")\n",
    "\n",
    "fig_evolution = make_subplots(\n",
    "    rows=rows, cols=cols,\n",
    "    subplot_titles=subplot_titles,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.15\n",
    ")\n",
    "\n",
    "for i, topic_id in enumerate(valid_topics):\n",
    "    row = (i // cols) + 1\n",
    "    col = (i % cols) + 1\n",
    "    \n",
    "    topic_data = topics_over_time[topics_over_time['Topic'] == topic_id]\n",
    "    \n",
    "    fig_evolution.add_trace(\n",
    "        go.Scatter(\n",
    "            x=topic_data['Timestamp'], \n",
    "            y=topic_data['Frequency'],\n",
    "            mode='lines+markers',\n",
    "            name=f\"Topic {topic_id}\",\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=4)\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig_evolution.update_layout(\n",
    "    title_text=\"Topic Evolution Over Time in Austrian Parliament\",\n",
    "    height=350 * rows + 100,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "for i in range(1, rows + 1):\n",
    "    for j in range(1, cols + 1):\n",
    "        fig_evolution.update_xaxes(title_text=\"Year\", row=i, col=j)\n",
    "        fig_evolution.update_yaxes(title_text=\"Frequency\", row=i, col=j)\n",
    "\n",
    "fig_evolution.show()\n",
    "\n",
    "# 2. Topic distribution\n",
    "fig_dist = px.bar(\n",
    "    topic_info[topic_info['Topic'] != -1].sort_values('Count', ascending=True),\n",
    "    x='Count', y='LLM_Name',\n",
    "    orientation='h',\n",
    "    title='Topic Distribution by Segment Count',\n",
    "    labels={'Count': 'Number of Segments', 'LLM_Name': 'Topic'}\n",
    ")\n",
    "fig_dist.update_layout(height=max(400, len(valid_topics) * 30))\n",
    "fig_dist.show()\n",
    "\n",
    "print(\"üé® Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7405e47",
   "metadata": {},
   "source": [
    "## Model Evaluation and Quality Metrics\n",
    "\n",
    "Comprehensive assessment of segmentation and topic modeling quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec38a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === QUALITY METRICS ===\n",
    "\n",
    "print(\"üìä Computing comprehensive quality metrics...\")\n",
    "\n",
    "# 1. Segmentation quality\n",
    "avg_coherence = np.mean([m['coherence_ratio'] for m in coherence_metrics if m['coherence_ratio'] > 0])\n",
    "avg_silhouette = np.mean([m['silhouette'] for m in coherence_metrics if m['silhouette'] > 0])\n",
    "\n",
    "# 2. Topic modeling quality\n",
    "avg_segments_per_topic = topic_info[topic_info['Topic'] != -1]['Count'].mean()\n",
    "topic_size_std = topic_info[topic_info['Topic'] != -1]['Count'].std()\n",
    "\n",
    "# 3. Segmentation statistics by sitting category\n",
    "boundary_stats_df = pd.DataFrame(boundary_stats)\n",
    "category_stats = boundary_stats_df.groupby('category').agg({\n",
    "    'segments_created': ['mean', 'std'],\n",
    "    'sitting_length': ['mean', 'count']\n",
    "}).round(2)\n",
    "\n",
    "# 4. Overall statistics\n",
    "total_speeches = len(embeddings)\n",
    "total_segments = segmented_df['Segment_ID_Enhanced'].nunique()\n",
    "avg_speeches_per_segment = total_speeches / total_segments\n",
    "\n",
    "# 5. Temporal coverage\n",
    "year_range = embeddings_with_topics['Year'].max() - embeddings_with_topics['Year'].min()\n",
    "topics_per_year = embeddings_with_topics.groupby('Year')['Topic_ID'].nunique().mean()\n",
    "\n",
    "print(\"\\nüìà Segmentation Quality:\")\n",
    "print(f\"  ‚Ä¢ Average coherence ratio: {avg_coherence:.3f}\")\n",
    "print(f\"  ‚Ä¢ Average silhouette score: {avg_silhouette:.3f}\")\n",
    "print(f\"  ‚Ä¢ Total segments created: {total_segments:,}\")\n",
    "print(f\"  ‚Ä¢ Average speeches per segment: {avg_speeches_per_segment:.1f}\")\n",
    "\n",
    "print(f\"\\nüèõÔ∏è Sitting Categories Performance:\")\n",
    "for category in ['short', 'medium', 'long']:\n",
    "    if category in category_stats.index:\n",
    "        stats = category_stats.loc[category]\n",
    "        print(f\"  ‚Ä¢ {category.capitalize()} sittings:\")\n",
    "        print(f\"    - Count: {stats[('sitting_length', 'count')]}\")\n",
    "        print(f\"    - Avg segments: {stats[('segments_created', 'mean')]:.1f} ¬± {stats[('segments_created', 'std')]:.1f}\")\n",
    "        print(f\"    - Avg length: {stats[('sitting_length', 'mean')]:.0f} speeches\")\n",
    "\n",
    "print(f\"\\nüéØ Topic Modeling Quality:\")\n",
    "print(f\"  ‚Ä¢ Topics created: {len(topic_info[topic_info['Topic'] != -1])}\")\n",
    "print(f\"  ‚Ä¢ Average segments per topic: {avg_segments_per_topic:.1f}\")\n",
    "print(f\"  ‚Ä¢ Topic size standard deviation: {topic_size_std:.1f}\")\n",
    "print(f\"  ‚Ä¢ Temporal coverage: {year_range} years\")\n",
    "print(f\"  ‚Ä¢ Average topics per year: {topics_per_year:.1f}\")\n",
    "\n",
    "# 6. Quality assessment\n",
    "outlier_count = len(embeddings_with_topics[embeddings_with_topics['Topic_ID'] == -1])\n",
    "outlier_percentage = (outlier_count / total_speeches) * 100\n",
    "print(f\"  ‚Ä¢ Outlier percentage: {outlier_percentage:.1f}%\")\n",
    "\n",
    "print(f\"\\nüîç Quality Assessment:\")\n",
    "if avg_coherence > 0.15:\n",
    "    print(\"  ‚úÖ Excellent segmentation coherence\")\n",
    "elif avg_coherence > 0.08:\n",
    "    print(\"  ‚ö†Ô∏è Good segmentation coherence\")\n",
    "else:\n",
    "    print(\"  ‚ùå Poor segmentation coherence - consider parameter adjustment\")\n",
    "\n",
    "if outlier_percentage < 10:\n",
    "    print(\"  ‚úÖ Good topic clustering quality\")\n",
    "elif outlier_percentage < 20:\n",
    "    print(\"  ‚ö†Ô∏è Moderate topic clustering quality\")\n",
    "else:\n",
    "    print(\"  ‚ùå High outlier rate - consider adjusting clustering parameters\")\n",
    "\n",
    "# Expected vs actual segments per sitting\n",
    "expected_range = (3, 8)  # Expected 3-8 segments for most sittings\n",
    "actual_avg = boundary_stats_df['segments_created'].mean()\n",
    "if expected_range[0] <= actual_avg <= expected_range[1]:\n",
    "    print(f\"  ‚úÖ Segment count in expected range: {actual_avg:.1f}\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Segment count outside expected range: {actual_avg:.1f} (expected: {expected_range[0]}-{expected_range[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20609935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVE RESULTS (OPTIONAL) ===\n",
    "# Uncomment to save enhanced results\n",
    "\n",
    "# print(\"üíæ Saving enhanced results...\")\n",
    "# embeddings_final = embeddings_with_topics.copy()\n",
    "# embeddings_final['Topic_Name'] = embeddings_final['Topic_ID'].map(topic_id_to_name_map)\n",
    "# embeddings_final.to_pickle('data folder/data/AT_with_enhanced_topics.pkl')\n",
    "# topic_info.to_pickle('data folder/data/topic_info_enhanced.pkl')\n",
    "# print(\"‚úÖ Results saved!\")\n",
    "\n",
    "print(\"\\nüéâ Analysis complete!\")\n",
    "print(\"üìã Summary:\")\n",
    "print(f\"  ‚Ä¢ {total_segments:,} segments from {len(boundary_stats):,} sittings\")\n",
    "print(f\"  ‚Ä¢ {len(valid_topics)} meaningful topics identified\")\n",
    "print(f\"  ‚Ä¢ Coherence score: {avg_coherence:.3f}\")\n",
    "print(f\"  ‚Ä¢ Temporal span: {year_range} years\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
