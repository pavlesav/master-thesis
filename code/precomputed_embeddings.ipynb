{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e08b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded data: (231752, 30)\n",
      "Columns: ['Sitting_ID', 'Speech_ID', 'Title', 'Date', 'Body', 'Term', 'Session', 'Meeting', 'Sitting', 'Agenda', 'Subcorpus', 'Lang', 'Speaker_role', 'Speaker_MP', 'Speaker_minister', 'Speaker_party', 'Speaker_party_name', 'Party_status', 'Party_orientation', 'Speaker_ID', 'Speaker_name', 'Speaker_gender', 'Speaker_birth', 'Text', 'Word_Count', 'Is_Too_Short', 'Is_Filtered', 'Speech_Embeddings', 'Segment_ID', 'Segment_Embeddings']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Load the data with embeddings\n",
    "embeddings = pd.read_pickle(r\"data folder\\data\\AT_with_embeddings_final.pkl\")\n",
    "\n",
    "print(f\"✅ Loaded data: {embeddings.shape}\")\n",
    "print(f\"Columns: {list(embeddings.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737eceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Data Overview:\n",
      "  • Total speeches: 231,752\n",
      "  • Speech embedding shape: (1024,)\n",
      "  • Segment embedding shape: (1024,)\n",
      "  • Unique segments: 5,728\n",
      "  • Average speeches per segment: 40.5\n",
      "\n",
      "🔍 Missing values:\n",
      "  • Segment_ID: 0\n",
      "  • Speech_Embeddings: 41119\n",
      "  • Segment_Embeddings: 0\n",
      "\n",
      "📈 Sitting length distribution:\n",
      "  • Min speeches per sitting: 1\n",
      "  • Max speeches per sitting: 1378\n",
      "  • Average speeches per sitting: 189.8\n",
      "  • Sittings with <50 speeches: 474\n",
      "  • Sittings with >200 speeches: 542\n",
      "\n",
      "📈 Sitting length distribution:\n",
      "  • Min speeches per sitting: 1\n",
      "  • Max speeches per sitting: 1378\n",
      "  • Average speeches per sitting: 189.8\n",
      "  • Sittings with <50 speeches: 474\n",
      "  • Sittings with >200 speeches: 542\n"
     ]
    }
   ],
   "source": [
    "# === DATA OVERVIEW ===\n",
    "print(\"📊 Data Overview:\")\n",
    "print(f\"  • Total speeches: {embeddings.shape[0]:,}\")\n",
    "print(f\"  • Speech embedding shape: {embeddings['Speech_Embeddings'][0].shape}\")\n",
    "print(f\"  • Segment embedding shape: {embeddings['Segment_Embeddings'][0].shape}\")\n",
    "print(f\"  • Unique segments: {embeddings['Segment_ID'].nunique():,}\")\n",
    "print(f\"  • Average speeches per segment: {embeddings.shape[0] / embeddings['Segment_ID'].nunique():.1f}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\n🔍 Missing values:\")\n",
    "print(f\"  • Segment_ID: {embeddings['Segment_ID'].isna().sum()}\")\n",
    "print(f\"  • Speech_Embeddings: {embeddings['Speech_Embeddings'].isna().sum()}\")\n",
    "print(f\"  • Segment_Embeddings: {embeddings['Segment_Embeddings'].isna().sum()}\")\n",
    "\n",
    "# Check sitting length distribution\n",
    "sitting_lengths = embeddings.groupby('Sitting_ID').size()\n",
    "print(f\"\\n📈 Sitting length distribution:\")\n",
    "print(f\"  • Min speeches per sitting: {sitting_lengths.min()}\")\n",
    "print(f\"  • Max speeches per sitting: {sitting_lengths.max()}\")\n",
    "print(f\"  • Average speeches per sitting: {sitting_lengths.mean():.1f}\")\n",
    "print(f\"  • Sittings with <50 speeches: {(sitting_lengths < 50).sum()}\")\n",
    "print(f\"  • Sittings with >200 speeches: {(sitting_lengths > 200).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5cc7c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Overall Speech Statistics:\n",
      "  • Total speeches in dataset: 231,752\n",
      "  • Total chairperson speeches: 125,038\n",
      "  • Chairperson percentage: 54.0%\n",
      "\n",
      "📋 Agenda-related Speech Analysis:\n",
      "\n",
      "Chairperson speeches only:\n",
      "  • Containing 'agenda': 11,613 (9.3% of chairperson speeches, 5.01% of all speeches)\n",
      "  • Containing 'agenda item': 5,039 (4.0% of chairperson speeches, 2.17% of all speeches)\n",
      "  • Containing 'next agenda': 0 (0.0% of chairperson speeches, 0.00% of all speeches)\n",
      "  • Containing 'next agenda item': 0 (0.0% of chairperson speeches, 0.00% of all speeches)\n",
      "\n",
      "Chairperson speeches only:\n",
      "  • Containing 'agenda': 11,613 (9.3% of chairperson speeches, 5.01% of all speeches)\n",
      "  • Containing 'agenda item': 5,039 (4.0% of chairperson speeches, 2.17% of all speeches)\n",
      "  • Containing 'next agenda': 0 (0.0% of chairperson speeches, 0.00% of all speeches)\n",
      "  • Containing 'next agenda item': 0 (0.0% of chairperson speeches, 0.00% of all speeches)\n",
      "\n",
      "📈 Summary of speeches:\n",
      "  • First speech at row: 2\n",
      "  • Last speech at row: 218,336\n",
      "  • Total row span: 218,334\n",
      "  • Average gap between agenda speeches: 18.8 rows\n",
      "\n",
      "📈 Summary of speeches:\n",
      "  • First speech at row: 2\n",
      "  • Last speech at row: 218,336\n",
      "  • Total row span: 218,334\n",
      "  • Average gap between agenda speeches: 18.8 rows\n"
     ]
    }
   ],
   "source": [
    "# === COMPREHENSIVE CHAIRPERSON AND AGENDA ANALYSIS ===\n",
    "\n",
    "print(\"📊 Overall Speech Statistics:\")\n",
    "print(f\"  • Total speeches in dataset: {len(embeddings):,}\")\n",
    "\n",
    "# Chairperson speeches\n",
    "chairperson_total = embeddings[embeddings['Speaker_role'] == 'Chairperson']\n",
    "print(f\"  • Total chairperson speeches: {len(chairperson_total):,}\")\n",
    "print(f\"  • Chairperson percentage: {len(chairperson_total)/len(embeddings)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n📋 Agenda-related Speech Analysis:\")\n",
    "\n",
    "# Various agenda patterns\n",
    "agenda_patterns = {\n",
    "    'agenda': embeddings['Text'].str.contains('agenda', case=False),\n",
    "    'agenda item': embeddings['Text'].str.contains('agenda item', case=False),\n",
    "    'next agenda': embeddings['Text'].str.contains('next agenda', case=False),\n",
    "    'next agenda item': embeddings['Text'].str.contains('next agenda item', case=False)\n",
    "}\n",
    "\n",
    "# # Count for all speeches\n",
    "# print(\"All speeches:\")\n",
    "# for pattern_name, pattern_mask in agenda_patterns.items():\n",
    "#     count = pattern_mask.sum()\n",
    "#     percentage = count / len(chairperson_total) * 100\n",
    "#     print(f\"  • Containing '{pattern_name}': {count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "print(\"\\nChairperson speeches only:\")\n",
    "# Count for chairperson speeches only\n",
    "for pattern_name, pattern_mask in agenda_patterns.items():\n",
    "    chairperson_with_pattern = embeddings[(embeddings['Speaker_role'] == 'Chairperson') & pattern_mask]\n",
    "    count = len(chairperson_with_pattern)\n",
    "    percentage_of_chairperson = count / len(chairperson_total) * 100 if len(chairperson_total) > 0 else 0\n",
    "    percentage_of_total = count / len(embeddings) * 100\n",
    "    print(f\"  • Containing '{pattern_name}': {count:,} ({percentage_of_chairperson:.1f}% of chairperson speeches, {percentage_of_total:.2f}% of all speeches)\")\n",
    "\n",
    "# Sample of 100 chairperson speeches with 'agenda'\n",
    "chairperson_with_agenda = embeddings[(embeddings['Speaker_role'] == 'Chairperson') & \n",
    "                                    (embeddings['Text'].str.contains('agenda', case=False))]\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_colwidth', 200)  # Limit text to 200 chars for readability\n",
    "\n",
    "sample_speeches = chairperson_with_agenda\n",
    "\n",
    "for i, (idx, row) in enumerate(sample_speeches.iterrows(), 1):\n",
    "    # Calculate rows between current and previous (if not first)\n",
    "    if i > 1:\n",
    "        prev_idx = prev_row_idx\n",
    "        rows_between = idx - prev_idx - 1\n",
    "        #print(f\"\\n[{rows_between} rows between previous and current]\")\n",
    "    \n",
    "    # print(f\"\\n🎤 Speech #{i} (Row index: {idx})\")\n",
    "    # print(f\"Sitting: {row['Sitting_ID']} | Speaker: {row['Speaker_role']}\")\n",
    "    # print(\"-\" * 80)\n",
    "    \n",
    "    # Truncate very long speeches for readability\n",
    "    # text = row['Text']\n",
    "    # if len(text) > 300:\n",
    "    #     text = text[:300] + \"...\"\n",
    "    # print(text)\n",
    "    # print(\"-\" * 80)\n",
    "    \n",
    "    prev_row_idx = idx\n",
    "\n",
    "print(f\"\\n📈 Summary of speeches:\")\n",
    "if len(sample_speeches) > 1:\n",
    "    first_idx = sample_speeches.index[0]\n",
    "    last_idx = sample_speeches.index[-1]\n",
    "    total_span = last_idx - first_idx\n",
    "    avg_gap = total_span / (len(sample_speeches) - 1) if len(sample_speeches) > 1 else 0\n",
    "    print(f\"  • First speech at row: {first_idx:,}\")\n",
    "    print(f\"  • Last speech at row: {last_idx:,}\")\n",
    "    print(f\"  • Total row span: {total_span:,}\")\n",
    "    print(f\"  • Average gap between agenda speeches: {avg_gap:.1f} rows\")\n",
    "\n",
    "# Reset display options\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d46b6c",
   "metadata": {},
   "source": [
    "## Parliamentary Segmentation\n",
    "\n",
    "Improved segmentation considering that Austrian parliament sittings can vary from 1-10 agendas (average 5-6).\n",
    "We'll implement adaptive segmentation that adjusts to sitting length and uses multiple similarity signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6b7189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced parliamentary segmentation function loaded (with agenda detection)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def parliamentary_segment_speeches(df, window_size=5, min_segment_size=3):\n",
    "    \"\"\"\n",
    "    Parliamentary segmentation with multi-scale analysis and chairperson agenda detection\n",
    "    \"\"\"\n",
    "    segment_ids = []\n",
    "    segmentation_metrics = []\n",
    "    \n",
    "    # Get unique sittings for progress tracking\n",
    "    unique_sittings = df['Sitting_ID'].unique()\n",
    "    print(f\"🔄 Processing {len(unique_sittings)} sittings...\")\n",
    "    \n",
    "    for sitting_id in tqdm(unique_sittings, desc=\"Segmenting sittings\", unit=\"sitting\"):\n",
    "        group = df[df['Sitting_ID'] == sitting_id]\n",
    "        sitting_length = len(group)\n",
    "        \n",
    "        if sitting_length < min_segment_size:\n",
    "            # Very small sitting - one segment\n",
    "            sitting_segments = [f\"{sitting_id}_seg_0\"] * len(group)\n",
    "            segment_ids.extend(sitting_segments)\n",
    "            segmentation_metrics.append({\n",
    "                'sitting_id': sitting_id,\n",
    "                'sitting_length': sitting_length,\n",
    "                'num_segments': 1,\n",
    "                'avg_segment_size': sitting_length,\n",
    "                'boundaries_found': 0,\n",
    "                'agenda_boundaries': 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        embeddings = np.array(group['Speech_Embeddings'].tolist())\n",
    "        \n",
    "        # --- NEW: Flexible target_segments formula ---\n",
    "        target_segments = max(2, int(np.ceil(sitting_length / 25)))\n",
    "        threshold_percentile = 40\n",
    "        \n",
    "        # === CHAIRPERSON AGENDA DETECTION ===\n",
    "        agenda_boundaries = set()\n",
    "        agenda_signals = []\n",
    "        \n",
    "        for i, (idx, row) in enumerate(group.iterrows()):\n",
    "            agenda_score = 0\n",
    "            \n",
    "            # Strong signal for chairperson with agenda mentions\n",
    "            if row['Speaker_role'] == 'Chairperson':\n",
    "                text = str(row['Text']).lower()\n",
    "                \n",
    "                if 'agenda item' in text:\n",
    "                    agenda_score = 1.0  # Strongest signal\n",
    "                elif 'agenda' in text:\n",
    "                    agenda_score = 0.7  # Strong signal\n",
    "                elif i == 0:  # First speech by chairperson (session start)\n",
    "                    agenda_score = 0.3  # Mild signal\n",
    "            \n",
    "            agenda_signals.append(agenda_score)\n",
    "            \n",
    "            # Add strong agenda boundaries\n",
    "            if agenda_score >= 0.7 and i >= min_segment_size and (sitting_length - i) >= min_segment_size:\n",
    "                agenda_boundaries.add(i)\n",
    "        \n",
    "        # === MULTI-SCALE SIMILARITY ANALYSIS ===\n",
    "        similarity_signals = {}\n",
    "        \n",
    "        # 1. Primary windowed similarity\n",
    "        similarities = []\n",
    "        for i in range(len(embeddings) - window_size):\n",
    "            window1 = np.mean(embeddings[i:i + window_size], axis=0)\n",
    "            window2 = np.mean(embeddings[i + window_size:i + 2*window_size], axis=0)\n",
    "            \n",
    "            sim = cosine_similarity(\n",
    "                window1.reshape(1, -1),\n",
    "                window2.reshape(1, -1)\n",
    "            )[0][0]\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        similarity_signals['primary'] = np.array(similarities)\n",
    "        \n",
    "        # 2. Point-to-point similarity for fine-grained detection\n",
    "        if len(embeddings) > 6:\n",
    "            point_sims = []\n",
    "            for i in range(len(embeddings) - 1):\n",
    "                sim = cosine_similarity(\n",
    "                    embeddings[i].reshape(1, -1),\n",
    "                    embeddings[i + 1].reshape(1, -1)\n",
    "                )[0][0]\n",
    "                point_sims.append(sim)\n",
    "            \n",
    "            # Align with primary signal\n",
    "            point_sims = np.array(point_sims)\n",
    "            if len(point_sims) > len(similarities):\n",
    "                point_sims = point_sims[:len(similarities)]\n",
    "            elif len(point_sims) < len(similarities):\n",
    "                padding = len(similarities) - len(point_sims)\n",
    "                point_sims = np.pad(point_sims, (0, padding), mode='edge')\n",
    "            \n",
    "            similarity_signals['point'] = point_sims\n",
    "        \n",
    "        # 3. Gradient-based change detection\n",
    "        if len(embeddings) > 10:\n",
    "            trajectory = []\n",
    "            for i in range(1, len(embeddings)):\n",
    "                displacement = np.linalg.norm(embeddings[i] - embeddings[i-1])\n",
    "                trajectory.append(float(displacement))\n",
    "            \n",
    "            trajectory = np.array(trajectory, dtype=np.float64)\n",
    "            if len(trajectory) > 3:\n",
    "                try:\n",
    "                    from scipy.ndimage import uniform_filter1d\n",
    "                    smoothed = uniform_filter1d(trajectory.astype(np.float64), size=3)\n",
    "                    gradient = np.gradient(smoothed)\n",
    "                    \n",
    "                    # Align with similarities\n",
    "                    if len(gradient) > len(similarities):\n",
    "                        gradient = gradient[:len(similarities)]\n",
    "                    elif len(gradient) < len(similarities):\n",
    "                        padding = len(similarities) - len(gradient)\n",
    "                        gradient = np.pad(gradient, (0, padding), mode='edge')\n",
    "                    \n",
    "                    similarity_signals['gradient'] = gradient\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        if len(similarity_signals['primary']) == 0:\n",
    "            sitting_segments = [f\"{sitting_id}_seg_0\"] * len(group)\n",
    "            segment_ids.extend(sitting_segments)\n",
    "            segmentation_metrics.append({\n",
    "                'sitting_id': sitting_id,\n",
    "                'sitting_length': sitting_length,\n",
    "                'num_segments': 1,\n",
    "                'avg_segment_size': sitting_length,\n",
    "                'boundaries_found': 0,\n",
    "                'agenda_boundaries': 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # === BOUNDARY DETECTION ===\n",
    "        candidate_boundaries = set()\n",
    "        \n",
    "        # 1. Add agenda boundaries (highest priority)\n",
    "        candidate_boundaries.update(agenda_boundaries)\n",
    "        \n",
    "        # 2. Find boundaries from primary similarity drops\n",
    "        primary_sims = similarity_signals['primary']\n",
    "        threshold = np.percentile(primary_sims, threshold_percentile)\n",
    "        \n",
    "        for i in range(len(primary_sims)):\n",
    "            if (primary_sims[i] < threshold and \n",
    "                i >= min_segment_size and \n",
    "                (len(group) - i - window_size) >= min_segment_size):\n",
    "                candidate_boundaries.add(i + window_size)\n",
    "        \n",
    "        # 3. Add from point-to-point analysis\n",
    "        if 'point' in similarity_signals:\n",
    "            point_threshold = np.percentile(similarity_signals['point'], threshold_percentile - 10)\n",
    "            for i in range(len(similarity_signals['point'])):\n",
    "                if (similarity_signals['point'][i] < point_threshold and \n",
    "                    i >= min_segment_size and \n",
    "                    (len(group) - i) >= min_segment_size):\n",
    "                    candidate_boundaries.add(i)\n",
    "        \n",
    "        # 4. Add from gradient analysis\n",
    "        if 'gradient' in similarity_signals:\n",
    "            gradient = similarity_signals['gradient']\n",
    "            gradient_threshold = np.percentile(np.abs(gradient), 75)\n",
    "            for i in range(len(gradient)):\n",
    "                if (np.abs(gradient[i]) > gradient_threshold and \n",
    "                    i >= min_segment_size and \n",
    "                    (len(group) - i) >= min_segment_size):\n",
    "                    candidate_boundaries.add(i)\n",
    "        \n",
    "        candidates = sorted(list(candidate_boundaries))\n",
    "        \n",
    "        # === BOUNDARY SELECTION WITH AGENDA PRIORITIZATION ===\n",
    "        boundaries = []\n",
    "        if candidates:\n",
    "            if len(candidates) <= target_segments - 1:\n",
    "                boundaries = candidates\n",
    "            else:\n",
    "                # Score candidates with agenda boost\n",
    "                candidate_scores = []\n",
    "                for c in candidates:\n",
    "                    score = 0\n",
    "                    \n",
    "                    # Agenda boost (highest priority)\n",
    "                    if c < len(agenda_signals):\n",
    "                        score += agenda_signals[c] * 5.0  # Very high weight for agenda\n",
    "                    \n",
    "                    # Primary similarity score\n",
    "                    if c - window_size >= 0 and c - window_size < len(primary_sims):\n",
    "                        score += (1 - primary_sims[c - window_size]) * 2.0\n",
    "                    \n",
    "                    # Point similarity score\n",
    "                    if 'point' in similarity_signals and c < len(similarity_signals['point']):\n",
    "                        score += (1 - similarity_signals['point'][c]) * 1.5\n",
    "                    \n",
    "                    # Gradient score\n",
    "                    if 'gradient' in similarity_signals and c < len(similarity_signals['gradient']):\n",
    "                        score += np.abs(similarity_signals['gradient'][c]) * 1.0\n",
    "                    \n",
    "                    candidate_scores.append((c, score))\n",
    "                \n",
    "                # Select top scoring boundaries\n",
    "                candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                boundaries = sorted([c for c, _ in candidate_scores[:target_segments-1]])\n",
    "        \n",
    "        # === BOUNDARY VALIDATION ===\n",
    "        validated_boundaries = []\n",
    "        for boundary in boundaries:\n",
    "            if not validated_boundaries or (boundary - validated_boundaries[-1]) >= min_segment_size:\n",
    "                validated_boundaries.append(boundary)\n",
    "        \n",
    "        boundaries = validated_boundaries\n",
    "        \n",
    "        # Assign segment IDs\n",
    "        current_segment = 0\n",
    "        sitting_segments = []\n",
    "        \n",
    "        for i in range(len(group)):\n",
    "            if i > 0 and (i - 1) in boundaries:\n",
    "                current_segment += 1\n",
    "            sitting_segments.append(f\"{sitting_id}_seg_{current_segment}\")\n",
    "        \n",
    "        segment_ids.extend(sitting_segments)\n",
    "        \n",
    "        # Store metrics\n",
    "        num_segments = len(set(sitting_segments))\n",
    "        agenda_bound_count = len([b for b in boundaries if b in agenda_boundaries])\n",
    "        \n",
    "        segmentation_metrics.append({\n",
    "            'sitting_id': sitting_id,\n",
    "            'sitting_length': sitting_length,\n",
    "            'num_segments': num_segments,\n",
    "            'avg_segment_size': sitting_length / num_segments,\n",
    "            'boundaries_found': len(boundaries),\n",
    "            'agenda_boundaries': agenda_bound_count,\n",
    "            'target_segments': target_segments,\n",
    "            'candidate_boundaries': len(candidates),\n",
    "            'signals_used': len(similarity_signals) + 1  # +1 for agenda signals\n",
    "        })\n",
    "    \n",
    "    df['Segment_ID'] = segment_ids\n",
    "    return df, segmentation_metrics\n",
    "\n",
    "print(\"✅ Enhanced parliamentary segmentation function loaded (with agenda detection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7faf133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏛️ Running Parliamentary Segmentation (optimized for more segments)...\n",
      "📊 Data to process:\n",
      "  • Unique sittings: 1,221\n",
      "  • Total speeches: 190,633\n",
      "  • Average speeches per sitting: 156.1\n",
      "🔄 Processing 1221 sittings...\n",
      "📊 Data to process:\n",
      "  • Unique sittings: 1,221\n",
      "  • Total speeches: 190,633\n",
      "  • Average speeches per sitting: 156.1\n",
      "🔄 Processing 1221 sittings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting sittings: 100%|██████████| 1221/1221 [03:10<00:00,  6.40sitting/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Parliamentary segmentation complete!\n",
      "📊 Results:\n",
      "  • Total speeches processed: 190,633\n",
      "  • Unique segments created: 7,043\n",
      "  • Average speeches per segment: 27.1\n",
      "\n",
      "📈 Segmentation Quality Overview:\n",
      "  • Average segments per sitting: 5.8\n",
      "  • Segments per sitting (std): 5.1\n",
      "  • Average segment size: 18.9\n",
      "  • Target vs actual correlation: 0.964\n",
      "  • Average candidate boundaries: 159.1\n",
      "  • Average signals used: 4.0\n",
      "\n",
      "🔍 Signal Usage Analysis:\n",
      "  • Sittings using point-to-point analysis: 790/1221 (64.7%)\n",
      "  • Sittings using gradient analysis: 777/1221 (63.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === APPLY PARLIAMENTARY SEGMENTATION ===\n",
    "print(\"🏛️ Running Parliamentary Segmentation (optimized for more segments)...\")\n",
    "\n",
    "# Check data size first\n",
    "long_speeches_df = embeddings[~embeddings['Is_Too_Short']].copy()\n",
    "unique_sittings = long_speeches_df['Sitting_ID'].nunique()\n",
    "total_speeches = len(long_speeches_df)\n",
    "\n",
    "print(f\"📊 Data to process:\")\n",
    "print(f\"  • Unique sittings: {unique_sittings:,}\")\n",
    "print(f\"  • Total speeches: {total_speeches:,}\")\n",
    "print(f\"  • Average speeches per sitting: {total_speeches/unique_sittings:.1f}\")\n",
    "\n",
    "# Run parliamentary segmentation (tuned for more segments)\n",
    "segmented_df, seg_metrics = parliamentary_segment_speeches(\n",
    "    long_speeches_df, \n",
    "    window_size=5,        \n",
    "    min_segment_size=3    # Smaller minimum for more segments\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Parliamentary segmentation complete!\")\n",
    "print(f\"📊 Results:\")\n",
    "print(f\"  • Total speeches processed: {len(segmented_df):,}\")\n",
    "print(f\"  • Unique segments created: {segmented_df['Segment_ID'].nunique():,}\")\n",
    "print(f\"  • Average speeches per segment: {len(segmented_df) / segmented_df['Segment_ID'].nunique():.1f}\")\n",
    "\n",
    "# Convert metrics to DataFrame for analysis\n",
    "metrics_df = pd.DataFrame(seg_metrics)\n",
    "\n",
    "print(f\"\\n📈 Segmentation Quality Overview:\")\n",
    "print(f\"  • Average segments per sitting: {metrics_df['num_segments'].mean():.1f}\")\n",
    "print(f\"  • Segments per sitting (std): {metrics_df['num_segments'].std():.1f}\")\n",
    "print(f\"  • Average segment size: {metrics_df['avg_segment_size'].mean():.1f}\")\n",
    "print(f\"  • Target vs actual correlation: {metrics_df[['target_segments', 'num_segments']].corr().iloc[0,1]:.3f}\")\n",
    "print(f\"  • Average candidate boundaries: {metrics_df['candidate_boundaries'].mean():.1f}\")\n",
    "print(f\"  • Average signals used: {metrics_df['signals_used'].mean():.1f}\")\n",
    "\n",
    "# Analysis of signal effectiveness\n",
    "sittings_with_point = (metrics_df['signals_used'] >= 2).sum()\n",
    "sittings_with_gradient = (metrics_df['signals_used'] >= 3).sum()\n",
    "print(f\"\\n🔍 Signal Usage Analysis:\")\n",
    "print(f\"  • Sittings using point-to-point analysis: {sittings_with_point}/{len(metrics_df)} ({sittings_with_point/len(metrics_df)*100:.1f}%)\")\n",
    "print(f\"  • Sittings using gradient analysis: {sittings_with_gradient}/{len(metrics_df)} ({sittings_with_gradient/len(metrics_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaaf62f",
   "metadata": {},
   "source": [
    "## Topic Modeling with BERTopic\n",
    "\n",
    "This section runs the topic modeling on the segmented speeches using BERTopic with guided topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BERTOPIC SETUP WITH GUIDED TOPICS ===\n",
    "from bertopic import BERTopic\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from umap import UMAP\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Define the 22 target topic categories\n",
    "label_list = [\n",
    "    \"Education\", \"Technology\", \"Health\", \"Environment\", \"Housing\", \"Labor\", \n",
    "    \"Defense\", \"Government Operations\", \"Social Welfare\", \"Other\", \"Macroeconomics\", \n",
    "    \"Domestic Commerce\", \"Civil Rights\", \"International Affairs\", \"Transportation\", \n",
    "    \"Immigration\", \"Law and Crime\", \"Agriculture\", \"Foreign Trade\", \"Culture\", \n",
    "    \"Public Lands\", \"Energy\"\n",
    "]\n",
    "\n",
    "# Detailed topic descriptions for better classification\n",
    "majortopics_description = {\n",
    "    'Macroeconomics': 'issues related to domestic macroeconomic policy, such as the state and prospect of the national economy, economic policy, inflation, interest rates, monetary policy, cost of living, unemployment rate, national budget, public debt, price control, tax enforcement, industrial revitalization and growth.',\n",
    "    'Civil Rights': 'issues related to civil rights and minority rights, discrimination towards races, gender, sexual orientation, handicap, and other minorities, voting rights, freedom of speech, religious freedoms, privacy rights, protection of personal data, abortion rights, anti-government activity groups (e.g., local insurgency groups), religion and the Church.',\n",
    "    'Health': 'issues related to health care, health care reforms, health insurance, drug industry, medical facilities, medical workers, disease prevention, treatment, and health promotion, drug and alcohol abuse, mental health, research in medicine, medical liability and unfair medical practices.',\n",
    "    'Agriculture': 'issues related to agriculture policy, fishing, agricultural foreign trade, food marketing, subsidies to farmers, food inspection and safety, animal and crop disease, pest control and pesticide regulation, welfare for animals in farms, pets, veterinary medicine, agricultural research.',\n",
    "    'Labor': 'issues related to labor, employment, employment programs, employee benefits, pensions and retirement accounts, minimum wage, labor law, job training, labor unions, worker safety and protection, youth employment and seasonal workers.',\n",
    "    'Education': 'issues related to educational policies, primary and secondary schools, student loans and education finance, the regulation of colleges and universities, school reforms, teachers, vocational training, evening schools, safety in schools, efforts to improve educational standards, and issues related to libraries, dictionaries, teaching material, research in education.',\n",
    "    'Environment': 'issues related to environmental policy, drinking water safety, all kinds of pollution (air, noise, soil), waste disposal, recycling, climate change, outdoor environmental hazards (e.g., asbestos), species and forest protection, marine and freshwater environment, hunting, regulation of laboratory or performance animals, land and water resource conservation, research in environmental technology.',\n",
    "    'Energy': 'issues related to energy policy, electricity, regulation of electrical utilities, nuclear energy and disposal of nuclear waste, natural gas and oil, drilling, oil spills, oil and gas prices, heat supply, shortages and gasoline regulation, coal production, alternative and renewable energy, energy conservation and energy efficiency, energy research.',\n",
    "    'Immigration': 'issues related to immigration, refugees, and citizenship, integration issues, regulation of residence permits, asylum applications; criminal offences and diseases caused by immigration.',\n",
    "    'Transportation': 'issues related to mass transportation construction and regulation, bus transport, regulation related to motor vehicles, road construction, maintenance and safety, parking facilities, traffic accidents statistics, air travel, rail travel, rail freight, maritime transportation, inland waterways and channels, transportation research and development.',\n",
    "    'Law and Crime': 'issues related to the control, prevention, and impact of crime; all law enforcement agencies, including border and customs, police, court system, prison system; terrorism, white collar crime, counterfeiting and fraud, cyber-crime, drug trafficking, domestic violence, child welfare, family law, juvenile crime.',\n",
    "    'Social Welfare': 'issues related to social welfare policy, the Ministry of Social Affairs, social services, poverty assistance for low-income families and for the elderly, parental leave and child care, assistance for people with physical or mental disabilities, including early retirement pension, discounts on public services, volunteer associations (e.g., Red Cross), charities, and youth organizations.',\n",
    "    'Housing': 'issues related to housing, urban affairs and community development, housing market, property tax, spatial planning, rural development, location permits, construction inspection, illegal construction, industrial and commercial building issues, national housing policy, housing for low-income individuals, rental housing, housing for the elderly, e.g., nursing homes, housing for the homeless and efforts to reduce homelessness, research related to housing.',\n",
    "    'Domestic Commerce': 'issues related to banking, finance and internal commerce, including stock exchange, investments, consumer finance, mortgages, credit cards, insurance availability and cost, accounting regulation, personal, commercial, and municipal bankruptcies, programs to promote small businesses, copyrights and patents, intellectual property, natural disaster preparedness and relief, consumer safety; regulation and promotion of tourism, sports, gambling, and personal fitness; domestic commerce research.',\n",
    "    'Defense': 'issues related to defense policy, military intelligence, espionage, weapons, military personnel, reserve forces, military buildings, military courts, nuclear weapons, civil defense, including firefighters and mountain rescue services, homeland security, military aid or arms sales to other countries, prisoners of war and collateral damage to civilian populations, military nuclear and hazardous waste disposal and military environmental compliance, defense alliances and agreements, direct foreign military operations, claims against military, defense research.',\n",
    "    'Technology': 'issues related to science and technology transfer and international science cooperation, research policy, government space programs and space exploration, telephones and telecommunication regulation, broadcast media (television, radio, newspapers, films), weather forecasting, geological surveys, computer industry, cyber security.',\n",
    "    'Foreign Trade': 'issues related to foreign trade, trade negotiations, free trade agreements, import regulation, export promotion and regulation, subsidies, private business investment and corporate development, competitiveness, exchange rates, the strength of national currency in comparison to other currencies, foreign investment and sales of companies abroad.',\n",
    "    'International Affairs': 'issues related to international affairs, foreign policy and relations to other countries, issues related to the Ministry of Foreign Affairs, foreign aid, international agreements (such as Kyoto agreement on the environment, the Schengen agreement), international organizations (including United Nations, UNESCO, International Olympic Committee, International Criminal Court), NGOs, issues related to diplomacy, embassies, citizens abroad; issues related to border control; issues related to international finance, including the World Bank and International Monetary Fund, the financial situation of the EU; issues related to a foreign country that do not impact the home country; issues related to human rights in other countries, international terrorism.',\n",
    "    'Government Operations': 'issues related to general government operations, the work of multiple departments, public employees, postal services, nominations and appointments, national mints, medals, and commemorative coins, management of government property, government procurement and contractors, public scandal and impeachment, claims against the government, the state inspectorate and audit, anti-corruption policies, regulation of political campaigns, political advertising and voter registration, census and statistics collection by government; issues related to local government, capital city and municipalities, including decentralization; issues related to national holidays.',\n",
    "    'Public Lands': 'issues related to national parks, memorials, historic sites, and protected areas, including the management and staffing of cultural sites; museums; use of public lands and forests, establishment and management of harbors and marinas; issues related to flood control, forest fires, livestock grazing.',\n",
    "    'Culture': 'issues related to cultural policies, Ministry of Culture, public spending on culture, cultural employees, issues related to support of theatres and artists; allocation of funds from the national lottery, issues related to cultural heritage.',\n",
    "    'Other': 'other topics not mentioning policy agendas, including the procedures of parliamentary meetings, e.g., points of order, voting procedures, meeting logistics; interpersonal speech, e.g., greetings, personal stories, tributes, interjections, arguments between the members; rhetorical speech, e.g., jokes, literary references.'\n",
    "}\n",
    "\n",
    "# Austrian parliament-specific stop words\n",
    "custom_stopwords = [\n",
    "    'mr', 'mrs', 'ms', 'dr', 'madam', 'honourable', 'member', 'members', 'vp', 'sp', 'fp', \n",
    "    'minister', 'speaker', 'deputy', 'president', 'chairman', 'chair', 'schilling', \n",
    "    'secretary', 'lord', 'lady', 'question', 'order', 'point', 'debate', 'motion', 'amendment',\n",
    "    'congratulations', 'congratulate', 'thanks', 'thank', 'say', 'one', 'want', 'know', 'think', \n",
    "    'believe', 'see', 'go', 'come', 'give', 'take', 'people', 'federal', 'government', 'austria', \n",
    "    'austrian', 'committee', 'call', 'said', 'already', 'please', 'request', 'proceed', 'reading',\n",
    "    'course', 'welcome', 'council', 'open', 'written', 'contain', 'items', 'item', 'yes', 'no', \n",
    "    'following', 'next', 'speech', 'year', 'years', 'state', 'also', 'would', 'like', 'may', 'must', \n",
    "    'upon', 'indeed', 'session', 'meeting', 'report', 'commission', 'behalf', 'gentleman', 'gentlemen', \n",
    "    'ladies', 'applause', 'group', 'colleague', 'colleagues', 'issue', 'issues', 'chancellor', 'court', \n",
    "    'ask', 'answer', 'reply', 'regard', 'regarding', 'regards', 'respect', 'respectfully', 'sign', \n",
    "    'shall', 'procedure', 'declare', 'hear', 'minutes', 'speaking', 'close', 'abg', 'mag', 'orf', 'wait'\n",
    "]\n",
    "\n",
    "all_stopwords = list(ENGLISH_STOP_WORDS) + custom_stopwords\n",
    "\n",
    "# Use enhanced segmentation for topic modeling\n",
    "segment_texts = segmented_df.groupby('Segment_ID')['Text'].apply(lambda x: ' '.join(x)).tolist()\n",
    "segment_embeddings = np.array(segmented_df.groupby('Segment_ID')['Segment_Embeddings'].first().tolist())\n",
    "\n",
    "# Configure vectorizer\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=all_stopwords,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "print(f\"🎯 Target categories: {len(label_list)} topics\")\n",
    "print(f\"📊 Topic modeling data prepared:\")\n",
    "print(f\"  • Segments for modeling: {len(segment_texts)}\")\n",
    "print(f\"  • Embedding dimension: {segment_embeddings.shape[1]}\")\n",
    "print(f\"📖 Topic descriptions loaded for enhanced classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HIERARCHICAL BERTOPIC (RECOMMENDED APPROACH) ===\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "def train_hierarchical_bertopic():\n",
    "    \"\"\"Train BERTopic with many subtopics, then map to 22 main categories.\"\"\"\n",
    "    print(\"🏗️ Training Hierarchical BERTopic with HDBSCAN...\")\n",
    "    \n",
    "    # Configure UMAP and HDBSCAN for more granular topics\n",
    "    umap_model = UMAP(n_neighbors=10, n_components=8, metric='cosine', random_state=42)\n",
    "    \n",
    "    # Use HDBSCAN to find clusters automatically.\n",
    "    # A smaller min_cluster_size will result in more, smaller topics.\n",
    "    clustering_model = HDBSCAN(\n",
    "        min_cluster_size=5,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    representation_model = KeyBERTInspired()\n",
    "    \n",
    "    topic_model_hierarchical = BERTopic(\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=clustering_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        min_topic_size=10,  # Align with min_cluster_size\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    topics, probs = topic_model_hierarchical.fit_transform(segment_texts, embeddings=segment_embeddings)\n",
    "    topic_info_hierarchical = topic_model_hierarchical.get_topic_info()\n",
    "    \n",
    "    print(f\"✅ Hierarchical model created {len(topic_info_hierarchical[topic_info_hierarchical['Topic'] != -1])} subtopics\")\n",
    "    \n",
    "    return topic_model_hierarchical, topics, topic_info_hierarchical\n",
    "\n",
    "# Train hierarchical model\n",
    "topic_model_hierarchical, topics_hierarchical, topic_info_hierarchical = train_hierarchical_bertopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09505f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info_hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e80a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLM CLASSIFICATION TO 22 CATEGORIES ===\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = os.path.join(os.pardir, '.env')\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path)\n",
    "    print(f\"✅ Loaded .env file from: {dotenv_path}\")\n",
    "else:\n",
    "    print(\"⚠️ .env file not found. Ensure OPENAI_API_KEY is set in environment.\")\n",
    "\n",
    "def classify_topic_to_22_categories(topic_words, topic_id=-1):\n",
    "    \"\"\"Enhanced classification using detailed topic descriptions.\"\"\"\n",
    "    if not isinstance(topic_words, list) or not topic_words:\n",
    "        return \"Other\"\n",
    "    \n",
    "    keywords_str = ', '.join(topic_words[:12])  # Use top 12 words for better context\n",
    "    \n",
    "    # Create detailed category descriptions for the prompt\n",
    "    category_descriptions = []\n",
    "    for category in label_list:\n",
    "        description = majortopics_description.get(category, f\"Issues related to {category.lower()}\")\n",
    "        category_descriptions.append(f\"• {category}: {description}\")\n",
    "    \n",
    "    categories_text = '\\n'.join(category_descriptions)\n",
    "    \n",
    "    prompt = f\"\"\"You are analyzing topics from parliamentary debates. \n",
    "\n",
    "TOPIC KEYWORDS: {keywords_str}\n",
    "\n",
    "AVAILABLE CATEGORIES WITH DESCRIPTIONS:\n",
    "{categories_text}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Analyze the keywords carefully in the context of parliamentary discussions\n",
    "2. Consider which category description best matches the semantic content of the keywords\n",
    "3. Look for key thematic indicators (e.g., \"economic\", \"health\", \"defense\", \"education\", etc.)\n",
    "4. If keywords relate to parliamentary procedures, interpersonal speech, or don't fit policy areas, choose \"Other\"\n",
    "5. Choose the single best-fitting category\n",
    "\n",
    "RESPONSE: Only output the exact category name from the list above.\"\"\"\n",
    "\n",
    "    try:\n",
    "        if not os.getenv('OPENAI_API_KEY'):\n",
    "            print(f\"Error: OPENAI_API_KEY not set for topic {topic_id}\")\n",
    "            return \"Other\"\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in political science, parliamentary procedures, and policy classification. You excel at accurately mapping topic keywords to policy domains.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.05,  # Very low temperature for consistency\n",
    "            max_tokens=30\n",
    "        )\n",
    "        \n",
    "        classification = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean the response and ensure exact match\n",
    "        classification = classification.replace('\"', '').replace(\"'\", \"\").strip()\n",
    "        \n",
    "        # Exact match check\n",
    "        if classification in label_list:\n",
    "            return classification\n",
    "        \n",
    "        # Fuzzy matching for partial matches\n",
    "        classification_lower = classification.lower()\n",
    "        for category in label_list:\n",
    "            if category.lower() == classification_lower:\n",
    "                return category\n",
    "            # Check if classification contains the category name\n",
    "            if category.lower() in classification_lower or classification_lower in category.lower():\n",
    "                return category\n",
    "                \n",
    "        # Special handling for common variations\n",
    "        category_mapping = {\n",
    "            'macro': 'Macroeconomics',\n",
    "            'economics': 'Macroeconomics', \n",
    "            'economic': 'Macroeeconomics',\n",
    "            'rights': 'Civil Rights',\n",
    "            'welfare': 'Social Welfare',\n",
    "            'social': 'Social Welfare',\n",
    "            'crime': 'Law and Crime',\n",
    "            'legal': 'Law and Crime',\n",
    "            'justice': 'Law and Crime',\n",
    "            'foreign': 'International Affairs',\n",
    "            'international': 'International Affairs',\n",
    "            'trade': 'Foreign Trade',\n",
    "            'government': 'Government Operations',\n",
    "            'administration': 'Government Operations'\n",
    "        }\n",
    "        \n",
    "        for key, mapped_category in category_mapping.items():\n",
    "            if key in classification_lower:\n",
    "                return mapped_category\n",
    "        \n",
    "        return \"Other\"  # Final fallback\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying topic {topic_id}: {e}\")\n",
    "        return \"Other\"\n",
    "\n",
    "def map_topics_to_22_categories(topic_info, approach_name):\n",
    "    \"\"\"Map topics to the 22 predefined categories using LLM classification.\"\"\"\n",
    "    print(f\"🤖 Classifying {approach_name} topics into 22 categories...\")\n",
    "    \n",
    "    topic_info_classified = topic_info.copy()\n",
    "    topic_info_classified['Category_22'] = \"Other\"\n",
    "    \n",
    "    classification_results = []\n",
    "    \n",
    "    for idx, row in topic_info_classified.iterrows():\n",
    "        if row['Topic'] != -1:\n",
    "            topic_words = row['Representation']\n",
    "            classification = classify_topic_to_22_categories(topic_words, row['Topic'])\n",
    "            topic_info_classified.loc[idx, 'Category_22'] = classification\n",
    "            \n",
    "            classification_results.append({\n",
    "                'Topic_ID': row['Topic'],\n",
    "                'Keywords': ', '.join(topic_words[:5]),\n",
    "                'Classification': classification,\n",
    "                'Count': row['Count']\n",
    "            })\n",
    "    \n",
    "    # Count topics per category\n",
    "    category_counts = topic_info_classified[topic_info_classified['Topic'] != -1]['Category_22'].value_counts()\n",
    "    \n",
    "    print(f\"\\n📊 {approach_name} - Classification results:\")\n",
    "    for category in label_list:\n",
    "        count = category_counts.get(category, 0)\n",
    "        if count > 0:\n",
    "            topic_count = len(classification_results)\n",
    "            percentage = (count / topic_count * 100) if topic_count > 0 else 0\n",
    "            print(f\"  • {category:<20}: {count:>2} topics ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    return topic_info_classified\n",
    "\n",
    "# Apply LLM classification\n",
    "print(\"🚀 Applying LLM classification to map subtopics to 22 categories...\")\n",
    "topic_info_classified = map_topics_to_22_categories(topic_info_hierarchical, \"Hierarchical BERTopic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE FINAL 22-TOPIC MAPPING ===\n",
    "\n",
    "def create_final_22_topic_mapping():\n",
    "    \"\"\"Create final mapping to 22 topics.\"\"\"\n",
    "    print(\"🏗️ Creating final 22-topic mapping...\")\n",
    "    \n",
    "    # Create segment-level mapping\n",
    "    segment_topic_map = pd.DataFrame({\n",
    "        'Segment_ID': segmented_df['Segment_ID'].unique(),\n",
    "        'Subtopic_ID': topics_hierarchical,\n",
    "    })\n",
    "    \n",
    "    # Map subtopics to 22 categories\n",
    "    subtopic_to_category = dict(zip(\n",
    "        topic_info_classified['Topic'], \n",
    "        topic_info_classified['Category_22']\n",
    "    ))\n",
    "    \n",
    "    segment_topic_map['Topic_22'] = segment_topic_map['Subtopic_ID'].map(subtopic_to_category)\n",
    "    segment_topic_map['Topic_22'] = segment_topic_map['Topic_22'].fillna('Other')\n",
    "    \n",
    "    # Merge with original data\n",
    "    embeddings_with_22_topics = segmented_df.merge(\n",
    "        segment_topic_map, \n",
    "        on='Segment_ID', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Generate topic statistics\n",
    "    category_stats = embeddings_with_22_topics.groupby('Topic_22').agg({\n",
    "        'Segment_ID': 'nunique',\n",
    "        'Text': 'count'\n",
    "    }).rename(columns={\n",
    "        'Segment_ID': 'Unique_Segments',\n",
    "        'Text': 'Total_Speeches'\n",
    "    }).sort_values('Total_Speeches', ascending=False)\n",
    "    \n",
    "    print(f\"\\n✅ Final 22-topic mapping created!\")\n",
    "    print(f\"📊 Topic distribution:\")\n",
    "    for topic, stats in category_stats.iterrows():\n",
    "        print(f\"  • {topic:<20}: {stats['Total_Speeches']:>4} speeches, {stats['Unique_Segments']:>3} segments\")\n",
    "    \n",
    "    return embeddings_with_22_topics, topic_info_classified, category_stats\n",
    "\n",
    "# Create final mapping\n",
    "final_embeddings, final_topic_info, category_stats = create_final_22_topic_mapping()\n",
    "\n",
    "print(f\"\\n🎉 SUCCESS: Mapped all topics to 22 predefined categories!\")\n",
    "print(f\"📈 Coverage: {len(category_stats)} out of {len(label_list)} categories have content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b85629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL RESULTS ===\n",
    "\n",
    "# Display final topic information\n",
    "print(\"📋 Final Topic Classifications:\")\n",
    "final_display = final_topic_info[final_topic_info['Topic'] != -1][['Topic', 'Count', 'Category_22', 'Representation']].copy()\n",
    "final_display['Keywords'] = final_display['Representation'].apply(lambda x: ', '.join(x[:5]))\n",
    "final_display = final_display.drop('Representation', axis=1).sort_values('Count', ascending=False)\n",
    "\n",
    "print(final_display.head(15).to_string(index=False))\n",
    "\n",
    "# Show category coverage\n",
    "print(f\"\\n📊 Category Coverage Summary:\")\n",
    "print(f\"Categories with content: {len(category_stats)}/{len(label_list)}\")\n",
    "print(f\"Most common categories:\")\n",
    "for category, stats in category_stats.head(10).iterrows():\n",
    "    percentage = (stats['Total_Speeches'] / final_embeddings.shape[0] * 100)\n",
    "    print(f\"  • {category:<20}: {percentage:>5.1f}% of speeches\")\n",
    "\n",
    "# Find and display categories with no content\n",
    "represented_categories = set(category_stats.index)\n",
    "unrepresented_categories = set(label_list) - represented_categories\n",
    "\n",
    "if unrepresented_categories:\n",
    "    print(f\"\\nCategories with no content (0% of speeches):\")\n",
    "    for category in sorted(list(unrepresented_categories)):\n",
    "        print(f\"  • {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f955d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVE RESULTS (OPTIONAL) ===\n",
    "# Uncomment to save the final results\n",
    "\n",
    "# print(\"💾 Saving final results...\")\n",
    "# final_embeddings.to_pickle('data folder/data/AT_with_22_topics_final.pkl')\n",
    "# final_topic_info.to_pickle('data folder/data/topic_info_22_categories.pkl')\n",
    "# category_stats.to_pickle('data folder/data/category_statistics.pkl')\n",
    "# print(\"✅ Results saved!\")\n",
    "\n",
    "print(\"\\n🎉 Analysis complete! Ready to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
