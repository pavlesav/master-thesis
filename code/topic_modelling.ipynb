{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e08b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded data:\n",
      "   • AT: (231759, 32)\n",
      "   • HR: (504338, 32)\n",
      "   • GB: (670912, 29)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Load the data with embeddings (already segmented)\n",
    "AT_combined = pd.read_pickle(r\"data folder\\AT\\AT_final.pkl\")\n",
    "AT_combined.drop(columns=['Segment_ID'], inplace=True)\n",
    "\n",
    "HR_combined= pd.read_pickle(r\"data folder\\HR\\HR_final.pkl\")\n",
    "HR_combined.drop(columns=['Segment_ID'], inplace=True)\n",
    "\n",
    "GB = pd.read_pickle(r\"data folder\\GB\\GB_final.pkl\")\n",
    "\n",
    "print(f\"✅ Loaded data:\")\n",
    "print(f\"   • AT: {AT_combined.shape}\")\n",
    "print(f\"   • HR: {HR_combined.shape}\")\n",
    "print(f\"   • GB: {GB.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e4a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Target categories: 23 topics (including Mix)\n"
     ]
    }
   ],
   "source": [
    "# === BERTOPIC SETUP AND CONFIGURATION ===\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define the 22 target topic categories + Mix category\n",
    "label_dict = {\n",
    "    \"Education\": \"Issues related to educational policies, primary and secondary schools, student loans and education finance, the regulation of colleges and universities, school reforms, teachers, vocational training, evening schools, safety in schools, efforts to improve educational standards, and issues related to libraries, dictionaries, teaching material, research in education\",\n",
    "    \"Technology\": \"Issues related to science and technology transfer and international science cooperation, research policy, government space programs and space exploration, telephones and telecommunication regulation, broadcast media (television, radio, newspapers, films), weather forecasting, geological surveys, computer industry, cyber security.\",\n",
    "    \"Health\": \"Issues related to health care, health care reforms, health insurance, drug industry, medical facilities, medical workers, disease prevention, treatment, and health promotion, drug and alcohol abuse, mental health, research in medicine, medical liability and unfair medical practices.\",\n",
    "    \"Environment\": \"Issues related to environmental policy, drinking water safety, all kinds of pollution (air, noise, soil), waste disposal, recycling, climate change, outdoor environmental hazards (e.g., asbestos), species and forest protection, marine and freshwater environment, hunting, regulation of laboratory or performance animals, land and water resource conservation, research in environmental technology.\",\n",
    "    \"Housing\": \"Issues related to housing, urban affairs and community development, housing market, property tax, spatial planning, rural development, location permits, construction inspection, illegal construction, industrial and commercial building issues, national housing policy, housing for low-income individuals, rental housing, housing for the elderly, e.g., nursing homes, housing for the homeless and efforts to reduce homelessness, research related to housing, construction inspection, illegal construction, industrial and commercial building issues, national housing policy, housing for low-income individuals, rental housing, housing for the elderly, e.g., nursing homes, housing for the homeless and efforts to reduce homelessness, research related to housing.\",\n",
    "    \"Labor\": \"Issues related to labor, employment, employment programs, employee benefits, pensions and retirement accounts, minimum wage, labor law, job training, labor unions, worker safety and protection, youth employment and seasonal workers.\",\n",
    "    \"Defense\": \"Issues related to defense policy, military intelligence, espionage, weapons, military personnel, reserve forces, military buildings, military courts, nuclear weapons, civil defense, including firefighters and mountain rescue services, homeland security, military aid or arms sales to other countries, prisoners of war and collateral damage to civilian populations, military nuclear and hazardous waste disposal and military environmental compliance, defense alliances and agreements, direct foreign military operations, claims against military, defense research.\",\n",
    "    \"Government Operations\": \"Issues related to general government operations, the work of multiple departments, public employees, postal services, nominations and appointments, national mints, medals, and commemorative coins, management of government property, government procurement and contractors, public scandal and impeachment, claims against the government, the state inspectorate and audit, anti-corruption policies, regulation of political campaigns, political advertising and voter registration, census and statistics collection by government; issues related to local government, capital city and municipalities, including decentralization; issues related to national holidays.\",\n",
    "    \"Social Welfare\": \"Issues related to social welfare policy, the Ministry of Social Affairs, social services, poverty assistance for low-income families and for the elderly, parental leave and child care, assistance for people with physical or mental disabilities, including early retirement pension, discounts on public services, volunteer associations (e.g., Red Cross), charities, and youth organizations.\",\n",
    "    \"Macroeconomics\": \"Issues related to domestic macroeconomic policy, such as the state and prospect of the national economy, economic policy,inflation, interest rates, monetary policy, cost of living, unemployment rate, national budget, public debt, price control, tax enforcement, industrial revitalization and growth.\",\n",
    "    \"Domestic Commerce\": \"Issues related to banking, finance and internal commerce, including stock exchange, investments, consumer finance, mortgages, credit cards, insurance availability and cost, accounting regulation, personal, commercial, and municipal bankruptcies, programs to promote small businesses, copyrights and patents, intellectual property, natural disaster preparedness and relief, consumer safety; regulation and promotion of tourism, sports, gambling, and personal fitness; domestic commerce research.\",\n",
    "    \"Civil Rights\": \"Issues related to civil rights and minority rights, discrimination towards races, gender, sexual orientation, handicap, and other minorities, voting rights, freedom of speech, religious freedoms, privacy rights, protection of personal data, abortion rights, anti-government activity groups (e.g., local insurgency groups), religion and the Church.\",\n",
    "    \"International Affairs\": \"Issues related to international affairs, foreign policy and relations to other countries, issues related to the Ministry of Foreign Affairs, foreign aid, international agreements (such as Kyoto agreement on the environment, the Schengen agreement), international organizations (including United Nations, UNESCO, International Olympic Committee, International Criminal Court), NGOs, issues related to diplomacy, embassies, citizens abroad; issues related to border control; issues related to international finance, including the World Bank and International Monetary Fund, the financial situation of the EU; issues related to a foreign country that do not impact the home country; issues related to human rights in other countries, international terrorism.\",\n",
    "    \"Transportation\": \"Issues related to mass transportation construction and regulation, bus transport, regulation related to motor vehicles, road construction, maintenance and safety, parking facilities, traffic accidents statistics, air travel, rail travel, rail freight, maritime transportation, inland waterways and channels, transportation research and development.\",\n",
    "    \"Immigration\": \"Issues related to immigration, refugees, and citizenship, integration issues, regulation of residence permits, asylum applications; criminal offences and diseases caused by immigration.\",\n",
    "    \"Law and Crime\": \"Issues related to the control, prevention, and impact of crime; all law enforcement agencies, including border and customs, police, court system, prison system; terrorism, white collar crime, counterfeiting and fraud, cyber-crime, drug trafficking, domestic violence, child welfare, family law, juvenile crime.\",\n",
    "    \"Agriculture\": \" Issues related to agriculture policy, fishing, agricultural foreign trade, food marketing, subsidies to farmers, food inspection and safety, animal and crop disease, pest control and pesticide regulation, welfare for animals in farms, pets, veterinary medicine, agricultural research.\",\n",
    "    \"Foreign Trade\": \"Issues related to foreign trade, trade negotiations, free trade agreements, import regulation, export promotion and regulation, subsidies, private business investment and corporate development, competitiveness, exchange rates, the strength of national currency in comparison to other currencies, foreign investment and sales of companies abroad.\",\n",
    "    \"Culture\": \"Issues related to cultural policies, Ministry of Culture, public spending on culture, cultural employees, issues related to support of theatres and artists; allocation of funds from the national lottery, issues related to cultural heritage.\",\n",
    "    \"Public Lands\": \"Issues related to national parks, memorials, historic sites, and protected areas, including the management and staffing of cultural sites; museums; use of public lands and forests, establishment and management of harbors and marinas; issues related to flood control, forest fires, livestock grazing.\",\n",
    "    \"Energy\": \"Issues related to energy policy, electricity, regulation of electrical utilities, nuclear energy and disposal of nuclear waste, natural gas and oil, drilling, oil spills, oil and gas prices, heat supply, shortages and gasoline regulation, coal production, alternative and renewable energy, energy conservation and energy efficiency, energy research.\",\n",
    "    \"Other\": \"Other topics not mentioning policy agendas, including the procedures of parliamentary meetings, e.g., points of order, voting procedures, meeting logistics; interpersonal speech, e.g., greetings, personal stories, tributes, interjections, arguments between the members; rhetorical speech, e.g., jokes, literary references.\",\n",
    "    \"Mix\": \"Use this category when the topic clearly spans multiple policy areas or when there is significant uncertainty about which single category best fits the topic. This is for topics that genuinely combine elements from 2-3 different categories in a meaningful way, making it difficult to assign to just one category with high confidence.\"\n",
    "}\n",
    "\n",
    "print(f\"🎯 Target categories: {len(label_dict.keys())} topics (including Mix)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3201f270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Stopwords configured:\n",
      "   • English: 417 words\n",
      "   • German: 272 words\n",
      "   • Croatian: 219 words\n"
     ]
    }
   ],
   "source": [
    "# === STOPWORDS CONFIGURATION ===\n",
    "\n",
    "# Enhanced stopword lists with comprehensive coverage\n",
    "english_custom_stopwords = [\n",
    "    'mr', 'mrs', 'ms', 'dr', 'madam', 'honorable', 'honourable', 'member', 'members', 'vp', 'sp', 'fp', \n",
    "    'minister', 'speaker', 'deputy', 'president', 'chairman', 'chair', 'schilling', \n",
    "    'secretary', 'lord', 'lady', 'question', 'order', 'point', 'debate', 'motion', 'amendment',\n",
    "    'congratulations', 'congratulate', 'thanks', 'thank', 'say', 'one', 'want', 'know', 'think', \n",
    "    'believe', 'see', 'go', 'come', 'give', 'take', 'people', 'federal', 'government', 'austria', \n",
    "    'austrian', 'committee', 'call', 'said', 'already', 'please', 'request', 'proceed', 'reading',\n",
    "    'course', 'welcome', 'council', 'open', 'written', 'contain', 'items', 'item', 'yes', 'no', \n",
    "    'following', 'next', 'speech', 'year', 'years', 'state', 'also', 'would', 'like', 'may', 'must', \n",
    "    'upon', 'indeed', 'session', 'meeting', 'report', 'commission', 'behalf', 'gentleman', 'gentlemen', \n",
    "    'ladies', 'applause', 'group', 'colleague', 'colleagues', 'issue', 'issues', 'chancellor', 'court', \n",
    "    'ask', 'answer', 'reply', 'regard', 'regarding', 'regards', 'respect', 'respectfully', 'sign', \n",
    "    'shall', 'procedure', 'declare', 'hear', 'minutes', 'speaking', 'close', 'abg', 'mag', 'orf', 'wait'\n",
    "]\n",
    "\n",
    "german_custom_stopwords = [\n",
    "    'der', 'die', 'das', 'und', 'in', 'zu', 'den', 'mit', 'von', 'für', \n",
    "    'auf', 'ist', 'im', 'sich', 'eine', 'sie', 'dem', 'nicht', 'ein', 'als',\n",
    "    'auch', 'es', 'an', 'werden', 'aus', 'er', 'hat', 'dass', 'wir', 'ich',\n",
    "    'haben', 'sind', 'kann', 'sehr', 'meine', 'muss', 'doch', 'wenn', 'sein',\n",
    "    'dann', 'weil', 'bei', 'nach', 'so', 'oder', 'aber', 'vor', 'über', 'noch',\n",
    "    'nur', 'wie', 'war', 'waren', 'wird', 'wurde', 'wurden', 'ihr', 'ihre',\n",
    "    'ihren', 'seiner', 'seine', 'seinem', 'seinen', 'dieser', 'diese', 'dieses',\n",
    "    'durch', 'ohne', 'gegen', 'unter', 'zwischen', 'während', 'bis', 'seit',\n",
    "    'danke', 'bitte', 'gern', 'abgeordnete', 'abgeordneten', 'bundesregierung',\n",
    "    'bundeskanzler', 'nationalrat', 'bundesrat', 'parlament', 'fraktion',\n",
    "    'ausschuss', 'sitzung', 'präsident', 'vizepräsident', 'minister',\n",
    "    'staatssekretär', 'klubobmann', 'antrag', 'anfrage', 'interpellation',\n",
    "    'dringliche', 'aktuelle', 'stunde', 'debatte', 'abstimmung', 'beschluss',\n",
    "    'gesetz', 'novelle', 'verordnung', 'regierungsvorlage', 'initiativantrag',\n",
    "    'danke', 'dankeschön', 'geschätzte', 'kolleginnen', 'kollegen', 'hohes'\n",
    "]\n",
    "\n",
    "croatian_custom_stopwords = [\n",
    "    'a', 'ako', 'ali', 'bi', 'bih', 'bila', 'bili', 'bilo', 'bio', 'bismo', \n",
    "    'biste', 'biti', 'bumo', 'da', 'do', 'duž', 'ga', 'hoće', 'hoćemo', \n",
    "    'hoćete', 'hoćeš', 'hoću', 'i', 'iako', 'ih', 'ili', 'iz', 'ja', 'je', \n",
    "    'jedna', 'jedne', 'jedno', 'jer', 'jesam', 'jesi', 'jesmo', 'jest', \n",
    "    'jeste', 'jesu', 'jim', 'joj', 'još', 'ju', 'kada', 'kako', 'kao', \n",
    "    'koja', 'koje', 'koji', 'kojima', 'koju', 'kroz', 'li', 'me', 'mene', \n",
    "    'meni', 'mi', 'mimo', 'moj', 'moja', 'moje', 'mu', 'na', 'nad', 'nakon', \n",
    "    'nam', 'nama', 'nas', 'naš', 'naša', 'naše', 'našeg', 'ne', 'nego', \n",
    "    'neka', 'neki', 'nekog', 'neku', 'nema', 'netko', 'neće', 'nećemo', \n",
    "    'nećete', 'nećeš', 'neću', 'nešto', 'ni', 'nije', 'nikoga', 'nikoje', \n",
    "    'nikoju', 'nisam', 'nisi', 'nismo', 'niste', 'nisu', 'njega', 'njegov', \n",
    "    'njegova', 'njegovo', 'njemu', 'njezin', 'njezina', 'njezino', 'njih', \n",
    "    'njihov', 'njihova', 'njihovo', 'njim', 'njima', 'njoj', 'nju', 'no', \n",
    "    'o', 'od', 'odmah', 'on', 'ona', 'oni', 'ono', 'ova', 'pa', 'pak', \n",
    "    'po', 'pod', 'pored', 'prije', 's', 'sa', 'sam', 'samo', 'se', 'sebe', \n",
    "    'sebi', 'si', 'smo', 'ste', 'su', 'sve', 'svi', 'svog', 'svoj', 'svoja', \n",
    "    'svoje', 'svom', 'ta', 'tada', 'taj', 'tako', 'te', 'tebe', 'tebi', \n",
    "    'ti', 'to', 'toj', 'tome', 'tu', 'tvoj', 'tvoja', 'tvoje', 'u', 'uz', \n",
    "    'vam', 'vama', 'vas', 'vaš', 'vaša', 'vaše', 'već', 'vi', 'vrlo', 'za', \n",
    "    'zar', 'će', 'ćemo', 'ćete', 'ćeš', 'ću', 'što', 'zastupnik', 'zastupnica', \n",
    "    'zastupnici', 'hvala', 'sabor', 'hrvatska', 'vlada', 'molim', 'gospodin', \n",
    "    'gospođa', 'premijer', 'predsjednik', 'predsjednica', 'ministar', 'ministrica',\n",
    "    'državni', 'tajnik', 'tajnica', 'odbor', 'sjednica', 'rasprava', 'prijedlog', \n",
    "    'zakon', 'odluka', 'glasovanje', 'amandman', 'interpelacija', 'pitanje', \n",
    "    'odgovor', 'klupski', 'obnašatelj', 'dužnosti', 'potpredsjednik', \n",
    "    'potpredsjednica', 'kolegice', 'kolege', 'dame', 'gospodo', 'poštovani', 'poštovana'\n",
    "]\n",
    "\n",
    "# Combine with NLTK stopwords\n",
    "german_nltk_stopwords = stopwords.words('german')\n",
    "all_german_stopwords = list(set(german_nltk_stopwords + german_custom_stopwords))\n",
    "all_croatian_stopwords = list(set(croatian_custom_stopwords))\n",
    "all_english_stopwords = list(set(list(ENGLISH_STOP_WORDS) + english_custom_stopwords))\n",
    "\n",
    "print(f\"📚 Stopwords configured:\")\n",
    "print(f\"   • English: {len(all_english_stopwords)} words\")\n",
    "print(f\"   • German: {len(all_german_stopwords)} words\")\n",
    "print(f\"   • Croatian: {len(all_croatian_stopwords)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11e7e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Optimized topic modeling functions configured\n"
     ]
    }
   ],
   "source": [
    "# === OPTIMIZED TOPIC MODELING FUNCTIONS ===\n",
    "from openai import OpenAI\n",
    "\n",
    "def classify_topic_enhanced(topic_words, topic_id=-1):\n",
    "    \"\"\"Enhanced classification with internal reasoning and confidence filtering.\"\"\"\n",
    "    keywords_str = ', '.join(topic_words[:20])  # Use top 20 words\n",
    "    categories_detailed = '\\n'.join([f\"• {cat}: {desc}\" for cat, desc in label_dict.items()])\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these parliamentary debate keywords and classify into ONE category.\n",
    "\n",
    "KEYWORDS: {keywords_str}\n",
    "\n",
    "CATEGORIES:\n",
    "{categories_detailed}\n",
    "\n",
    "Instructions:\n",
    "1. Think step-by-step about what policy domain these keywords represent\n",
    "2. Consider which government ministry would handle these issues\n",
    "3. Look for domain-specific terminology (e.g., \"medical\" → Health, \"school/university\" → Education)\n",
    "4. If keywords clearly span multiple domains or you're uncertain, use \"Mix\"\n",
    "5. Use \"Other\" only for procedural/non-policy content\n",
    "\n",
    "First reason through your decision, then provide your final classification.\n",
    "\n",
    "Format:\n",
    "REASONING: [your step-by-step analysis]\n",
    "CATEGORY: [exact category name]\n",
    "CONFIDENCE: [HIGH/MEDIUM/LOW]\"\"\"\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert parliamentary policy classifier. Always reason through your decision first, then provide category and confidence level.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.02,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    response_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Parse response\n",
    "    category = \"Mix\"\n",
    "    confidence = \"LOW\"\n",
    "    \n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.startswith('CATEGORY:'):\n",
    "            category = line.split(':', 1)[1].strip()\n",
    "        elif line.startswith('CONFIDENCE:'):\n",
    "            confidence = line.split(':', 1)[1].strip()\n",
    "    \n",
    "    # Clean category name\n",
    "    category = category.replace('\"', '').replace(\"'\", \"\").strip()\n",
    "    \n",
    "    if category not in label_dict.keys():\n",
    "        print(f\"⚠️ Warning: OpenAI returned '{category}'. Defaulting to 'Mix'\")\n",
    "        category = \"Mix\"\n",
    "        confidence = \"LOW\"\n",
    "    \n",
    "    # Apply confidence filtering - convert low confidence to \"Other\"\n",
    "    if confidence == \"LOW\":\n",
    "        category = \"Other\"\n",
    "    \n",
    "    return category, confidence\n",
    "\n",
    "def run_bertopic_optimized(df, dataset_name, language, text_column, segment_id_column, embedding_column, min_cluster_size=6):\n",
    "    \"\"\"Optimized BERTopic focused on classification accuracy.\"\"\"\n",
    "    print(f\"\\n🔍 Running Optimized BERTopic for {dataset_name} ({language})\")\n",
    "    print(f\"   Min cluster size: {min_cluster_size}\")\n",
    "    \n",
    "    # Group by segment and aggregate text\n",
    "    print(\"📝 Grouping text by segments...\")\n",
    "    grouped_data = df.groupby(segment_id_column).agg({\n",
    "        text_column: ' '.join,\n",
    "        embedding_column: 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    documents = grouped_data[text_column].tolist()\n",
    "    embeddings = np.array(grouped_data[embedding_column].tolist())\n",
    "    segment_ids = grouped_data[segment_id_column].tolist()\n",
    "    \n",
    "    print(f\"📊 Prepared {len(documents)} segments for topic modeling\")\n",
    "    \n",
    "    # Language-specific stopwords\n",
    "    stopwords_list = {\n",
    "        \"english\": all_english_stopwords,\n",
    "        \"german\": all_german_stopwords,\n",
    "        \"croatian\": all_croatian_stopwords\n",
    "    }.get(language, all_english_stopwords)\n",
    "    \n",
    "    # Optimized components for better clustering\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=stopwords_list,\n",
    "        ngram_range=(1, 3),      # Include phrases\n",
    "        min_df=5,                # Higher threshold\n",
    "        max_df=0.7,              # Remove very common terms\n",
    "        max_features=15000,      # Larger vocabulary\n",
    "        lowercase=True,\n",
    "        strip_accents='unicode'\n",
    "    )\n",
    "    \n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=20,          # Larger neighborhood\n",
    "        n_components=15,         # More dimensions\n",
    "        min_dist=0.1,\n",
    "        metric='cosine',\n",
    "        random_state=42,\n",
    "        low_memory=True\n",
    "    )\n",
    "    \n",
    "    # Fixed HDBSCAN configuration - removed prediction_data=True to avoid conflict\n",
    "    hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=1,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        cluster_selection_epsilon=0.02,\n",
    "        allow_single_cluster=False\n",
    "    )\n",
    "    \n",
    "    # BERTopic model - disable calculate_probabilities to avoid the conflict\n",
    "    topic_model = BERTopic(\n",
    "        top_n_words=20,          # Extract 20 words for classification\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        verbose=True,\n",
    "        calculate_probabilities=False,  # Disabled to avoid conflict\n",
    "        embedding_model=None\n",
    "    )\n",
    "    \n",
    "    print(\"🤖 Fitting optimized BERTopic model...\")\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    topics, _ = topic_model.fit_transform(documents, embeddings)\n",
    "    \n",
    "    # Statistics\n",
    "    topics_array = np.array(topics)\n",
    "    outlier_count = (topics_array == -1).sum()\n",
    "    outlier_percentage = (outlier_count / len(topics_array)) * 100\n",
    "    \n",
    "    print(f\"✅ Topic modeling completed!\")\n",
    "    print(f\"   Found {len(set(topics))} topics (outliers: {outlier_count}/{len(topics_array)} = {outlier_percentage:.1f}%)\")\n",
    "    \n",
    "    # Topic processing - only classification, no naming\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    print(\"🤖 Classifying topics with enhanced OpenAI...\")\n",
    "    topic_categories = {}\n",
    "    topic_confidences = {}\n",
    "    \n",
    "    for idx, row in topic_info.iterrows():\n",
    "        topic_id = row['Topic']\n",
    "        \n",
    "        if topic_id == -1:\n",
    "            topic_categories[topic_id] = \"Other\"\n",
    "            topic_confidences[topic_id] = \"HIGH\"\n",
    "            print(f\"   Topic {topic_id}: [OUTLIERS] → Other\")\n",
    "            continue\n",
    "        \n",
    "        topic_words = [word for word, _ in topic_model.get_topic(topic_id)]\n",
    "        \n",
    "        # Enhanced classification with confidence filtering\n",
    "        category, confidence = classify_topic_enhanced(topic_words, topic_id)\n",
    "        topic_categories[topic_id] = category\n",
    "        topic_confidences[topic_id] = confidence\n",
    "        \n",
    "        print(f\"   Topic {topic_id}: → {category} ({confidence})\")\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    # Enhanced topic_info\n",
    "    topic_info['Category'] = topic_info['Topic'].map(topic_categories)\n",
    "    topic_info['Classification_Confidence'] = topic_info['Topic'].map(topic_confidences)\n",
    "    \n",
    "    # Reorder columns\n",
    "    cols = list(topic_info.columns)\n",
    "    new_cols = ['Topic', 'Category', 'Classification_Confidence'] + [col for col in cols if col not in ['Topic', 'Category', 'Classification_Confidence']]\n",
    "    topic_info = topic_info[new_cols]\n",
    "    \n",
    "    # Create segment mapping (without probabilities since we disabled them)\n",
    "    segment_topics = pd.DataFrame({\n",
    "        segment_id_column: segment_ids,\n",
    "        f'Segment_Topic_{dataset_name}_{language}': topics,\n",
    "        f'Segment_Category_{dataset_name}_{language}': [topic_categories.get(t, \"Other\") for t in topics],\n",
    "        f'Segment_Classification_Confidence_{dataset_name}_{language}': [topic_confidences.get(t, \"LOW\") for t in topics]\n",
    "    })\n",
    "    \n",
    "    # Merge back\n",
    "    df_result = df.merge(segment_topics, on=segment_id_column, how='left')\n",
    "    \n",
    "    return df_result, topic_model, topic_info\n",
    "\n",
    "print(\"🔧 Optimized topic modeling functions configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d25238",
   "metadata": {},
   "source": [
    "# Topic Modeling Execution\n",
    "\n",
    "Using the optimized approach for all datasets with enhanced classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a2ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting optimized topic modeling for all datasets...\n",
      "\n",
      "🇬🇧 British Parliament\n",
      "\n",
      "🔍 Running Optimized BERTopic for GB (english)\n",
      "   Min cluster size: 5\n",
      "📝 Grouping text by segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 13:27:39,743 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Prepared 33381 segments for topic modeling\n",
      "🤖 Fitting optimized BERTopic model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 13:29:10,607 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-10-07 13:29:10,611 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-10-07 13:29:10,611 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-10-07 13:29:13,682 - BERTopic - Cluster - Completed ✓\n",
      "2025-10-07 13:29:13,717 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-10-07 13:29:13,682 - BERTopic - Cluster - Completed ✓\n",
      "2025-10-07 13:29:13,717 - BERTopic - Representation - Fine-tuning topics using representation models.\n"
     ]
    }
   ],
   "source": [
    "# === PROCESS ALL DATASETS WITH OPTIMIZED APPROACH ===\n",
    "print(\"🚀 Starting optimized topic modeling for all datasets...\")\n",
    "\n",
    "# === GB DATASET (English only) ===\n",
    "print(\"\\n🇬🇧 British Parliament\")\n",
    "GB_processed, gb_model, gb_topics = run_bertopic_optimized(\n",
    "    GB, \"GB\", \"english\", \"Text\", \"Segment_ID\", \"segment_embeddings_english\", min_cluster_size=5\n",
    ")\n",
    "GB_processed = GB_processed.rename(columns={\n",
    "    'Segment_Category_GB_english': 'my_topic'\n",
    "})\n",
    "\n",
    "# Save GB immediately\n",
    "print(\"💾 Saving GB dataset...\")\n",
    "pd.to_pickle(GB_processed, r\"data folder\\GB\\GB_with_topics.pkl\")\n",
    "pd.to_pickle(gb_topics, r\"data folder\\GB\\GB_topic_info.pkl\")\n",
    "print(f\"✅ Saved GB: {GB_processed.shape} segments → data folder\\\\GB\\\\GB_with_topics.pkl\")\n",
    "print(f\"✅ Saved GB topics: {gb_topics.shape} → data folder\\\\GB\\\\GB_topic_info.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AT DATASET (English + German) ===\n",
    "print(\"\\n🇦🇹 Austrian Parliament - English\")\n",
    "AT_processed, at_en_model, at_en_topics = run_bertopic_optimized(\n",
    "    AT_combined, \"AT\", \"english\", \"Text\", \"Segment_ID_english\", \"segment_embeddings_english\", min_cluster_size=4\n",
    ")\n",
    "\n",
    "# Save AT English immediately\n",
    "print(\"💾 Saving AT English topics...\")\n",
    "pd.to_pickle(at_en_topics, r\"data folder\\AT\\AT_topic_info_english.pkl\")\n",
    "print(f\"✅ Saved AT English topics: {at_en_topics.shape} → data folder\\\\AT\\\\AT_topic_info_english.pkl\")\n",
    "\n",
    "print(\"\\n🇦🇹 Austrian Parliament - German\")\n",
    "AT_processed, at_de_model, at_de_topics = run_bertopic_optimized(\n",
    "    AT_processed, \"AT\", \"german\", \"Text_native_language\", \"Segment_ID_english\", \"segment_embeddings_native_language\", min_cluster_size=4\n",
    ")\n",
    "\n",
    "AT_processed = AT_processed.rename(columns={\n",
    "    'Segment_Category_AT_english': 'my_topic_en',\n",
    "    'Segment_Category_AT_german': 'my_topic_native_language'\n",
    "})\n",
    "\n",
    "# Save AT complete dataset immediately\n",
    "print(\"💾 Saving AT complete dataset...\")\n",
    "pd.to_pickle(AT_processed, r\"data folder\\AT\\AT_with_topics.pkl\")\n",
    "pd.to_pickle(at_de_topics, r\"data folder\\AT\\AT_topic_info_german.pkl\")\n",
    "print(f\"✅ Saved AT complete: {AT_processed.shape} segments → data folder\\\\AT\\\\AT_with_topics.pkl\")\n",
    "print(f\"✅ Saved AT German topics: {at_de_topics.shape} → data folder\\\\AT\\\\AT_topic_info_german.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf331e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HR DATASET (English + Croatian) ===\n",
    "print(\"\\n🇭🇷 Croatian Parliament - English\")\n",
    "HR_processed, hr_en_model, hr_en_topics = run_bertopic_optimized(\n",
    "    HR_combined, \"HR\", \"english\", \"Text\", \"Segment_ID_english\", \"segment_embeddings_english\", min_cluster_size=5\n",
    ")\n",
    "\n",
    "# Save HR English immediately\n",
    "print(\"💾 Saving HR English topics...\")\n",
    "pd.to_pickle(hr_en_topics, r\"data folder\\HR\\HR_topic_info_english.pkl\")\n",
    "print(f\"✅ Saved HR English topics: {hr_en_topics.shape} → data folder\\\\HR\\\\HR_topic_info_english.pkl\")\n",
    "\n",
    "print(\"\\n🇭🇷 Croatian Parliament - Croatian\")\n",
    "HR_processed, hr_hr_model, hr_hr_topics = run_bertopic_optimized(\n",
    "    HR_processed, \"HR\", \"croatian\", \"Text_native_language\", \"Segment_ID_english\", \"segment_embeddings_native_language\", min_cluster_size=5\n",
    ")\n",
    "\n",
    "HR_processed = HR_processed.rename(columns={\n",
    "    'Segment_Category_HR_english': 'my_topic_en',\n",
    "    'Segment_Category_HR_croatian': 'my_topic_native_language'\n",
    "})\n",
    "\n",
    "# Save HR complete dataset immediately\n",
    "print(\"💾 Saving HR complete dataset...\")\n",
    "pd.to_pickle(HR_processed, r\"data folder\\HR\\HR_with_topics.pkl\")\n",
    "pd.to_pickle(hr_hr_topics, r\"data folder\\HR\\HR_topic_info_croatian.pkl\")\n",
    "print(f\"✅ Saved HR complete: {HR_processed.shape} segments → data folder\\\\HR\\\\HR_with_topics.pkl\")\n",
    "print(f\"✅ Saved HR Croatian topics: {hr_hr_topics.shape} → data folder\\\\HR\\\\HR_topic_info_croatian.pkl\")\n",
    "\n",
    "print(\"\\n🎉 All optimized topic modeling completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb806db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL SUMMARY ===\n",
    "print(\"📊 Processing Summary:\")\n",
    "print(f\"   • GB: {len(GB_processed):,} segments with topics ✅ SAVED\")\n",
    "print(f\"   • AT: {len(AT_processed):,} segments with topics (EN + DE) ✅ SAVED\")\n",
    "print(f\"   • HR: {len(HR_processed):,} segments with topics (EN + HR) ✅ SAVED\")\n",
    "print(f\"   • Total: 5 topic info files saved ✅\")\n",
    "\n",
    "print(\"\\n📈 Topic Distribution:\")\n",
    "# GB distribution\n",
    "if 'my_topic' in GB_processed.columns:\n",
    "    gb_dist = GB_processed['my_topic'].value_counts()\n",
    "    print(f\"\\n🇬🇧 GB: {len(gb_dist)} categories, top: {gb_dist.head(3).to_dict()}\")\n",
    "\n",
    "# AT distribution\n",
    "if 'my_topic_en' in AT_processed.columns:\n",
    "    at_en_dist = AT_processed['my_topic_en'].value_counts()\n",
    "    print(f\"🇦🇹 AT (EN): {len(at_en_dist)} categories, top: {at_en_dist.head(3).to_dict()}\")\n",
    "    \n",
    "if 'my_topic_native_language' in AT_processed.columns:\n",
    "    at_de_dist = AT_processed['my_topic_native_language'].value_counts()\n",
    "    print(f\"🇦🇹 AT (DE): {len(at_de_dist)} categories, top: {at_de_dist.head(3).to_dict()}\")\n",
    "\n",
    "# HR distribution\n",
    "if 'my_topic_en' in HR_processed.columns:\n",
    "    hr_en_dist = HR_processed['my_topic_en'].value_counts()\n",
    "    print(f\"🇭🇷 HR (EN): {len(hr_en_dist)} categories, top: {hr_en_dist.head(3).to_dict()}\")\n",
    "    \n",
    "if 'my_topic_native_language' in HR_processed.columns:\n",
    "    hr_hr_dist = HR_processed['my_topic_native_language'].value_counts()\n",
    "    print(f\"🇭🇷 HR (HR): {len(hr_hr_dist)} categories, top: {hr_hr_dist.head(3).to_dict()}\")\n",
    "\n",
    "print(\"\\n🎯 Next Steps:\")\n",
    "print(\"1. Calculate F1 scores and compare with benchmark (0.75)\")\n",
    "print(\"2. Analyze cross-language consistency\")\n",
    "print(\"3. Fine-tune parameters if needed\")\n",
    "print(\"4. Use visualization.ipynb for detailed analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
