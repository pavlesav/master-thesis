{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfcaf4b",
   "metadata": {},
   "source": [
    "# Parliamentary Speech Topic Modeling\n",
    "\n",
    "Applies BERTopic with GMM clustering to discover topics, then uses GPT-4 to classify them into 23 policy categories.\n",
    "\n",
    "**Input**: Processed data from data_preprocessing.ipynb  \n",
    "**Output**: Same dataframes with added topic classification columns  \n",
    "**Method**: Segment-level topic modeling with GPM + OpenAI classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550298e",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0823b2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "   Policy categories: 23\n",
      "   Stopwords: EN=130, DE=116, HR=219\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Policy categories for classification (full CAP descriptions)\n",
    "POLICY_CATEGORIES = {\n",
    "    \"Education\": \"Issues related to educational policies, primary and secondary schools, student loans and education finance, the regulation of colleges and universities, school reforms, teachers, vocational training, evening schools, safety in schools, efforts to improve educational standards, and issues related to libraries, dictionaries, teaching material, research in education\",\n",
    "    \"Technology\": \"Issues related to science and technology transfer and international science cooperation, research policy, government space programs and space exploration, telephones and telecommunication regulation, broadcast media (television, radio, newspapers, films), weather forecasting, geological surveys, computer industry, cyber security\",\n",
    "    \"Health\": \"Issues related to health care, health care reforms, health insurance, drug industry, medical facilities, medical workers, disease prevention, treatment, and health promotion, drug and alcohol abuse, mental health, research in medicine, medical liability and unfair medical practices\",\n",
    "    \"Environment\": \"Issues related to environmental policy, drinking water safety, all kinds of pollution (air, noise, soil), waste disposal, recycling, climate change, outdoor environmental hazards (e.g., asbestos), species and forest protection, marine and freshwater environment, hunting, regulation of laboratory or performance animals, land and water resource conservation, research in environmental technology\",\n",
    "    \"Housing\": \"Issues related to housing, urban affairs and community development, housing market, property tax, spatial planning, rural development, location permits, construction inspection, illegal construction, industrial and commercial building issues, national housing policy, housing for low-income individuals, rental housing, housing for the elderly, e.g., nursing homes, housing for the homeless and efforts to reduce homelessness, research related to housing\",\n",
    "    \"Labor\": \"Issues related to labor, employment, employment programs, employee benefits, pensions and retirement accounts, minimum wage, labor law, job training, labor unions, worker safety and protection, youth employment and seasonal workers\",\n",
    "    \"Defense\": \"Issues related to defense policy, military intelligence, espionage, weapons, military personnel, reserve forces, military buildings, military courts, nuclear weapons, civil defense, including firefighters and mountain rescue services, homeland security, military aid or arms sales to other countries, prisoners of war and collateral damage to civilian populations, military nuclear and hazardous waste disposal and military environmental compliance, defense alliances and agreements, direct foreign military operations, claims against military, defense research\",\n",
    "    \"Government Operations\": \"Issues related to general government operations, the work of multiple departments, public employees, postal services, nominations and appointments, national mints, medals, and commemorative coins, management of government property, government procurement and contractors, public scandal and impeachment, claims against the government, the state inspectorate and audit, anti-corruption policies, regulation of political campaigns, political advertising and voter registration, census and statistics collection by government; issues related to local government, capital city and municipalities, including decentralization; issues related to national holidays\",\n",
    "    \"Social Welfare\": \"Issues related to social welfare policy, the Ministry of Social Affairs, social services, poverty assistance for low-income families and for the elderly, parental leave and child care, assistance for people with physical or mental disabilities, including early retirement pension, discounts on public services, volunteer associations (e.g., Red Cross), charities, and youth organizations\",\n",
    "    \"Macroeconomics\": \"Issues related to domestic macroeconomic policy, such as the state and prospect of the national economy, economic policy, inflation, interest rates, monetary policy, cost of living, unemployment rate, national budget, public debt, price control, tax enforcement, industrial revitalization and growth\",\n",
    "    \"Domestic Commerce\": \"Issues related to banking, finance and internal commerce, including stock exchange, investments, consumer finance, mortgages, credit cards, insurance availability and cost, accounting regulation, personal, commercial, and municipal bankruptcies, programs to promote small businesses, copyrights and patents, intellectual property, natural disaster preparedness and relief, consumer safety; regulation and promotion of tourism, sports, gambling, and personal fitness; domestic commerce research\",\n",
    "    \"Civil Rights\": \"Issues related to civil rights and minority rights, discrimination towards races, gender, sexual orientation, handicap, and other minorities, voting rights, freedom of speech, religious freedoms, privacy rights, protection of personal data, abortion rights, anti-government activity groups (e.g., local insurgency groups), religion and the Church\",\n",
    "    \"International Affairs\": \"Issues related to international affairs, foreign policy and relations to other countries, issues related to the Ministry of Foreign Affairs, foreign aid, international agreements (such as Kyoto agreement on the environment, the Schengen agreement), international organizations (including United Nations, UNESCO, International Olympic Committee, International Criminal Court), NGOs, issues related to diplomacy, embassies, citizens abroad; issues related to border control; issues related to international finance, including the World Bank and International Monetary Fund, the financial situation of the EU; issues related to a foreign country that do not impact the home country; issues related to human rights in other countries, international terrorism\",\n",
    "    \"Transportation\": \"Issues related to mass transportation construction and regulation, bus transport, regulation related to motor vehicles, road construction, maintenance and safety, parking facilities, traffic accidents statistics, air travel, rail travel, rail freight, maritime transportation, inland waterways and channels, transportation research and development\",\n",
    "    \"Immigration\": \"Issues related to immigration, refugees, and citizenship, integration issues, regulation of residence permits, asylum applications; criminal offences and diseases caused by immigration\",\n",
    "    \"Law and Crime\": \"Issues related to the control, prevention, and impact of crime; all law enforcement agencies, including border and customs, police, court system, prison system; terrorism, white collar crime, counterfeiting and fraud, cyber-crime, drug trafficking, domestic violence, child welfare, family law, juvenile crime\",\n",
    "    \"Agriculture\": \"Issues related to agriculture policy, fishing, agricultural foreign trade, food marketing, subsidies to farmers, food inspection and safety, animal and crop disease, pest control and pesticide regulation, welfare for animals in farms, pets, veterinary medicine, agricultural research\",\n",
    "    \"Foreign Trade\": \"Issues related to foreign trade, trade negotiations, free trade agreements, import regulation, export promotion and regulation, subsidies, private business investment and corporate development, competitiveness, exchange rates, the strength of national currency in comparison to other currencies, foreign investment and sales of companies abroad\",\n",
    "    \"Culture\": \"Issues related to cultural policies, Ministry of Culture, public spending on culture, cultural employees, issues related to support of theatres and artists; allocation of funds from the national lottery, issues related to cultural heritage\",\n",
    "    \"Public Lands\": \"Issues related to national parks, memorials, historic sites, and protected areas, including the management and staffing of cultural sites; museums; use of public lands and forests, establishment and management of harbors and marinas; issues related to flood control, forest fires, livestock grazing\",\n",
    "    \"Energy\": \"Issues related to energy policy, electricity, regulation of electrical utilities, nuclear energy and disposal of nuclear waste, natural gas and oil, drilling, oil spills, oil and gas prices, heat supply, shortages and gasoline regulation, coal production, alternative and renewable energy, energy conservation and energy efficiency, energy research\",\n",
    "    \"Other\": \"Other topics not mentioning policy agendas, including the procedures of parliamentary meetings, e.g., points of order, voting procedures, meeting logistics; interpersonal speech, e.g., greetings, personal stories, tributes, interjections, arguments between the members; rhetorical speech, e.g., jokes, literary references\",\n",
    "    \"Mix\": \"Use this category when the topic clearly spans multiple policy areas or when there is significant uncertainty about which single category best fits the topic. This is for topics that genuinely combine elements from 2-3 different categories in a meaningful way, making it difficult to assign to just one category with high confidence\"\n",
    "}\n",
    "\n",
    "# Language-specific stopwords (comprehensive lists)\n",
    "ENGLISH_STOPWORDS = [\n",
    "    'mr', 'mrs', 'ms', 'dr', 'madam', 'honorable', 'honourable', 'member', 'members', 'vp', 'sp', 'fp', 'ae', 'po',\n",
    "    'minister', 'speaker', 'deputy', 'president', 'chairman', 'chair', 'schilling', 'my', 'lords', 'lord', 'bzs', 'prll', 'bz',\n",
    "    'secretary', 'lord', 'gp', 'lady', 'question', 'order', 'point', 'debate', 'motion', 'amendment', 'backbench', 'week',\n",
    "    'congratulations', 'congratulate', 'thanks', 'thank', 'say', 'one', 'want', 'know', 'think', 'noble', 'opg',\n",
    "    'believe', 'see', 'go', 'come', 'give', 'take', 'people', 'federal', 'government', 'austria', 'baroness',\n",
    "    'austrian', 'committee', 'call', 'said', 'already', 'please', 'request', 'proceed', 'reading', 'prime',\n",
    "    'course', 'welcome', 'council', 'open', 'written', 'contain', 'items', 'item', 'yes', 'no',\n",
    "    'following', 'next', 'speech', 'year', 'years', 'state', 'also', 'would', 'like', 'may', 'must',\n",
    "    'upon', 'indeed', 'session', 'meeting', 'report', 'commission', 'behalf', 'gentleman', 'gentlemen',\n",
    "    'ladies', 'applause', 'group', 'colleague', 'colleagues', 'issue', 'issues', 'chancellor', 'court',\n",
    "    'ask', 'answer', 'reply', 'regard', 'regarding', 'regards', 'respect', 'respectfully', 'sign',\n",
    "    'shall', 'procedure', 'declare', 'hear', 'minutes', 'speaking', 'close', 'abg', 'mag', 'orf', 'wait'\n",
    "]\n",
    "\n",
    "GERMAN_STOPWORDS = [\n",
    "    'der', 'die', 'das', 'und', 'in', 'zu', 'den', 'mit', 'von', 'f√ºr', 'bb', 'bz', 'bzs', 'prll',\n",
    "    'auf', 'ist', 'im', 'sich', 'eine', 'sie', 'dem', 'nicht', 'ein', 'als',\n",
    "    'auch', 'es', 'an', 'werden', 'aus', 'er', 'hat', 'dass', 'wir', 'ich',\n",
    "    'haben', 'sind', 'kann', 'sehr', 'meine', 'muss', 'doch', 'wenn', 'sein',\n",
    "    'dann', 'weil', 'bei', 'nach', 'so', 'oder', 'aber', 'vor', '√ºber', 'noch',\n",
    "    'nur', 'wie', 'war', 'waren', 'wird', 'wurde', 'wurden', 'ihr', 'ihre',\n",
    "    'ihren', 'seiner', 'seine', 'seinem', 'seinen', 'dieser', 'diese', 'dieses',\n",
    "    'durch', 'ohne', 'gegen', 'unter', 'zwischen', 'w√§hrend', 'bis', 'seit',\n",
    "    'danke', 'bitte', 'gern', 'abgeordnete', 'abgeordneten', 'bundesregierung',\n",
    "    'bundeskanzler', 'nationalrat', 'bundesrat', 'parlament', 'fraktion',\n",
    "    'ausschuss', 'sitzung', 'pr√§sident', 'vizepr√§sident', 'minister',\n",
    "    'staatssekret√§r', 'klubobmann', 'antrag', 'anfrage', 'interpellation',\n",
    "    'dringliche', 'aktuelle', 'stunde', 'debatte', 'abstimmung', 'beschluss',\n",
    "    'gesetz', 'novelle', 'verordnung', 'regierungsvorlage', 'initiativantrag',\n",
    "    'danke', 'dankesch√∂n', 'gesch√§tzte', 'kolleginnen', 'kollegen', 'hohes'\n",
    "]\n",
    "\n",
    "CROATIAN_STOPWORDS = [\n",
    "    'a', 'ako', 'ali', 'bi', 'bih', 'bila', 'bili', 'bilo', 'bio', 'bismo',\n",
    "    'biste', 'biti', 'bumo', 'da', 'do', 'du≈æ', 'ga', 'hoƒáe', 'hoƒáemo',\n",
    "    'hoƒáete', 'hoƒáe≈°', 'hoƒáu', 'i', 'iako', 'ih', 'ili', 'iz', 'ja', 'je',\n",
    "    'jedna', 'jedne', 'jedno', 'jer', 'jesam', 'jesi', 'jesmo', 'jest',\n",
    "    'jeste', 'jesu', 'jim', 'joj', 'jo≈°', 'ju', 'kada', 'kako', 'kao',\n",
    "    'koja', 'koje', 'koji', 'kojima', 'koju', 'kroz', 'li', 'me', 'mene',\n",
    "    'meni', 'mi', 'mimo', 'moj', 'moja', 'moje', 'mu', 'na', 'nad', 'nakon',\n",
    "    'nam', 'nama', 'nas', 'na≈°', 'na≈°a', 'na≈°e', 'na≈°eg', 'ne', 'nego',\n",
    "    'neka', 'neki', 'nekog', 'neku', 'nema', 'netko', 'neƒáe', 'neƒáemo',\n",
    "    'neƒáete', 'neƒáe≈°', 'neƒáu', 'ne≈°to', 'ni', 'nije', 'nikoga', 'nikoje',\n",
    "    'nikoju', 'nisam', 'nisi', 'nismo', 'niste', 'nisu', 'njega', 'njegov',\n",
    "    'njegova', 'njegovo', 'njemu', 'njezin', 'njezina', 'njezino', 'njih',\n",
    "    'njihov', 'njihova', 'njihovo', 'njim', 'njima', 'njoj', 'nju', 'no',\n",
    "    'o', 'od', 'odmah', 'on', 'ona', 'oni', 'ono', 'ova', 'pa', 'pak',\n",
    "    'po', 'pod', 'pored', 'prije', 's', 'sa', 'sam', 'samo', 'se', 'sebe',\n",
    "    'sebi', 'si', 'smo', 'ste', 'su', 'sve', 'svi', 'svog', 'svoj', 'svoja',\n",
    "    'svoje', 'svom', 'ta', 'tada', 'taj', 'tako', 'te', 'tebe', 'tebi',\n",
    "    'ti', 'to', 'toj', 'tome', 'tu', 'tvoj', 'tvoja', 'tvoje', 'u', 'uz',\n",
    "    'vam', 'vama', 'vas', 'va≈°', 'va≈°a', 'va≈°e', 'veƒá', 'vi', 'vrlo', 'za',\n",
    "    'zar', 'ƒáe', 'ƒáemo', 'ƒáete', 'ƒáe≈°', 'ƒáu', '≈°to', 'zastupnik', 'zastupnica',\n",
    "    'zastupnici', 'hvala', 'sabor', 'hrvatska', 'vlada', 'molim', 'gospodin',\n",
    "    'gospoƒëa', 'premijer', 'predsjednik', 'predsjednica', 'ministar', 'ministrica',\n",
    "    'dr≈æavni', 'tajnik', 'tajnica', 'odbor', 'sjednica', 'rasprava', 'prijedlog',\n",
    "    'zakon', 'odluka', 'glasovanje', 'amandman', 'interpelacija', 'pitanje',\n",
    "    'odgovor', 'klupski', 'obna≈°atelj', 'du≈ænosti', 'potpredsjednik',\n",
    "    'potpredsjednica', 'kolegice', 'kolege', 'dame', 'gospodo', 'po≈°tovani', 'po≈°tovana'\n",
    "]\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"english\": ENGLISH_STOPWORDS,\n",
    "    \"german\": GERMAN_STOPWORDS,\n",
    "    \"croatian\": CROATIAN_STOPWORDS\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Policy categories: {len(POLICY_CATEGORIES)}\")\n",
    "print(f\"   Stopwords: EN={len(ENGLISH_STOPWORDS)}, DE={len(GERMAN_STOPWORDS)}, HR={len(CROATIAN_STOPWORDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55404e49",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the processed dataframes from data_preprocessing.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1728eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded from: data folder\n",
      "   AT=(231759, 32), HR=(504338, 32), GB=(670912, 29)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path where data_preprocessing.ipynb saves processed data\n",
    "BASE_DATA_DIR = r\"data folder\"\n",
    "\n",
    "# Load processed datasets\n",
    "AT = pd.read_pickle(os.path.join(BASE_DATA_DIR, \"AT/AT_speeches_processed.pkl\"))\n",
    "HR = pd.read_pickle(os.path.join(BASE_DATA_DIR, \"HR/HR_speeches_processed.pkl\"))\n",
    "GB = pd.read_pickle(os.path.join(BASE_DATA_DIR, \"GB/GB_speeches_processed.pkl\"))\n",
    "\n",
    "print(f\"‚úÖ Loaded from: {BASE_DATA_DIR}\")\n",
    "print(f\"   AT={AT.shape}, HR={HR.shape}, GB={GB.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8e04f",
   "metadata": {},
   "source": [
    "## Topic Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa313950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Functions defined\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "class GMMClustering:\n",
    "    \"\"\"GMM clustering for BERTopic\"\"\"\n",
    "    def __init__(self, n_components=200, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.labels_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        model = GaussianMixture(n_components=self.n_components, random_state=self.random_state,\n",
    "                               covariance_type='tied', max_iter=300)\n",
    "        model.fit(X)\n",
    "        self.labels_ = model.predict(X)\n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "\n",
    "\n",
    "def create_topic_model(language, n_clusters=200):\n",
    "    \"\"\"Create BERTopic model with GMM clustering\"\"\"\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=STOPWORDS.get(language, STOPWORDS['english']),\n",
    "        ngram_range=(1, 2), min_df=5, max_df=0.9, max_features=20000\n",
    "    )\n",
    "    \n",
    "    umap_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.05, \n",
    "                     metric='cosine', random_state=42)\n",
    "    \n",
    "    gmm_model = GMMClustering(n_components=n_clusters)\n",
    "    \n",
    "    return BERTopic(\n",
    "        vectorizer_model=vectorizer,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=gmm_model,\n",
    "        embedding_model=None,\n",
    "        top_n_words=15,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_segments(df, segment_col, text_col, embedding_col):\n",
    "    \"\"\"Group speeches into segments\"\"\"\n",
    "    grouped = df.groupby(segment_col).agg({\n",
    "        text_col: ' '.join,\n",
    "        embedding_col: 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return (grouped[text_col].tolist(), \n",
    "            np.array(grouped[embedding_col].tolist()),\n",
    "            grouped[segment_col].tolist())\n",
    "\n",
    "\n",
    "def classify_with_gpt(topic_words, country, language, max_retries=3):\n",
    "    \"\"\"Classify topic using GPT-4 - returns both topic name and CAP category\"\"\"\n",
    "    categories_str = '\\n'.join([f\"‚Ä¢ {cat}: {desc}\" for cat, desc in POLICY_CATEGORIES.items()])\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these parliamentary keywords and provide TWO outputs IN ENGLISH:\n",
    "\n",
    "Country: {country} Parliament\n",
    "Source Language: {language}\n",
    "Keywords: {', '.join(topic_words)}\n",
    "\n",
    "IMPORTANT: Regardless of the source language, provide your response entirely in English.\n",
    "\n",
    "TASK 1 - Topic Name: Create a short, descriptive name IN ENGLISH (2-4 words) that captures what this topic is about discussed in the parliamentary meeting.\n",
    "TASK 2 - CAP Classification: Classify into ONE of these policy categories:\n",
    "\n",
    "{categories_str}\n",
    "\n",
    "Instructions:\n",
    "- Always respond in English, even if keywords are in German, Croatian, or other languages\n",
    "- For Topic Name: Be specific and descriptive (e.g., \"Healthcare Reform\", \"Military Defense Budget\")\n",
    "- For CAP Classification: Choose the most specific policy category\n",
    "- Use \"Other\" for procedural/non-policy content\n",
    "- Use \"Mix\" only if clearly spanning multiple domains\n",
    "- Be conservative: default to \"Other\" if uncertain\n",
    "\n",
    "Format your response EXACTLY as:\n",
    "TOPIC: [your English topic name]\n",
    "CATEGORY: [exact category name from list]\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a parliamentary policy classifier. Always respond in English.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.01,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            \n",
    "            text = response.choices[0].message.content.strip()\n",
    "            topic_name = \"Unknown\"\n",
    "            category = \"Other\"\n",
    "            \n",
    "            for line in text.split('\\n'):\n",
    "                if line.startswith('TOPIC:'):\n",
    "                    topic_name = line.split(':', 1)[1].strip().replace('\"', '').replace(\"'\", \"\")\n",
    "                elif line.startswith('CATEGORY:'):\n",
    "                    cat = line.split(':', 1)[1].strip().replace('\"', '').replace(\"'\", \"\")\n",
    "                    if cat in POLICY_CATEGORIES:\n",
    "                        category = cat\n",
    "            \n",
    "            return topic_name, category\n",
    "        except Exception as e:\n",
    "            if \"insufficient_quota\" in str(e) or \"429\" in str(e):\n",
    "                print(f\"‚ö†Ô∏è API quota exceeded for topic with keywords: {', '.join(topic_words[:5])}...\")\n",
    "                return \"Unknown\", \"Other\"\n",
    "            elif attempt < max_retries - 1:\n",
    "                print(f\"‚ö†Ô∏è Retry {attempt + 1}/{max_retries} for error: {e}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                print(f\"‚ùå Error after {max_retries} attempts: {e}\")\n",
    "                return \"Unknown\", \"Other\"\n",
    "    \n",
    "    return \"Unknown\", \"Other\"\n",
    "\n",
    "\n",
    "def optimize_cluster_size(embeddings, cluster_range=None, sample_size=5000):\n",
    "    \"\"\"Find optimal number of clusters using Silhouette Score\"\"\"\n",
    "    if cluster_range is None:\n",
    "        cluster_range = list(range(150, 251, 5)) \n",
    "    \n",
    "    print(f\"\\nOptimizing cluster size (testing: {len(cluster_range)} values from {min(cluster_range)} to {max(cluster_range)})...\")\n",
    "    \n",
    "    # Sample for faster optimization\n",
    "    if len(embeddings) > sample_size:\n",
    "        indices = np.random.choice(len(embeddings), sample_size, replace=False)\n",
    "        X_sample = embeddings[indices]\n",
    "    else:\n",
    "        X_sample = embeddings\n",
    "    \n",
    "    # Reduce dimensionality first\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.05, \n",
    "                     metric='cosine', random_state=42)\n",
    "    X_reduced = umap_model.fit_transform(X_sample.astype(np.float32))\n",
    "    \n",
    "    scores = {}\n",
    "    for n_clusters in cluster_range:\n",
    "        print(f\"  Testing n={n_clusters}...\", end=\" \")\n",
    "        gmm = GaussianMixture(n_components=n_clusters, random_state=42,\n",
    "                             covariance_type='tied', max_iter=300)\n",
    "        labels = gmm.fit_predict(X_reduced)\n",
    "        score = silhouette_score(X_reduced, labels, sample_size=min(2000, len(X_reduced)))\n",
    "        scores[n_clusters] = score\n",
    "        print(f\"silhouette={score:.4f}\")\n",
    "    \n",
    "    optimal_n = max(scores, key=scores.get)\n",
    "    print(f\"\\n‚úÖ Optimal clusters: {optimal_n} (score={scores[optimal_n]:.4f})\")\n",
    "    return optimal_n\n",
    "\n",
    "\n",
    "def run_topic_pipeline(df, country, language, text_col, segment_col, embedding_col, n_clusters=None):\n",
    "    \"\"\"Complete topic modeling pipeline with optional optimization\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{country} Parliament - {language}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    documents, embeddings, segment_ids = prepare_segments(df, segment_col, text_col, embedding_col)\n",
    "    print(f\"Processing {len(documents)} segments...\")\n",
    "    \n",
    "    # Optimize cluster size if not provided\n",
    "    if n_clusters is None:\n",
    "        n_clusters = optimize_cluster_size(embeddings)\n",
    "    else:\n",
    "        print(f\"Using fixed cluster size: {n_clusters}\")\n",
    "    \n",
    "    # Fit model\n",
    "    topic_model = create_topic_model(language, n_clusters)\n",
    "    topics, _ = topic_model.fit_transform(documents, embeddings.astype(np.float32))\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    \n",
    "    print(f\"Discovered {len(set(topics))} topics\")\n",
    "    \n",
    "    # Classify topics\n",
    "    print(\"Classifying with GPT-4...\")\n",
    "    topic_metadata = {}\n",
    "    for idx, row in tqdm(topic_info.iterrows(), total=len(topic_info)):\n",
    "        topic_id = row['Topic']\n",
    "        words = [w for w, _ in topic_model.get_topic(topic_id)]\n",
    "        topic_name, category = classify_with_gpt(words, country, language)\n",
    "        topic_metadata[topic_id] = {\n",
    "            'keywords': ', '.join(words[:15]),  # Top 15 n-grams (unigrams + bigrams)\n",
    "            'topic_name': topic_name,\n",
    "            'cap_category': category\n",
    "        }\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    # Create segment-to-metadata mapping\n",
    "    segment_topic_map = {}\n",
    "    for seg_id, topic_id in zip(segment_ids, topics):\n",
    "        meta = topic_metadata.get(topic_id, {'keywords': '', 'topic_name': 'Unknown', 'cap_category': 'Other'})\n",
    "        segment_topic_map[seg_id] = meta\n",
    "    \n",
    "    # Map metadata to each row in the dataframe via segment_col\n",
    "    df_result = df.copy()\n",
    "    df_result[f'Topic_Keywords_{country}_{language}'] = df_result[segment_col].map(\n",
    "        lambda x: segment_topic_map.get(x, {}).get('keywords', '')\n",
    "    )\n",
    "    df_result[f'Topic_Name_{country}_{language}'] = df_result[segment_col].map(\n",
    "        lambda x: segment_topic_map.get(x, {}).get('topic_name', 'Unknown')\n",
    "    )\n",
    "    df_result[f'CAP_Category_{country}_{language}'] = df_result[segment_col].map(\n",
    "        lambda x: segment_topic_map.get(x, {}).get('cap_category', 'Other')\n",
    "    )\n",
    "    \n",
    "    # Show distribution\n",
    "    cat_dist = df_result[f'CAP_Category_{country}_{language}'].value_counts()\n",
    "    print(f\"\\nTop CAP categories (by speech rows):\")\n",
    "    for cat, count in cat_dist.head(5).items():\n",
    "        print(f\"  {cat}: {count}\")\n",
    "    \n",
    "    return df_result, topic_metadata\n",
    "\n",
    "print(\"‚úÖ Functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7e894",
   "metadata": {},
   "source": [
    "## Apply Topic Modeling\n",
    "\n",
    "Run topic modeling with optimized cluster sizes for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8547c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 1: Great Britain (English)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "GB Parliament - english\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 37605 segments...\n",
      "\n",
      "Optimizing cluster size (testing: 21 values from 150 to 250)...\n",
      "  Testing n=150... silhouette=0.3275\n",
      "  Testing n=155... silhouette=0.3255\n",
      "  Testing n=160... silhouette=0.3310\n",
      "  Testing n=165... silhouette=0.3239\n",
      "  Testing n=170... silhouette=0.3255\n",
      "  Testing n=175... silhouette=0.3349\n",
      "  Testing n=180... silhouette=0.3328\n",
      "  Testing n=185... silhouette=0.3325\n",
      "  Testing n=190... silhouette=0.3298\n",
      "  Testing n=195... silhouette=0.3368\n",
      "  Testing n=200... silhouette=0.3350\n",
      "  Testing n=205... silhouette=0.3272\n",
      "  Testing n=210... silhouette=0.3251\n",
      "  Testing n=215... silhouette=0.3426\n",
      "  Testing n=220... silhouette=0.3280\n",
      "  Testing n=225... silhouette=0.3272\n",
      "  Testing n=230... silhouette=0.3299\n",
      "  Testing n=235... silhouette=0.3308\n",
      "  Testing n=240... silhouette=0.3278\n",
      "  Testing n=245... silhouette=0.3222\n",
      "  Testing n=250... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 18:57:44,101 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silhouette=0.3251\n",
      "\n",
      "‚úÖ Optimal clusters: 215 (score=0.3426)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 18:59:10,919 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2025-12-02 18:59:10,924 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-02 19:00:26,375 - BERTopic - Cluster - Completed ‚úì\n",
      "2025-12-02 19:00:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-02 19:07:42,624 - BERTopic - Representation - Completed ‚úì\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 215 topics\n",
      "Classifying with GPT-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 215/215 [06:56<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top CAP categories (by speech rows):\n",
      "  International Affairs: 98108\n",
      "  Health: 82113\n",
      "  Law and Crime: 51314\n",
      "  Other: 41959\n",
      "  Social Welfare: 38314\n",
      "\n",
      "============================================================\n",
      "STAGE 2: Austria (English + German)\n",
      "============================================================\n",
      "Step 1: Optimize cluster size on English embeddings...\n",
      "\n",
      "Optimizing cluster size (testing: 21 values from 150 to 250)...\n",
      "  Testing n=150... silhouette=0.4174\n",
      "  Testing n=155... silhouette=0.4288\n",
      "  Testing n=160... silhouette=0.4181\n",
      "  Testing n=165... silhouette=0.4360\n",
      "  Testing n=170... silhouette=0.4378\n",
      "  Testing n=175... silhouette=0.4260\n",
      "  Testing n=180... silhouette=0.4369\n",
      "  Testing n=185... silhouette=0.4234\n",
      "  Testing n=190... silhouette=0.4164\n",
      "  Testing n=195... silhouette=0.4194\n",
      "  Testing n=200... silhouette=0.4149\n",
      "  Testing n=205... silhouette=0.4227\n",
      "  Testing n=210... silhouette=0.4122\n",
      "  Testing n=215... silhouette=0.4057\n",
      "  Testing n=220... silhouette=0.4152\n",
      "  Testing n=225... silhouette=0.4074\n",
      "  Testing n=230... silhouette=0.3990\n",
      "  Testing n=235... silhouette=0.4128\n",
      "  Testing n=240... silhouette=0.4063\n",
      "  Testing n=245... silhouette=0.3990\n",
      "  Testing n=250... silhouette=0.3935\n",
      "\n",
      "‚úÖ Optimal clusters: 170 (score=0.4378)\n",
      "‚úÖ Will use 170 clusters for both English and German\n",
      "\n",
      "============================================================\n",
      "AT Parliament - english\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 19:22:20,501 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 25568 segments...\n",
      "Using fixed cluster size: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 19:23:24,424 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2025-12-02 19:23:24,428 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-02 19:23:53,828 - BERTopic - Cluster - Completed ‚úì\n",
      "2025-12-02 19:23:53,844 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-02 19:27:34,312 - BERTopic - Representation - Completed ‚úì\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 170 topics\n",
      "Classifying with GPT-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [05:24<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top CAP categories (by speech rows):\n",
      "  Macroeconomics: 34423\n",
      "  Law and Crime: 22789\n",
      "  Other: 21273\n",
      "  Education: 16702\n",
      "  Health: 14472\n",
      "\n",
      "============================================================\n",
      "AT Parliament - german\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 19:36:12,760 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 23752 segments...\n",
      "Using fixed cluster size: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 19:37:02,091 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2025-12-02 19:37:02,094 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-02 19:37:42,980 - BERTopic - Cluster - Completed ‚úì\n",
      "2025-12-02 19:37:42,986 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-02 19:42:24,579 - BERTopic - Representation - Completed ‚úì\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 170 topics\n",
      "Classifying with GPT-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [05:09<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top CAP categories (by speech rows):\n",
      "  Other: 44147\n",
      "  Macroeconomics: 27512\n",
      "  Civil Rights: 22285\n",
      "  Education: 15550\n",
      "  Health: 12843\n",
      "\n",
      "============================================================\n",
      "STAGE 3: Croatia (English + Croatian)\n",
      "============================================================\n",
      "Step 1: Optimize cluster size on English embeddings...\n",
      "\n",
      "Optimizing cluster size (testing: 21 values from 150 to 250)...\n",
      "  Testing n=150... silhouette=0.3132\n",
      "  Testing n=155... silhouette=0.3009\n",
      "  Testing n=160... silhouette=0.3055\n",
      "  Testing n=165... silhouette=0.3070\n",
      "  Testing n=170... silhouette=0.3073\n",
      "  Testing n=175... silhouette=0.3111\n",
      "  Testing n=180... silhouette=0.2983\n",
      "  Testing n=185... silhouette=0.3119\n",
      "  Testing n=190... silhouette=0.2927\n",
      "  Testing n=195... silhouette=0.3104\n",
      "  Testing n=200... silhouette=0.3057\n",
      "  Testing n=205... silhouette=0.3068\n",
      "  Testing n=210... silhouette=0.2951\n",
      "  Testing n=215... silhouette=0.3072\n",
      "  Testing n=220... silhouette=0.3039\n",
      "  Testing n=225... silhouette=0.2896\n",
      "  Testing n=230... silhouette=0.2814\n",
      "  Testing n=235... silhouette=0.2909\n",
      "  Testing n=240... silhouette=0.2845\n",
      "  Testing n=245... silhouette=0.2850\n",
      "  Testing n=250... silhouette=0.2846\n",
      "\n",
      "‚úÖ Optimal clusters: 150 (score=0.3132)\n",
      "‚úÖ Will use 150 clusters for both English and Croatian\n",
      "\n",
      "============================================================\n",
      "HR Parliament - english\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 19:52:19,261 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 32749 segments...\n",
      "Using fixed cluster size: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 19:53:30,712 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2025-12-02 19:53:30,716 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-02 19:54:35,795 - BERTopic - Cluster - Completed ‚úì\n",
      "2025-12-02 19:54:35,813 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-02 19:59:52,731 - BERTopic - Representation - Completed ‚úì\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 150 topics\n",
      "Classifying with GPT-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [04:32<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top CAP categories (by speech rows):\n",
      "  Government Operations: 79711\n",
      "  Macroeconomics: 77083\n",
      "  Health: 34483\n",
      "  Law and Crime: 32903\n",
      "  Other: 32836\n",
      "\n",
      "============================================================\n",
      "HR Parliament - croatian\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 20:08:48,093 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 39889 segments...\n",
      "Using fixed cluster size: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 20:10:12,778 - BERTopic - Dimensionality - Completed ‚úì\n",
      "2025-12-02 20:10:12,782 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-02 20:11:43,265 - BERTopic - Cluster - Completed ‚úì\n",
      "2025-12-02 20:11:43,278 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-02 20:18:23,494 - BERTopic - Representation - Completed ‚úì\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 150 topics\n",
      "Classifying with GPT-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [04:30<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top CAP categories (by speech rows):\n",
      "  Macroeconomics: 88863\n",
      "  Government Operations: 80921\n",
      "  Other: 52987\n",
      "  Social Welfare: 32571\n",
      "  Domestic Commerce: 31502\n",
      "\n",
      "============================================================\n",
      "‚úÖ Topic modeling complete for all datasets\n",
      "============================================================\n",
      "   GB (English): Auto-optimized clusters\n",
      "   AT (English + German): 170 clusters (optimized on English)\n",
      "   HR (English + Croatian): 150 clusters (optimized on English)\n"
     ]
    }
   ],
   "source": [
    "# GB (English only) - with optimization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 1: Great Britain (English)\")\n",
    "print(\"=\"*60)\n",
    "GB_final, gb_cats = run_topic_pipeline(\n",
    "    GB, 'GB', 'english', 'Text_English', 'Segment_ID_English',\n",
    "    'Segment_Embeddings_English', n_clusters=None  # Auto-optimize\n",
    ")\n",
    "\n",
    "# AT (English + German) - optimize on English, reuse for German\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2: Austria (English + German)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, optimize on English\n",
    "print(\"Step 1: Optimize cluster size on English embeddings...\")\n",
    "AT_documents_en, AT_embeddings_en, AT_segment_ids_en = prepare_segments(\n",
    "    AT, 'Segment_ID_English', 'Text_English', 'Segment_Embeddings_English'\n",
    ")\n",
    "optimal_clusters_AT = optimize_cluster_size(AT_embeddings_en)\n",
    "print(f\"‚úÖ Will use {optimal_clusters_AT} clusters for both English and German\")\n",
    "\n",
    "# Run topic modeling with fixed cluster size\n",
    "AT_temp, at_en_cats = run_topic_pipeline(\n",
    "    AT, 'AT', 'english', 'Text_English', 'Segment_ID_English',\n",
    "    'Segment_Embeddings_English', n_clusters=optimal_clusters_AT  # Use optimized value\n",
    ")\n",
    "\n",
    "AT_final, at_de_cats = run_topic_pipeline(\n",
    "    AT_temp, 'AT', 'german', 'Text_Native', 'Segment_ID_Native',\n",
    "    'Segment_Embeddings_Native', n_clusters=optimal_clusters_AT  # Reuse same value\n",
    ")\n",
    "\n",
    "# HR (English + Croatian) - optimize on English, reuse for Croatian\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 3: Croatia (English + Croatian)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, optimize on English\n",
    "print(\"Step 1: Optimize cluster size on English embeddings...\")\n",
    "HR_documents_en, HR_embeddings_en, HR_segment_ids_en = prepare_segments(\n",
    "    HR, 'Segment_ID_English', 'Text_English', 'Segment_Embeddings_English'\n",
    ")\n",
    "optimal_clusters_HR = optimize_cluster_size(HR_embeddings_en)\n",
    "print(f\"‚úÖ Will use {optimal_clusters_HR} clusters for both English and Croatian\")\n",
    "\n",
    "# Run topic modeling with fixed cluster size\n",
    "HR_temp, hr_en_cats = run_topic_pipeline(\n",
    "    HR, 'HR', 'english', 'Text_English', 'Segment_ID_English',\n",
    "    'Segment_Embeddings_English', n_clusters=optimal_clusters_HR  # Use optimized value\n",
    ")\n",
    "\n",
    "HR_final, hr_hr_cats = run_topic_pipeline(\n",
    "    HR_temp, 'HR', 'croatian', 'Text_Native', 'Segment_ID_Native',\n",
    "    'Segment_Embeddings_Native', n_clusters=optimal_clusters_HR  # Reuse same value\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Topic modeling complete for all datasets\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   GB (English): Auto-optimized clusters\")\n",
    "print(f\"   AT (English + German): {optimal_clusters_AT} clusters (optimized on English)\")\n",
    "print(f\"   HR (English + Croatian): {optimal_clusters_HR} clusters (optimized on English)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17321229",
   "metadata": {},
   "source": [
    "## Save Topic Metadata\n",
    "\n",
    "Save topic metadata as CSV files for easy inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ae94a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Topic metadata CSV files saved to: data folder\n",
      "\n",
      "üìÑ Files created:\n",
      "  GB: GB_topic_metadata.csv\n",
      "  AT: AT_english_topic_metadata.csv, AT_german_topic_metadata.csv\n",
      "  HR: HR_english_topic_metadata.csv, HR_croatian_topic_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Save topic metadata as CSV for easy inspection\n",
    "def save_topic_metadata(topic_dict, filename):\n",
    "    \"\"\"Save topic metadata to CSV\"\"\"\n",
    "    rows = []\n",
    "    for topic_id, meta in topic_dict.items():\n",
    "        rows.append({\n",
    "            'Topic_ID': topic_id,\n",
    "            'Keywords': meta['keywords'],\n",
    "            'Topic_Name': meta['topic_name'],\n",
    "            'CAP_Category': meta['cap_category']\n",
    "        })\n",
    "    pd.DataFrame(rows).to_csv(filename, index=False)\n",
    "\n",
    "save_topic_metadata(gb_cats, os.path.join(BASE_DATA_DIR, \"GB/GB_topic_metadata.csv\"))\n",
    "save_topic_metadata(at_en_cats, os.path.join(BASE_DATA_DIR, \"AT/AT_english_topic_metadata.csv\"))\n",
    "save_topic_metadata(at_de_cats, os.path.join(BASE_DATA_DIR, \"AT/AT_german_topic_metadata.csv\"))\n",
    "save_topic_metadata(hr_en_cats, os.path.join(BASE_DATA_DIR, \"HR/HR_english_topic_metadata.csv\"))\n",
    "save_topic_metadata(hr_hr_cats, os.path.join(BASE_DATA_DIR, \"HR/HR_croatian_topic_metadata.csv\"))\n",
    "\n",
    "print(\"‚úÖ Topic metadata CSV files saved to:\", BASE_DATA_DIR)\n",
    "print(f\"\\nüìÑ Files created:\")\n",
    "print(f\"  GB: GB_topic_metadata.csv\")\n",
    "print(f\"  AT: AT_english_topic_metadata.csv, AT_german_topic_metadata.csv\")\n",
    "print(f\"  HR: HR_english_topic_metadata.csv, HR_croatian_topic_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fc3cd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "View topic distribution across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a841ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Statistics After Topic Modeling\n",
      "============================================================\n",
      "\n",
      "üá¨üáß Great Britain (GB):\n",
      "   Total speeches: 670,912\n",
      "   Unique topics: 207\n",
      "   CAP categories: 21\n",
      "\n",
      "üá¶üáπ Austria (AT):\n",
      "   Total speeches: 231,759\n",
      "   English - Unique topics: 154\n",
      "   German - Unique topics: 142\n",
      "\n",
      "üá≠üá∑ Croatia (HR):\n",
      "   Total speeches: 504,338\n",
      "   English - Unique topics: 144\n",
      "   Croatian - Unique topics: 138\n",
      "\n",
      "üìä Overall CAP Category Distribution (All Speeches)\n",
      "============================================================\n",
      " 1. Macroeconomics: 261,850 (12.2%)\n",
      " 2. Government Operations: 211,776 (9.9%)\n",
      " 3. Other: 193,202 (9.0%)\n",
      " 4. Health: 164,049 (7.7%)\n",
      " 5. International Affairs: 147,677 (6.9%)\n",
      " 6. Law and Crime: 138,300 (6.5%)\n",
      " 7. Social Welfare: 112,648 (5.3%)\n",
      " 8. Civil Rights: 105,654 (4.9%)\n",
      " 9. Labor: 99,555 (4.6%)\n",
      "10. Education: 98,583 (4.6%)\n",
      "\n",
      "Total classified speeches: 2,143,106\n",
      "\n",
      "‚úÖ Topic modeling pipeline complete!\n",
      "‚úÖ All dataframes now contain topic keywords, names, and CAP categories for each speech\n"
     ]
    }
   ],
   "source": [
    "# Combine all topic columns for overview\n",
    "print(\"üìä Dataset Statistics After Topic Modeling\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüá¨üáß Great Britain (GB):\")\n",
    "print(f\"   Total speeches: {len(GB_final):,}\")\n",
    "print(f\"   Unique topics: {GB_final['Topic_Name_GB_english'].nunique()}\")\n",
    "print(f\"   CAP categories: {GB_final['CAP_Category_GB_english'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüá¶üáπ Austria (AT):\")\n",
    "print(f\"   Total speeches: {len(AT_final):,}\")\n",
    "print(f\"   English - Unique topics: {AT_final['Topic_Name_AT_english'].nunique()}\")\n",
    "print(f\"   German - Unique topics: {AT_final['Topic_Name_AT_german'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüá≠üá∑ Croatia (HR):\")\n",
    "print(f\"   Total speeches: {len(HR_final):,}\")\n",
    "print(f\"   English - Unique topics: {HR_final['Topic_Name_HR_english'].nunique()}\")\n",
    "print(f\"   Croatian - Unique topics: {HR_final['Topic_Name_HR_croatian'].nunique()}\")\n",
    "\n",
    "# Combine all CAP categories for overall distribution\n",
    "all_categories = []\n",
    "all_categories.extend(GB_final['CAP_Category_GB_english'].dropna().tolist())\n",
    "all_categories.extend(AT_final['CAP_Category_AT_english'].dropna().tolist())\n",
    "all_categories.extend(AT_final['CAP_Category_AT_german'].dropna().tolist())\n",
    "all_categories.extend(HR_final['CAP_Category_HR_english'].dropna().tolist())\n",
    "all_categories.extend(HR_final['CAP_Category_HR_croatian'].dropna().tolist())\n",
    "\n",
    "cat_dist = pd.Series(all_categories).value_counts()\n",
    "\n",
    "print(\"\\nüìä Overall CAP Category Distribution (All Speeches)\")\n",
    "print(\"=\"*60)\n",
    "for i, (cat, count) in enumerate(cat_dist.head(10).items(), 1):\n",
    "    pct = count / len(all_categories) * 100\n",
    "    print(f\"{i:2d}. {cat}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal classified speeches: {len(all_categories):,}\")\n",
    "\n",
    "print(\"\\n‚úÖ Topic modeling pipeline complete!\")\n",
    "print(f\"‚úÖ All dataframes now contain topic keywords, names, and CAP categories for each speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c471b6d",
   "metadata": {},
   "source": [
    "## Consensus Topic Determination\n",
    "\n",
    "Determine consensus topics from multiple language predictions and merge with human labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eddf421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Consensus topics determined\n",
      "   GB unique consensus topics: 21\n",
      "   AT unique consensus topics: 22\n",
      "   HR unique consensus topics: 21\n"
     ]
    }
   ],
   "source": [
    "def determine_consensus(row, topic_cols, is_chairperson_col='Speaker_role'):\n",
    "    \"\"\"Determine consensus topic from multiple predictions\"\"\"\n",
    "    topics = [row[col] for col in topic_cols if col in row.index]\n",
    "    topics = [t for t in topics if t != 'Other']\n",
    "    \n",
    "    # Single topic column - no consensus needed\n",
    "    if len(topic_cols) == 1:\n",
    "        return row[topic_cols[0]] if topic_cols[0] in row.index else 'Other'\n",
    "    \n",
    "    # Chairperson requires unanimous agreement\n",
    "    if row.get(is_chairperson_col) == 'Chairperson':\n",
    "        return topics[0] if len(set(topics)) == len(topics) == len(topic_cols) else 'Other'\n",
    "    \n",
    "    # Regular speakers\n",
    "    if not topics:\n",
    "        return 'Other'\n",
    "    \n",
    "    topic_counts = pd.Series(topics).value_counts()\n",
    "    \n",
    "    # For 2 columns: both agree or mark as Mix\n",
    "    if len(topic_cols) == 2:\n",
    "        return topic_counts.idxmax() if topic_counts.iloc[0] == 2 else 'Mix'\n",
    "    \n",
    "    # Fallback for other cases\n",
    "    return topic_counts.idxmax() if topic_counts.iloc[0] > 1 else 'Mix'\n",
    "\n",
    "# Apply consensus for each dataset\n",
    "HR_final['topic_consensus'] = HR_final.apply(lambda r: determine_consensus(\n",
    "    r, ['CAP_Category_HR_english', 'CAP_Category_HR_croatian']), axis=1)\n",
    "\n",
    "AT_final['topic_consensus'] = AT_final.apply(lambda r: determine_consensus(\n",
    "    r, ['CAP_Category_AT_english', 'CAP_Category_AT_german']), axis=1)\n",
    "\n",
    "GB_final['topic_consensus'] = GB_final.apply(lambda r: determine_consensus(\n",
    "    r, ['CAP_Category_GB_english']), axis=1)\n",
    "\n",
    "print(\"‚úÖ Consensus topics determined\")\n",
    "print(f\"   GB unique consensus topics: {GB_final['topic_consensus'].nunique()}\")\n",
    "print(f\"   AT unique consensus topics: {AT_final['topic_consensus'].nunique()}\")\n",
    "print(f\"   HR unique consensus topics: {HR_final['topic_consensus'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6b634",
   "metadata": {},
   "source": [
    "## Merge with Human Labels\n",
    "\n",
    "Load and merge human-labeled test sets for GB and HR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2716a145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Human labels merged\n",
      "   GB speeches with labels: 876\n",
      "   HR speeches with labels: 869\n"
     ]
    }
   ],
   "source": [
    "# Load human labels\n",
    "hr_labels = pd.read_json(os.path.join(BASE_DATA_DIR, \"HR/ParlaCAP-test-hr.jsonl\"), lines=True)\n",
    "gb_labels = pd.read_json(os.path.join(BASE_DATA_DIR, \"GB/ParlaCAP-test-en.jsonl\"), lines=True)\n",
    "\n",
    "# Merge with HR\n",
    "HR_final = HR_final.merge(hr_labels[['id', 'labels']], left_on='ID', right_on='id', how='left')\n",
    "HR_final.rename(columns={'labels': 'True_label'}, inplace=True)\n",
    "HR_final.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Merge with GB\n",
    "GB_final = GB_final.merge(gb_labels[['id', 'labels']], left_on='ID', right_on='id', how='left')\n",
    "GB_final.rename(columns={'labels': 'True_label'}, inplace=True)\n",
    "GB_final.drop(columns=['id'], inplace=True)\n",
    "\n",
    "print(\"‚úÖ Human labels merged\")\n",
    "print(f\"   GB speeches with labels: {GB_final['True_label'].notna().sum():,}\")\n",
    "print(f\"   HR speeches with labels: {HR_final['True_label'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f2aed",
   "metadata": {},
   "source": [
    "## Load and Merge LIWC Data\n",
    "\n",
    "Add LIWC-22 linguistic features to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8edffcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LIWC data merged\n",
      "   AT: 231,759 speeches with LIWC features\n",
      "   HR: 504,338 speeches with LIWC features\n",
      "   GB: 670,912 speeches with LIWC features\n"
     ]
    }
   ],
   "source": [
    "# Load LIWC results\n",
    "AT_LIWC = pd.read_csv(os.path.join(BASE_DATA_DIR, \"AT/AT_LIWC_results.csv\"))\n",
    "HR_LIWC = pd.read_csv(os.path.join(BASE_DATA_DIR, \"HR/HR_LIWC_results.csv\"))\n",
    "GB_LIWC = pd.read_csv(os.path.join(BASE_DATA_DIR, \"GB/GB_LIWC_results.csv\"))\n",
    "\n",
    "# Merge with existing data\n",
    "AT_final = AT_final.merge(AT_LIWC, on='ID', how='inner')\n",
    "HR_final = HR_final.merge(HR_LIWC, on='ID', how='inner')\n",
    "GB_final = GB_final.merge(GB_LIWC, on='ID', how='inner')\n",
    "\n",
    "# Add country identifier\n",
    "AT_final['Country'] = 'Austria'\n",
    "HR_final['Country'] = 'Croatia'\n",
    "GB_final['Country'] = 'Great Britain'\n",
    "\n",
    "# Process dates\n",
    "for df in [AT_final, HR_final, GB_final]:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Calculate speaker age\n",
    "for df in [AT_final, HR_final, GB_final]:\n",
    "    df['Speaker_birth'] = pd.to_numeric(df['Speaker_birth'], errors='coerce')\n",
    "    df['Speaker_age'] = df['Year'] - df['Speaker_birth']\n",
    "\n",
    "print(\"‚úÖ LIWC data merged\")\n",
    "print(f\"   AT: {len(AT_final):,} speeches with LIWC features\")\n",
    "print(f\"   HR: {len(HR_final):,} speeches with LIWC features\")\n",
    "print(f\"   GB: {len(GB_final):,} speeches with LIWC features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d843a",
   "metadata": {},
   "source": [
    "## Save Final Datasets\n",
    "\n",
    "Save complete datasets with all features: topics, consensus, labels, and LIWC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ea414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final datasets saved!\n",
      "\n",
      " Final Dataset Summary:\n",
      "   GB: 670,912 speeches √ó 155 columns\n",
      "   AT: 231,759 speeches √ó 160 columns\n",
      "   HR: 504,338 speeches √ó 161 columns\n"
     ]
    }
   ],
   "source": [
    "# Cleanup: Remove unnecessary columns before saving\n",
    "columns_to_remove = ['Segment']  \n",
    "\n",
    "for df in [GB_final, AT_final, HR_final]:\n",
    "    for col in columns_to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Save final datasets ready for visualization\n",
    "GB_final.to_pickle(os.path.join(BASE_DATA_DIR, \"GB/GB_final.pkl\"))\n",
    "AT_final.to_pickle(os.path.join(BASE_DATA_DIR, \"AT/AT_final.pkl\"))\n",
    "HR_final.to_pickle(os.path.join(BASE_DATA_DIR, \"HR/HR_final.pkl\"))\n",
    "\n",
    "print(\"\\n Final datasets saved!\")\n",
    "print(\"\\n Final Dataset Summary:\")\n",
    "print(f\"   GB: {GB_final.shape[0]:,} speeches √ó {GB_final.shape[1]} columns\")\n",
    "print(f\"   AT: {AT_final.shape[0]:,} speeches √ó {AT_final.shape[1]} columns\")\n",
    "print(f\"   HR: {HR_final.shape[0]:,} speeches √ó {HR_final.shape[1]} columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
