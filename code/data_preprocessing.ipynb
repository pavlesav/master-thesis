{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdce8ef6",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing for Austrian Parliament Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ab631",
   "metadata": {},
   "source": [
    "Saving to csv is not optimal, there exists cell truncation in excel and some text columns are too big to be represented in excel causing spillover to next columns and all sorts of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning Dropped row: Row count mismatch in ParlaMint-AT-en_2001-07-06-021-XXI-NRSITZ-00076.txt (323) and ParlaMint-AT-en_2001-07-06-021-XXI-NRSITZ-00076-meta.tsv (325) in 2001.\n",
      "Warning Dropped row: Row count mismatch in ParlaMint-AT-en_2008-09-24-023-XXIII-NRSITZ-00072.txt (486) and ParlaMint-AT-en_2008-09-24-023-XXIII-NRSITZ-00072-meta.tsv (491) in 2008.\n",
      "\n",
      "Processed and merged data\n",
      "Shape of the created df: (231752, 24)\n",
      "Applying text preprocessing...\n",
      "Finished creating 'Processed_Text' column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Processed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please take a seat. – I also ask the photograp...</td>\n",
       "      <td>seat photographer stop activity attend first c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The President of the Federal Republic of Germa...</td>\n",
       "      <td>republic germany resolution january provided a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We go into the agenda and arrive at the 1st. P...</td>\n",
       "      <td>agenda arrive fishing ludmilla perfuss read fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They will be vowed unbreakable fidelity of the...</td>\n",
       "      <td>vowed unbreakable fidelity republic constant f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you. The Angelobung's done with it. Ladi...</td>\n",
       "      <td>angelobung done committed conscientious exerci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Please take a seat. – I also ask the photograp...   \n",
       "1  The President of the Federal Republic of Germa...   \n",
       "2  We go into the agenda and arrive at the 1st. P...   \n",
       "3  They will be vowed unbreakable fidelity of the...   \n",
       "4  Thank you. The Angelobung's done with it. Ladi...   \n",
       "\n",
       "                                      Processed_Text  \n",
       "0  seat photographer stop activity attend first c...  \n",
       "1  republic germany resolution january provided a...  \n",
       "2  agenda arrive fishing ludmilla perfuss read fi...  \n",
       "3  vowed unbreakable fidelity republic constant f...  \n",
       "4  angelobung done committed conscientious exerci...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Define standard and custom stopwords\n",
    "standard_stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = [\n",
    "    'mr', 'mrs', 'ms', 'madam', 'honourable', 'member', 'members', 'vp', 'sp', 'today', \n",
    "    'speaker', 'deputy', 'president', 'chairman', 'chair',\n",
    "    'secretary', 'lord', 'lady', 'question', 'order', 'point', 'debate',\n",
    "    'motion', 'amendment', 'hear', 'minutes', 'speaking', 'close', 'abg',\n",
    "    'congratulations', 'congratulate', 'thanks', 'thank', 'say', 'one', 'want',\n",
    "    'know', 'think', 'believe', 'see', 'go', 'come', 'give', 'take',\n",
    "    'people', 'federal', 'government', 'austria', 'austrian', 'committee', 'call', 'said',\n",
    "    'already', 'committee', 'para', 'please', 'request', 'proceed', 'reading', 'people',\n",
    "    'course', 'mag', 'welcome', 'council', 'open', 'written', 'contain', 'items', 'item',\n",
    "    'peter', 'jonah', 'auer', 'vow', 'yes', 'no', 'following', 'orf', 'wait', 'ing', \n",
    "    'next', 'speech', 'year', 'years', 'state', \n",
    "    'also', 'would', 'like', 'may', 'must', 'upon', 'indeed', 'item',\n",
    "    'session', 'meeting', 'report', 'commission', 'behalf', 'gentleman', 'gentlemen', 'ladies', 'thank',\n",
    "    'applause', 'group', 'colleague', 'colleagues', 'issue', 'issues',\n",
    "    'chancellor', 'court', 'ask', 'answer', 'reply', 'regard', 'regarding',\n",
    "    'regards', 'respect', 'respectfully', 'sign', 'shall', 'procedure', 'declare'\n",
    "]\n",
    "stop_words = standard_stop_words.union(set(custom_stopwords))\n",
    "punctuation = set(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenize, lowercase, remove stop words, punctuation, and lemmatize.\"\"\"\n",
    "    text = str(text) # Ensure text is string\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Keep only alphabetic tokens, remove combined stopwords\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "parent_folder = r\"data folder\\ParlaMint-AT-en.ana\\ParlaMint-AT-en.txt\"\n",
    "\n",
    "year_folders = sorted([f for f in os.listdir(parent_folder) if f.isdigit()])\n",
    "\n",
    "df_list = []\n",
    "\n",
    "def process_year_folder(year_folder):\n",
    "    folder_path = os.path.join(parent_folder, year_folder)\n",
    "    txt_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".txt\")])\n",
    "    tsv_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\"-meta.tsv\")])\n",
    "    \n",
    "    assert len(txt_files) == len(tsv_files), f\"Mismatch in TXT and TSV file counts for {year_folder}!\"\n",
    "    \n",
    "    for txt_file, tsv_file in zip(txt_files, tsv_files):\n",
    "        txt_path = os.path.join(folder_path, txt_file)\n",
    "        tsv_path = os.path.join(folder_path, tsv_file)\n",
    "\n",
    "        try:\n",
    "            df_txt = pd.read_csv(txt_path, sep=\"\\t\", header=None, names=[\"ID\", \"Text\"], encoding=\"utf-8\")\n",
    "            df_meta = pd.read_csv(tsv_path, sep=\"\\t\", encoding=\"utf-8\", index_col=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading files in {year_folder}: {txt_file}, {tsv_file}. Error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if len(df_txt) != len(df_meta):\n",
    "            print(f\"Warning Dropped row: Row count mismatch in {txt_file} ({len(df_txt)}) and {tsv_file} ({len(df_meta)}) in {year_folder}.\")\n",
    "\n",
    "        merged_df = pd.merge(df_meta, df_txt, on=\"ID\", how=\"inner\")\n",
    "        df_list.append(merged_df)\n",
    "\n",
    "# Process all year folders\n",
    "for year in year_folders:\n",
    "    process_year_folder(year)\n",
    "\n",
    "# Combine all data into one big dataframe\n",
    "AT = pd.concat(df_list, ignore_index=True)\n",
    "print(\"\\nProcessed and merged data\")\n",
    "print(f\"Shape of the created df: {AT.shape}\")\n",
    "\n",
    "# Apply preprocessing to create the 'Processed_Text' column\n",
    "print(\"Applying text preprocessing...\")\n",
    "AT['Processed_Text'] = AT['Text'].apply(preprocess_text)\n",
    "print(\"Finished creating 'Processed_Text' column.\")\n",
    "AT[['Text', 'Processed_Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3adb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AT.rename(columns={'Text_ID': 'Sitting_ID', 'ID': 'Speech_ID'}, inplace=True)\n",
    "\n",
    "# drop unnecessary columns:  Body, Term, Session, Meeting, Sitting, Agenda, Subcorpus, Lang\n",
    "AT.drop(columns=['Body', 'Term', 'Session', 'Meeting', 'Sitting', 'Agenda', 'Subcorpus', 'Lang'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803e9e5",
   "metadata": {},
   "source": [
    "## Austria specific Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4d787",
   "metadata": {},
   "source": [
    "### Fix Party Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83459aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Party Abbreviation Counts:\n",
      "Speaker_party\n",
      "SPÖ         78087\n",
      "ÖVP         72466\n",
      "FPÖ         48411\n",
      "GRÜNE       16726\n",
      "BZÖ          4493\n",
      "-            3855\n",
      "NEOS         3565\n",
      "LIF          1919\n",
      "STRONACH     1612\n",
      "JETZT         618\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fix case inconsistencies in Speaker_party\n",
    "AT['Speaker_party'] = AT['Speaker_party'].replace({'Grüne': 'GRÜNE'})\n",
    "\n",
    "party_mapping = {\n",
    "        \"SPÖ\": \"Parliamentary group of the Social Democratic Party of Austria\",\n",
    "        \"ÖVP\": \"Parliamentary group of the Austrian People's Party\",\n",
    "        \"FPÖ\": \"Parliamentary group of the Austrian Freedom Party\",\n",
    "        \"GRÜNE\": \"Parliamentary group of The Greens - The Green Alternative\",\n",
    "        \"BZÖ\": \"Austrian People's Party\",\n",
    "        \"NEOS\": \"Parliamentary group of NEOS\",\n",
    "        \"LIF\": \"Parliamentary group of Liberal Forum\",\n",
    "        \"STRONACH\": \"Parliamentary group Team Stronach\",\n",
    "        \"JETZT\": \"Parliamentary group JETZT - Pilz List\",\n",
    "        \"-\": \"-\" # Keep partyless designation\n",
    "    }\n",
    "AT['Speaker_party_name'] = AT['Speaker_party'].map(party_mapping)\n",
    "\n",
    "print('\\nOriginal Party Abbreviation Counts:')\n",
    "print(AT['Speaker_party'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f522fa",
   "metadata": {},
   "source": [
    "### Fix Speaker Names (Due to Name Changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba218a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous Speaker IDs before cleaning: 13\n",
      "Ambiguous Speaker IDs after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# Check initial state: Count unique Speaker_name values per Speaker_ID\n",
    "id_name_counts_before = AT.groupby('Speaker_ID')['Speaker_name'].nunique()\n",
    "ambiguous_ids_before = id_name_counts_before[id_name_counts_before > 1]\n",
    "print(f\"Ambiguous Speaker IDs before cleaning: {len(ambiguous_ids_before)}\")\n",
    "\n",
    "name_mapping = {\n",
    "        'Glawischnig-Piesczek, Eva': 'Glawischnig-Piesczek, Eva',\n",
    "        'Glawischnig, Eva': 'Glawischnig-Piesczek, Eva',\n",
    "        'Belakowitsch-Jenewein, Dagmar': 'Belakowitsch-Jenewein, Dagmar',\n",
    "        'Belakowitsch, Dagmar': 'Belakowitsch-Jenewein, Dagmar',\n",
    "        'Steibl, Ridi': 'Steibl, Ridi Maria',\n",
    "        'Steibl, Ridi Maria': 'Steibl, Ridi Maria',\n",
    "        'Bernhard, Michael': 'Bernhard-Pock, Michael',\n",
    "        'Pock, Michael': 'Bernhard-Pock, Michael',\n",
    "        'Fuhrmann, Silvia': 'Fuhrmann-Grünberger, Silvia',\n",
    "        'Grünberger, Silvia': 'Fuhrmann-Grünberger, Silvia',\n",
    "        'Holzinger-Vogtenhuber, Daniela': 'Holzinger-Vogtenhuber, Daniela',\n",
    "        'Holzinger, Daniela': 'Holzinger-Vogtenhuber, Daniela',\n",
    "        'Binder-Maier, Gabriele': 'Binder-Maier, Gabriele',\n",
    "        'Binder, Gabriele': 'Binder-Maier, Gabriele',\n",
    "        'Gartelgruber, Carmen': 'Gartelgruber-Schimanek, Carmen',\n",
    "        'Schimanek, Carmen': 'Gartelgruber-Schimanek, Carmen',\n",
    "        'Aumayr, Anna Elisabeth': 'Aumayr-Achatz, Anna Elisabeth',\n",
    "        'Achatz, Anna Elisabeth': 'Aumayr-Achatz, Anna Elisabeth',\n",
    "        'Vorderwinkler, Petra': 'Vorderwinkler-Tanzler, Petra',\n",
    "        'Tanzler, Petra': 'Vorderwinkler-Tanzler, Petra',\n",
    "        'Gastinger, Karin': 'Gastinger-Miklautsch, Karin',\n",
    "        'Miklautsch, Karin': 'Gastinger-Miklautsch, Karin',\n",
    "        'Rinner, Sylvia': 'Rinner-Prettenthaler, Sylvia',\n",
    "        'Prettenthaler, Sylvia': 'Rinner-Prettenthaler, Sylvia',\n",
    "        'Steßl-Mühlbacher, Sonja': 'Steßl-Mühlbacher, Sonja',\n",
    "        'Steßl, Sonja': 'Steßl-Mühlbacher, Sonja'\n",
    "    }\n",
    "\n",
    "AT['Speaker_name'] = AT['Speaker_name'].replace(name_mapping)    \n",
    "\n",
    "id_name_counts_after = AT.groupby('Speaker_ID')['Speaker_name'].nunique()\n",
    "ambiguous_ids_after = id_name_counts_after[id_name_counts_after > 1]\n",
    "print(f\"Ambiguous Speaker IDs after cleaning: {len(ambiguous_ids_after)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac677e",
   "metadata": {},
   "source": [
    "Check for cases where the same name might correspond to different IDs (less common)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d825235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Names associated with multiple Speaker_IDs: 1\n",
      "Speaker_name\n",
      "Rauch, Johannes    2\n",
      "Name: Speaker_ID, dtype: int64\n",
      "\n",
      "Investigating 'Rauch, Johannes':\n",
      "Speaker_ID\n",
      "PAD_21029    59\n",
      "PAD_83152    45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group by Speaker_name and count unique Speaker_IDs\n",
    "name_id_counts = AT.groupby('Speaker_name')['Speaker_ID'].nunique()\n",
    "\n",
    "# Filter to find names associated with multiple Speaker_IDs\n",
    "ambiguous_speakers = name_id_counts[name_id_counts > 1]\n",
    "\n",
    "print(f\"\\nNames associated with multiple Speaker_IDs: {len(ambiguous_speakers)}\")\n",
    "if len(ambiguous_speakers) > 0:\n",
    "    print(ambiguous_speakers)\n",
    "    ambiguous_name = ambiguous_speakers.index[0]\n",
    "    print(f\"\\nInvestigating '{ambiguous_name}':\")\n",
    "    print(AT[AT['Speaker_name'] == ambiguous_name]['Speaker_ID'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ba7a9",
   "metadata": {},
   "source": [
    "## Filter by Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c0b05",
   "metadata": {},
   "source": [
    "### Short speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d1a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape before marking short speeches: (231752, 18)\n",
      "Number of short speeches marked: 41119\n",
      "count    231752.000000\n",
      "mean        293.371669\n",
      "std         461.934589\n",
      "min           1.000000\n",
      "25%          12.000000\n",
      "50%          54.000000\n",
      "75%         466.000000\n",
      "max       35419.000000\n",
      "Name: Word_Count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate word count for the processed text\n",
    "AT['Word_Count'] = AT['Text'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "print(f\"\\nShape before marking short speeches: {AT.shape}\")\n",
    "min_word_count = 10\n",
    "\n",
    "# Create a new boolean column 'Is_Short_Speech'\n",
    "AT['Is_Short_Speech'] = AT['Word_Count'] < min_word_count\n",
    "\n",
    "print(f\"Number of short speeches marked: {AT['Is_Short_Speech'].sum()}\")\n",
    "\n",
    "print(AT['Word_Count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774d757",
   "metadata": {},
   "source": [
    "### Very long speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "851b6507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "99.9th percentile for Word_Count: 3545.00 words.\n",
      "Original DataFrame shape: (231752, 19)\n",
      "Filtered DataFrame shape: (231521, 19)\n",
      "Number of rows removed: 231\n",
      "\n",
      "Filtered Utterance Word Count Statistics (top 0.1% removed):\n",
      "count    231521.000000\n",
      "mean        288.715672\n",
      "std         430.061195\n",
      "min           1.000000\n",
      "25%          12.000000\n",
      "50%          54.000000\n",
      "75%         464.000000\n",
      "max        3545.000000\n",
      "Name: Word_Count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 99.9th percentile for Word_Count\n",
    "percentile_99_9 = AT['Word_Count'].quantile(0.999)\n",
    "print(f\"\\n99.9th percentile for Word_Count: {percentile_99_9:.2f} words.\")\n",
    "\n",
    "# Store the original DataFrame shape\n",
    "original_shape = AT.shape\n",
    "print(f\"Original DataFrame shape: {original_shape}\")\n",
    "\n",
    "# Filter out the top 0.1% longest speeches\n",
    "AT = AT[AT['Word_Count'] <= percentile_99_9].copy()\n",
    "\n",
    "# Display the shape of the new DataFrame\n",
    "filtered_shape = AT.shape\n",
    "print(f\"Filtered DataFrame shape: {filtered_shape}\")\n",
    "print(f\"Number of rows removed: {original_shape[0] - filtered_shape[0]}\")\n",
    "\n",
    "# Display basic statistics for utterance word counts on the filtered DataFrame\n",
    "print(\"\\nFiltered Utterance Word Count Statistics (top 0.1% removed):\")\n",
    "print(AT['Word_Count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38959176",
   "metadata": {},
   "source": [
    "The code in the cell above **does filter** the `AT` DataFrame. \n",
    "Specifically, it removes the **longest speeches**, i.e., those in the top 0.1% by word count. \n",
    "The line `AT = AT[AT['Word_Count'] <= percentile_99_9].copy()` modifies the DataFrame `AT` by keeping only the speeches whose word count is less than or equal to the 99.9th percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44beb0ed",
   "metadata": {},
   "source": [
    "### Merge Consecutive Speeches by Same Speaker in Same Sitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455f50b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source AT DataFrame shape:(231521, 19)\n",
      "Number of short speeches in AT: 41119\n",
      "Number of non-short speeches in AT: 190402\n",
      "Shape of AT_preprocessed (short speeches filtered, consecutive speeches merged): (170096, 19)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSource AT DataFrame shape:{AT.shape}\")\n",
    "print(f\"Number of short speeches in AT: {AT['Is_Short_Speech'].sum()}\")\n",
    "print(f\"Number of non-short speeches in AT: {len(AT) - AT['Is_Short_Speech'].sum()}\")\n",
    "\n",
    "# Create AT_non_short by filtering out short speeches from a copy of AT\n",
    "AT_non_short = AT[AT['Is_Short_Speech'] == False].copy()\n",
    "\n",
    "# Condition for merging: same speaker AND same sitting (Text_ID)\n",
    "is_prev_same_speaker_and_sitting = (\n",
    "    (AT_non_short['Speaker_ID'] == AT_non_short['Speaker_ID'].shift(1)) &\n",
    "    (AT_non_short['Sitting_ID'] == AT_non_short['Sitting_ID'].shift(1))\n",
    ")\n",
    "speech_block_id = (~is_prev_same_speaker_and_sitting).cumsum()\n",
    "\n",
    "# Assign the block ID temporarily for grouping\n",
    "AT_non_short['speech_block_id'] = speech_block_id\n",
    "\n",
    "# Define aggregation functions\n",
    "# 'Is_Short_Speech' will be 'first' for these blocks, which is False by definition of AT_non_short.\n",
    "agg_funcs = {\n",
    "    'Text': ' '.join,\n",
    "    'Processed_Text': ' '.join\n",
    "}\n",
    "# Add 'first' aggregation for all other columns\n",
    "first_agg_cols = {\n",
    "    col: 'first' for col in AT_non_short.columns\n",
    "    if col not in ['Text', 'Processed_Text', 'speech_block_id']\n",
    "}\n",
    "agg_funcs.update(first_agg_cols)\n",
    "\n",
    "# Perform aggregation to create AT_preprocessed\n",
    "AT_preprocessed = AT_non_short.groupby('speech_block_id').agg(agg_funcs).reset_index(drop=True)\n",
    "\n",
    "print(f\"Shape of AT_preprocessed (short speeches filtered, consecutive speeches merged): {AT_preprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36d979",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ce7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame (AT) with shape (231521, 19) saved to 'AT.pkl'(Pickle format)\n",
      "Preprocessed DataFrame (AT_preprocessed) with shape (170096, 19) saved to 'AT_preprocessed.pkl'(Pickle format)\n"
     ]
    }
   ],
   "source": [
    "AT.to_pickle(r'data folder\\data\\AT.pkl')\n",
    "AT_preprocessed.to_pickle(r'data folder\\data\\AT_preprocessed.pkl')\n",
    "print(f\"Original DataFrame (AT) with shape {AT.shape} saved to 'AT.pkl'(Pickle format)\")\n",
    "print(f\"Preprocessed DataFrame (AT_preprocessed) with shape {AT_preprocessed.shape} saved to 'AT_preprocessed.pkl'(Pickle format)\")\n",
    "\n",
    "# also save AT to .csv but without the 'Processed_Text' column\n",
    "# AT.drop(columns=['Processed_Text'], inplace=True)\n",
    "# AT.to_csv(r'data folder\\data\\AT.csv', index=False, encoding=\"utf-8\")\n",
    "# print(f\"Original DataFrame (AT) with shape {AT.shape} saved to 'AT.csv' (CSV format)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
