{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51d4797",
   "metadata": {},
   "source": [
    "# Parlamint Data Processing Pipeline\n",
    "\n",
    "Automated pipeline for processing ParlaMint 5.0 data. \n",
    "\n",
    "Take raw files and create a big dataframe with additional calculated columns for speech embeddings, segment_ID and segment embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b64778",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "### 1. Download Data\n",
    "\n",
    "Download ParlaMint 5.0 corpus from [CLARIN.SI](https://www.clarin.si/repository/xmlui/):\n",
    "- **English (machine-translated)**: [Link](https://www.clarin.si/repository/xmlui/handle/11356/2006) - Universal, works for all countries\n",
    "- **Native languages**: [Link](https://www.clarin.si/repository/xmlui/handle/11356/2004) - Optional for bilingual analysis\n",
    "\n",
    "**Minimum requirement:** At least one language version per country.\n",
    "\n",
    "### 2. Extract & Organize\n",
    "\n",
    "Extract downloaded files to `BASE_DATA_DIR` (configure in next cell):\n",
    "\n",
    "```\n",
    "data folder/\n",
    "‚îú‚îÄ‚îÄ AT/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ParlaMint5.0-AT-en.ana/ParlaMint-AT-en.txt/  (optional)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ParlaMint-AT/ParlaMint-AT.txt/              (optional)\n",
    "‚îú‚îÄ‚îÄ HR/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ParlaMint5.0-HR-en.ana/ParlaMint-HR-en.txt/  (optional)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ParlaMint-HR/ParlaMint-HR.txt/              (optional)\n",
    "‚îî‚îÄ‚îÄ GB/\n",
    "    ‚îî‚îÄ‚îÄ ParlaMint-GB/ParlaMint-GB.txt/              (required)\n",
    "```\n",
    "\n",
    "### 3. Run All Cells\n",
    "\n",
    "The notebook automatically:\n",
    "- Detects available data (English-only / Native-only / Bilingual)\n",
    "- Optimizes segmentation parameters per country\n",
    "- Creates checkpoints for recovery from interruptions\n",
    "\n",
    "**Adding new countries native language:** Modify `CONFIG` in the next cell with paths and native keywords (for boundary detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43204423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "üìç Data directory: data folder\n",
      "üìä Countries: AT (Austria), HR (Croatia), GB (Great Britain)\n",
      "üíæ Checkpoints: data folder\\checkpoints\n",
      "\n",
      "‚ÑπÔ∏è  Segmentation parameters auto-optimized per country\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === UNIFIED CONFIGURATION ===\n",
    "BASE_DATA_DIR = r\"data folder\"  # ‚Üê CHANGE THIS to your data location\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DATA_DIR, \"checkpoints\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DATA_DIR, \"processed\")\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Country configurations\n",
    "CONFIG = {\n",
    "    'AT': {\n",
    "        'name': 'Austria',\n",
    "        'bilingual': True,\n",
    "        'english_path': os.path.join(BASE_DATA_DIR, \"AT\", \"ParlaMint5.0-AT-en.ana\", \"ParlaMint-AT-en.txt\"),\n",
    "        'native_path': os.path.join(BASE_DATA_DIR, \"AT\", \"ParlaMint-AT\", \"ParlaMint-AT.txt\"),\n",
    "        'native_keywords': ['tagesordnung', 'tagesordnungspunkt', 'punkt', 'verhandlung', \n",
    "                           'behandlung', 'n√§chster', 'weiter', 'fortsetzen']\n",
    "    },\n",
    "    'HR': {\n",
    "        'name': 'Croatia',\n",
    "        'bilingual': True,\n",
    "        'english_path': os.path.join(BASE_DATA_DIR, \"HR\", \"ParlaMint5.0-HR-en.ana\", \"ParlaMint-HR-en.txt\"),\n",
    "        'native_path': os.path.join(BASE_DATA_DIR, \"HR\", \"ParlaMint-HR\", \"ParlaMint-HR.txt\"),\n",
    "        'native_keywords': ['dnevni', 'red', 'toƒçka', 'taƒçka', 'sljedeƒái', 'sljedeƒáe',\n",
    "                           'prijedlog', 'zakon', 'tema', 'nastavljamo', 'prelazimo']\n",
    "    },\n",
    "    'GB': {\n",
    "        'name': 'Great Britain',\n",
    "        'bilingual': False,\n",
    "        'english_path': os.path.join(BASE_DATA_DIR, \"GB\", \"ParlaMint-GB\", \"ParlaMint-GB.txt\"),\n",
    "        'native_keywords': None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Keywords for detecting agenda item transitions (used for English and as fallback)\n",
    "ENGLISH_KEYWORDS = ['agenda', 'proceed', 'point', 'item', 'topic', 'next', 'following', 'move on']\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"üìç Data directory: {BASE_DATA_DIR}\")\n",
    "# Fix the f-string syntax\n",
    "country_list = ', '.join([f\"{c} ({CONFIG[c]['name']})\" for c in CONFIG.keys()])\n",
    "print(f\"üìä Countries: {country_list}\")\n",
    "print(f\"üíæ Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Segmentation parameters auto-optimized per country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ba04b",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "Load parliamentary speeches from year-based folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebe758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading data from source...\n",
      "\n",
      "============================================================\n",
      "Austria (AT)\n",
      "============================================================\n",
      "Loading from: data folder\\AT\\ParlaMint5.0-AT-en.ana\\ParlaMint-AT-en.txt\n",
      "  ‚ö†Ô∏è Path not found: data folder\\AT\\ParlaMint5.0-AT-en.ana\\ParlaMint-AT-en.txt\n",
      "Loading from: data folder\\AT\\ParlaMint-AT\\ParlaMint-AT.txt\n",
      "============================================================\n",
      "Austria (AT)\n",
      "============================================================\n",
      "Loading from: data folder\\AT\\ParlaMint5.0-AT-en.ana\\ParlaMint-AT-en.txt\n",
      "  ‚ö†Ô∏è Path not found: data folder\\AT\\ParlaMint5.0-AT-en.ana\\ParlaMint-AT-en.txt\n",
      "Loading from: data folder\\AT\\ParlaMint-AT\\ParlaMint-AT.txt\n",
      "  Found 27 year folders: 1996 to 2022\n",
      "\n",
      "  Found 27 year folders: 1996 to 2022\n",
      "  Loaded 231,759 speeches\n",
      "  Loaded 231,759 speeches\n",
      "  ‚úÖ NATIVE ONLY mode (231,759 speeches)\n",
      "\n",
      "============================================================\n",
      "Croatia (HR)\n",
      "============================================================\n",
      "Loading from: data folder\\HR\\ParlaMint5.0-HR-en.ana\\ParlaMint-HR-en.txt\n",
      "  ‚ö†Ô∏è Path not found: data folder\\HR\\ParlaMint5.0-HR-en.ana\\ParlaMint-HR-en.txt\n",
      "Loading from: data folder\\HR\\ParlaMint-HR\\ParlaMint-HR.txt\n",
      "  Found 20 year folders: 2003 to 2022\n",
      "  ‚úÖ NATIVE ONLY mode (231,759 speeches)\n",
      "\n",
      "============================================================\n",
      "Croatia (HR)\n",
      "============================================================\n",
      "Loading from: data folder\\HR\\ParlaMint5.0-HR-en.ana\\ParlaMint-HR-en.txt\n",
      "  ‚ö†Ô∏è Path not found: data folder\\HR\\ParlaMint5.0-HR-en.ana\\ParlaMint-HR-en.txt\n",
      "Loading from: data folder\\HR\\ParlaMint-HR\\ParlaMint-HR.txt\n",
      "  Found 20 year folders: 2003 to 2022\n",
      "  Loaded 504,338 speeches\n",
      "  Loaded 504,338 speeches\n",
      "  ‚úÖ NATIVE ONLY mode (504,338 speeches)\n",
      "\n",
      "============================================================\n",
      "Great Britain (GB)\n",
      "============================================================\n",
      "Loading from: data folder\\GB\\ParlaMint-GB\\ParlaMint-GB.txt\n",
      "  Found 8 year folders: 2015 to 2022\n",
      "  ‚úÖ NATIVE ONLY mode (504,338 speeches)\n",
      "\n",
      "============================================================\n",
      "Great Britain (GB)\n",
      "============================================================\n",
      "Loading from: data folder\\GB\\ParlaMint-GB\\ParlaMint-GB.txt\n",
      "  Found 8 year folders: 2015 to 2022\n",
      "  Loaded 670,912 speeches\n",
      "  Loaded 670,912 speeches\n",
      "  ‚úÖ ENGLISH ONLY mode (670,912 speeches)\n",
      "  ‚úÖ ENGLISH ONLY mode (670,912 speeches)\n",
      "\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "‚úÖ Data loaded: ['AT', 'HR', 'GB']\n",
      "\n",
      "üíæ Checkpoint saved\n",
      "\n",
      "‚úÖ Data loaded: ['AT', 'HR', 'GB']\n"
     ]
    }
   ],
   "source": [
    "def load_parlamint_data(parent_folder):\n",
    "    \"\"\"Load ParlaMint data from year folders.\"\"\"\n",
    "    print(f\"Loading from: {parent_folder}\")\n",
    "    \n",
    "    if not os.path.exists(parent_folder):\n",
    "        print(f\"  ‚ö†Ô∏è Path not found: {parent_folder}\")\n",
    "        return None\n",
    "    \n",
    "    df_list = []\n",
    "    year_folders = sorted([f for f in os.listdir(parent_folder) \n",
    "                          if os.path.isdir(os.path.join(parent_folder, f))])\n",
    "    \n",
    "    if not year_folders:\n",
    "        print(f\"  ‚ö†Ô∏è No year folders found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(year_folders)} year folders: {year_folders[0]} to {year_folders[-1]}\")\n",
    "    \n",
    "    for year_folder in year_folders:\n",
    "        folder_path = os.path.join(parent_folder, year_folder)\n",
    "        meta_files = [f for f in os.listdir(folder_path) \n",
    "                     if f.endswith('-meta.tsv') and not f.endswith('-ana-meta.tsv')]\n",
    "        \n",
    "        for meta_file in meta_files:\n",
    "            base = meta_file.replace('-meta.tsv', '')\n",
    "            meta_path = os.path.join(folder_path, meta_file)\n",
    "            txt_path = os.path.join(folder_path, base + '.txt')\n",
    "            \n",
    "            try:\n",
    "                df_meta = pd.read_csv(meta_path, sep='\\t', encoding='utf-8', index_col=False)\n",
    "                \n",
    "                text_map = {}\n",
    "                with open(txt_path, encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('\\t', 1)\n",
    "                        if len(parts) == 2:\n",
    "                            text_map[parts[0]] = parts[1]\n",
    "                \n",
    "                df_meta['Text'] = df_meta['ID'].map(text_map)\n",
    "                df_meta = df_meta[df_meta['Text'].notnull() & (df_meta['Text'].str.strip() != '')]\n",
    "                \n",
    "                if len(df_meta) > 0:  # Only append non-empty dataframes\n",
    "                    df_list.append(df_meta)\n",
    "            except Exception as e:\n",
    "                print(f\"    Error: {meta_file}: {e}\")\n",
    "    \n",
    "    if not df_list:\n",
    "        print(f\"  ‚ö†Ô∏è No valid data loaded\")\n",
    "        return None\n",
    "    \n",
    "    df_all = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    if len(df_all) == 0:\n",
    "        print(f\"  ‚ö†Ô∏è All speeches were empty - no data loaded\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Loaded {len(df_all):,} speeches\")\n",
    "    return df_all\n",
    "\n",
    "\n",
    "# Load all countries\n",
    "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step1_raw_data.pkl')\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(\"üìÇ Loading from checkpoint...\")\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        raw_data = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(raw_data)} countries\")\n",
    "    print(\"\\n‚ö†Ô∏è  NOTE: If you added/removed language data:\")\n",
    "    print(f\"   Delete checkpoints: rm -rf {CHECKPOINT_DIR}/*\")\n",
    "    print(\"   Then rerun this cell\")\n",
    "    \n",
    "    # Reconstruct mode (needed for steps 2-4)\n",
    "    for code, df in raw_data.items():\n",
    "        has_en = 'Text_English' in df.columns and df['Text_English'].notna().any()\n",
    "        has_nat = 'Text_Native' in df.columns and df['Text_Native'].notna().any()\n",
    "        if has_en and has_nat:\n",
    "            CONFIG[code]['mode'] = 'bilingual'\n",
    "        elif has_en:\n",
    "            CONFIG[code]['mode'] = 'english_only'\n",
    "        elif has_nat:\n",
    "            CONFIG[code]['mode'] = 'native_only'\n",
    "        else:\n",
    "            CONFIG[code]['mode'] = 'unknown'\n",
    "else:\n",
    "    print(\"üîÑ Loading data from source...\")\n",
    "    raw_data = {}\n",
    "    \n",
    "    for code, config in CONFIG.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{config['name']} ({code})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        df_english = load_parlamint_data(config['english_path'])\n",
    "        df_native = load_parlamint_data(config['native_path']) if config['bilingual'] else None\n",
    "        \n",
    "        # Start with base dataframe (prefer English for metadata)\n",
    "        if df_english is not None:\n",
    "            df = df_english.copy()\n",
    "            df = df.rename(columns={'Text': 'Text_English'})\n",
    "            has_english = True\n",
    "        elif df_native is not None:\n",
    "            df = df_native.copy()\n",
    "            df = df.rename(columns={'Text': 'Text_Native'})\n",
    "            has_english = False\n",
    "        else:\n",
    "            print(f\"  ‚ùå No data found - skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Add missing language columns (always have both columns)\n",
    "        if has_english and df_native is not None:\n",
    "            # Merge native text\n",
    "            df = df.merge(df_native[['ID', 'Text']], on='ID', how='left', suffixes=('', '_native'))\n",
    "            df = df.rename(columns={'Text_native': 'Text_Native'})\n",
    "            config['mode'] = 'bilingual'\n",
    "            print(f\"  ‚úÖ BILINGUAL mode ({len(df):,} speeches)\")\n",
    "        elif has_english:\n",
    "            # Only English available\n",
    "            df['Text_Native'] = None\n",
    "            config['mode'] = 'english_only'\n",
    "            print(f\"  ‚úÖ ENGLISH ONLY mode ({len(df):,} speeches)\")\n",
    "        else:\n",
    "            # Only Native available\n",
    "            df['Text_English'] = None\n",
    "            config['mode'] = 'native_only'\n",
    "            print(f\"  ‚úÖ NATIVE ONLY mode ({len(df):,} speeches)\")\n",
    "        \n",
    "        raw_data[code] = df\n",
    "    \n",
    "    if not raw_data:\n",
    "        raise ValueError(\"No data loaded. Check your data paths in CONFIG.\")\n",
    "    \n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(raw_data, f)\n",
    "    print(f\"\\nüíæ Checkpoint saved\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded: {list(raw_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a311e2",
   "metadata": {},
   "source": [
    "## Step 2: Speech Embeddings\n",
    "\n",
    "Generate BGE-m3 1024 dimensional embeddings for each speech. The model has a token limit of 8192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e174810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] Austria - NATIVE_ONLY\n",
      "\n",
      "============================================================\n",
      "Generating Speech Embeddings (Text_Native)\n",
      "============================================================\n",
      "Device: cpu, Batch size: 16\n",
      "\n",
      "============================================================\n",
      "Generating Speech Embeddings (Text_Native)\n",
      "============================================================\n",
      "Device: cpu, Batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|          | 64/231759 [02:20<115:50:49,  1.80s/speech]Token indices sequence length is longer than the specified maximum sequence length for this model (9284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9284 > 8192). Running this sequence through the model will result in indexing errors\n",
      "Embedding:   0%|          | 80/231759 [08:17<592:19:08,  9.20s/speech]"
     ]
    }
   ],
   "source": [
    "def add_speech_embeddings(df, text_column='Text', checkpoint_prefix=''):\n",
    "    \"\"\"Generate BGE-m3 embeddings for speeches with 10% checkpoint intervals.\"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Generating Speech Embeddings ({text_column})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 64 if device == \"cuda\" else 16\n",
    "    print(f\"Device: {device}, Batch size: {batch_size}\")\n",
    "    \n",
    "    model = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
    "    tokenizer = model.tokenizer\n",
    "    \n",
    "    # Optimized chunking: 25% overlap with maximum token size\n",
    "    MAX_TOKENS = 8192\n",
    "    CHUNK_SIZE = 8000\n",
    "    STRIDE = 6000\n",
    "    \n",
    "    texts = df[text_column].astype(str).values\n",
    "    total = len(texts)\n",
    "    checkpoint_interval = max(1, total // 10)  # Every 10%\n",
    "    \n",
    "    embeddings = []\n",
    "    last_checkpoint = 0  # Track last checkpoint position\n",
    "    \n",
    "    with tqdm(total=total, desc=\"Embedding\", unit=\"speech\") as pbar:\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = []\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "                \n",
    "                if len(token_ids) <= MAX_TOKENS:\n",
    "                    emb = model.encode([text], convert_to_tensor=False, show_progress_bar=False)[0]\n",
    "                else:\n",
    "                    chunks = []\n",
    "                    for start in range(0, len(token_ids), STRIDE):\n",
    "                        end = min(start + CHUNK_SIZE, len(token_ids))\n",
    "                        chunk = tokenizer.decode(token_ids[start:end], skip_special_tokens=True)\n",
    "                        chunks.append(chunk)\n",
    "                    emb = np.mean(model.encode(chunks, convert_to_tensor=False, show_progress_bar=False), axis=0)\n",
    "                \n",
    "                batch_embeddings.append(emb)\n",
    "            \n",
    "            embeddings.extend(batch_embeddings)\n",
    "            pbar.update(len(batch_texts))\n",
    "            \n",
    "            # Checkpoint every 10% (improved logic)\n",
    "            if checkpoint_prefix and len(embeddings) - last_checkpoint >= checkpoint_interval:\n",
    "                checkpoint_file = os.path.join(CHECKPOINT_DIR, f'{checkpoint_prefix}_partial.pkl')\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(embeddings, f)\n",
    "                progress = int((len(embeddings) / total) * 100)\n",
    "                print(f\"  üíæ Checkpoint saved: {progress}% complete ({len(embeddings):,}/{total:,})\")\n",
    "                last_checkpoint = len(embeddings)\n",
    "            \n",
    "            # More aggressive GPU cleanup\n",
    "            if device == \"cuda\" and i % 1000 == 0:  # Changed from 10000 to 1000\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "    \n",
    "    # Clean up partial checkpoint\n",
    "    if checkpoint_prefix:\n",
    "        partial_checkpoint = os.path.join(CHECKPOINT_DIR, f'{checkpoint_prefix}_partial.pkl')\n",
    "        if os.path.exists(partial_checkpoint):\n",
    "            os.remove(partial_checkpoint)\n",
    "    \n",
    "    df_result = df.copy()\n",
    "    df_result['Speech_Embeddings'] = embeddings\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "# Process all countries\n",
    "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step2_speech_embeddings.pkl')\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(\"üìÇ Loading from checkpoint...\")\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        processed_data = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(processed_data)} countries\")\n",
    "else:\n",
    "    processed_data = {}\n",
    "    \n",
    "    for idx, (code, df) in enumerate(raw_data.items(), 1):\n",
    "        config = CONFIG[code]\n",
    "        print(f\"\\n[{idx}/{len(raw_data)}] {config['name']} - {config['mode'].upper()}\")\n",
    "        \n",
    "        df_emb = df.copy()\n",
    "        \n",
    "        # English embeddings (only if data exists)\n",
    "        if config['mode'] in ['bilingual', 'english_only'] and df['Text_English'].notna().any():\n",
    "            df_temp = add_speech_embeddings(df, 'Text_English', f'step2_{code}_en')\n",
    "            df_emb['Speech_Embeddings_English'] = df_temp['Speech_Embeddings']\n",
    "        else:\n",
    "            df_emb['Speech_Embeddings_English'] = None\n",
    "        \n",
    "        # Native embeddings (only if data exists)\n",
    "        if config['mode'] in ['bilingual', 'native_only'] and df['Text_Native'].notna().any():\n",
    "            df_temp = add_speech_embeddings(df, 'Text_Native', f'step2_{code}_native')\n",
    "            df_emb['Speech_Embeddings_Native'] = df_temp['Speech_Embeddings']\n",
    "        else:\n",
    "            df_emb['Speech_Embeddings_Native'] = None\n",
    "        \n",
    "        processed_data[code] = df_emb\n",
    "        \n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(processed_data, f)\n",
    "    \n",
    "    if os.path.exists(os.path.join(CHECKPOINT_DIR, 'step1_raw_data.pkl')):\n",
    "        os.remove(os.path.join(CHECKPOINT_DIR, 'step1_raw_data.pkl'))\n",
    "\n",
    "print(f\"\\n‚úÖ Speech embeddings complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92af41",
   "metadata": {},
   "source": [
    "## Step 3: Segmentation & Segment IDs\n",
    "\n",
    "Find segment boundaries using **automatic parameter optimization** and multi-signal detection.\n",
    "\n",
    "**How it works:**\n",
    "1. **Auto-optimize parameters** for each country using silhouette scoring\n",
    "   - `window_size`: 3-15 speeches (context window for similarity comparison)\n",
    "   - `min_segment_size`: 5-50 speeches (minimum speeches per topic)\n",
    "\n",
    "2. **Multi-signal boundary detection:**\n",
    "   - **Keyword detection** (weight: 3.0): Chairperson announcements (\"next agenda item\")\n",
    "   - **Similarity drop** (weight: 2.0): Semantic shift between speech windows\n",
    "   - **Distance spike** (weight: 1.0): Large embedding jumps between consecutive speeches\n",
    "\n",
    "3. **Validation:**\n",
    "   - Must be local maximum (highest in ¬±5 speech window)\n",
    "   - Must be ‚â• min_segment_size apart from other boundaries\n",
    "   - Adaptive threshold (top 20% of boundary scores)\n",
    "\n",
    "**Strategy:** Prefer over-segmentation (easier to merge similar segments later than to split under-segmented ones)\n",
    "\n",
    "**Session handling:**\n",
    "- Each `Text_ID` (parliamentary sitting) is segmented independently\n",
    "- Brief sessions (< min_segment_size) ‚Üí 1 segment\n",
    "- Regular sessions ‚Üí Multiple segments based on detected boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_segmentation_params(embeddings, min_window=3, max_window=15, min_seg=5, max_seg=50):\n",
    "    \"\"\"\n",
    "    Optimize WINDOW_SIZE and MIN_SEGMENT_SIZE using silhouette score.\n",
    "    Tests different parameter combinations and returns the best.\n",
    "    \n",
    "    Ranges adjusted for:\n",
    "    - Finer granularity (window: 3-15) to catch short topic transitions\n",
    "    - Longer segments (min_seg: 5-50) to accommodate multi-hour parliamentary debates\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    \n",
    "    best_score = -1\n",
    "    best_params = {'window_size': 7, 'min_segment_size': 10}\n",
    "    \n",
    "    # Sample if too large (for efficiency)\n",
    "    if len(embeddings) > 1000:\n",
    "        indices = np.random.choice(len(embeddings), 1000, replace=False)\n",
    "        sample_embs = embeddings[indices]\n",
    "    else:\n",
    "        sample_embs = embeddings\n",
    "    \n",
    "    for window in range(min_window, max_window + 1, 2):\n",
    "        for min_seg in range(min_seg, max_seg + 1, 5):\n",
    "            if len(sample_embs) < window * 2:\n",
    "                continue\n",
    "            \n",
    "            # Quick clustering to evaluate\n",
    "            n_clusters = max(2, len(sample_embs) // min_seg)\n",
    "            if n_clusters >= len(sample_embs):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "                labels = clustering.fit_predict(sample_embs)\n",
    "                \n",
    "                if len(np.unique(labels)) > 1:\n",
    "                    score = silhouette_score(sample_embs, labels)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {'window_size': window, 'min_segment_size': min_seg}\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "\n",
    "def detect_boundaries_robust(embeddings, texts, roles, keywords, window_size, min_segment_size):\n",
    "    \"\"\"\n",
    "    Robust boundary detection using multiple weighted signals.\n",
    "    \n",
    "    Strategy: Prefer over-segmentation to under-segmentation.\n",
    "    - Over-segmented topics can be merged in post-processing\n",
    "    - Under-segmented topics are difficult to split without re-running pipeline\n",
    "    \"\"\"\n",
    "    n = len(embeddings)\n",
    "    if n < min_segment_size * 2:\n",
    "        return []\n",
    "    \n",
    "    boundary_scores = np.zeros(n)\n",
    "    \n",
    "    # === Signal 1: Keyword Detection (Strong Signal) ===\n",
    "    for i, (text, role) in enumerate(zip(texts, roles)):\n",
    "        if 'Chairperson' in str(role):\n",
    "            text_lower = str(text).lower()\n",
    "            if any(kw in text_lower for kw in keywords):\n",
    "                boundary_scores[i] += 3.0  # High weight\n",
    "    \n",
    "    # === Signal 2: Cosine Similarity Drop (Medium Signal) ===\n",
    "    if n > window_size * 2:\n",
    "        for i in range(window_size, n - window_size):\n",
    "            # Compare windows before and after position i\n",
    "            w_before = embeddings[max(0, i-window_size):i]\n",
    "            w_after = embeddings[i:min(n, i+window_size)]\n",
    "            \n",
    "            if len(w_before) > 0 and len(w_after) > 0:\n",
    "                mean_before = np.mean(w_before, axis=0)\n",
    "                mean_after = np.mean(w_after, axis=0)\n",
    "                \n",
    "                sim = cosine_similarity(mean_before.reshape(1, -1), mean_after.reshape(1, -1))[0][0]\n",
    "                \n",
    "                # Invert similarity to boundary score (low similarity = high boundary score)\n",
    "                boundary_scores[i] += (1 - sim) * 2.0  # Medium weight\n",
    "    \n",
    "    # === Signal 3: Embedding Distance Spike (Weak Signal) ===\n",
    "    for i in range(1, n):\n",
    "        dist = np.linalg.norm(embeddings[i] - embeddings[i-1])\n",
    "        # Normalize and add\n",
    "        boundary_scores[i] += min(dist / 10.0, 1.0)  # Cap at 1.0\n",
    "    \n",
    "    # === Find peaks in boundary scores ===\n",
    "    # Use adaptive threshold - lowered to 80th percentile for more boundaries (over-segmentation)\n",
    "    threshold = np.percentile(boundary_scores, 80)  # Top 20% as candidates\n",
    "    \n",
    "    candidates = []\n",
    "    for i in range(min_segment_size, n - min_segment_size):\n",
    "        if boundary_scores[i] > threshold:\n",
    "            # Check if it's a local maximum\n",
    "            window = boundary_scores[max(0, i-5):min(n, i+6)]\n",
    "            if boundary_scores[i] == np.max(window):\n",
    "                candidates.append((i, boundary_scores[i]))\n",
    "    \n",
    "    # Sort by score and apply minimum distance constraint\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    validated = []\n",
    "    for pos, score in candidates:\n",
    "        if not validated or all(abs(pos - v) >= min_segment_size for v in validated):\n",
    "            validated.append(pos)\n",
    "    \n",
    "    return sorted(validated)\n",
    "\n",
    "\n",
    "def create_segments(dataset, embedding_col, text_col, keywords, text_id_col='Text_ID', \n",
    "                   optimize_params=True, optimized_params=None):\n",
    "    \"\"\"\n",
    "    Create segments with automatic parameter optimization.\n",
    "    \n",
    "    Design philosophy: Favor over-segmentation\n",
    "    - Better to split a long debate into multiple segments\n",
    "    - Post-processing can merge similar adjacent segments\n",
    "    - Under-segmentation is difficult to fix retroactively\n",
    "    \n",
    "    Args:\n",
    "        optimized_params: Optional dict with {'window_size': int, 'min_segment_size': int}\n",
    "                         If provided, skips optimization step\n",
    "    \"\"\"\n",
    "    all_segments = []\n",
    "    stats = {\n",
    "        'total_segments': 0, \n",
    "        'keyword_boundaries': 0, \n",
    "        'similarity_boundaries': 0,\n",
    "        'optimized_params': None\n",
    "    }\n",
    "    \n",
    "    # Use provided params or optimize\n",
    "    if optimized_params:\n",
    "        window_size = optimized_params['window_size']\n",
    "        min_segment_size = optimized_params['min_segment_size']\n",
    "        stats['optimized_params'] = optimized_params\n",
    "        print(f\"Using provided params: window_size={window_size}, min_segment_size={min_segment_size}\")\n",
    "    elif optimize_params:\n",
    "        print(\"üîç Optimizing segmentation parameters...\")\n",
    "        sample_session = dataset[text_id_col].unique()[0]\n",
    "        sample_data = dataset[dataset[text_id_col] == sample_session]\n",
    "        sample_embs = np.array(sample_data[embedding_col].tolist())\n",
    "        \n",
    "        if len(sample_embs) >= 50:\n",
    "            optimal = optimize_segmentation_params(sample_embs)\n",
    "            window_size = optimal['window_size']\n",
    "            min_segment_size = optimal['min_segment_size']\n",
    "            stats['optimized_params'] = optimal\n",
    "            print(f\"‚úÖ Optimized: window_size={window_size}, min_segment_size={min_segment_size}\")\n",
    "        else:\n",
    "            # Fallback to reasonable defaults\n",
    "            window_size = 7\n",
    "            min_segment_size = 10\n",
    "            print(f\"‚ö†Ô∏è Sample too small, using safe defaults: window_size={window_size}, min_segment_size={min_segment_size}\")\n",
    "    else:\n",
    "        # Should not happen but provide fallback\n",
    "        window_size = 7\n",
    "        min_segment_size = 10\n",
    "    \n",
    "    # Process each session\n",
    "    for session_id in tqdm(dataset[text_id_col].unique(), desc=\"Segmenting\", unit=\"session\"):\n",
    "        session = dataset[dataset[text_id_col] == session_id].reset_index(drop=True)\n",
    "        \n",
    "        if len(session) < min_segment_size:\n",
    "            all_segments.append({\n",
    "                'Text_ID': session_id, \n",
    "                'Segment_ID': f\"{session_id}_seg_1\",\n",
    "                'Start_Index': 0, \n",
    "                'End_Index': len(session) - 1\n",
    "            })\n",
    "            stats['total_segments'] += 1\n",
    "            continue\n",
    "        \n",
    "        embeddings = np.array(session[embedding_col].tolist())\n",
    "        texts = session[text_col].values\n",
    "        roles = session['Speaker_role'].values\n",
    "        \n",
    "        # Detect boundaries\n",
    "        boundaries = detect_boundaries_robust(\n",
    "            embeddings, texts, roles, keywords, window_size, min_segment_size\n",
    "        )\n",
    "        \n",
    "        # Count boundary types (approximate)\n",
    "        for b in boundaries:\n",
    "            if 'Chairperson' in str(roles[b]):\n",
    "                stats['keyword_boundaries'] += 1\n",
    "            else:\n",
    "                stats['similarity_boundaries'] += 1\n",
    "        \n",
    "        # Create segments\n",
    "        breaks = [0] + boundaries + [len(session)]\n",
    "        for idx in range(len(breaks) - 1):\n",
    "            start, end = breaks[idx], breaks[idx + 1] - 1\n",
    "            all_segments.append({\n",
    "                'Text_ID': session_id,\n",
    "                'Segment_ID': f\"{session_id}_seg_{idx + 1}\",\n",
    "                'Start_Index': start,\n",
    "                'End_Index': end\n",
    "            })\n",
    "            stats['total_segments'] += 1\n",
    "    \n",
    "    return all_segments, stats\n",
    "\n",
    "\n",
    "def add_segment_ids_to_df(df, segments, text_id_col='Text_ID'):\n",
    "    \"\"\"Map segment IDs to dataframe rows.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['Segment_ID'] = None\n",
    "    \n",
    "    for seg in segments:\n",
    "        mask = df[text_id_col] == seg['Text_ID']\n",
    "        indices = df[mask].index\n",
    "        if len(indices) > seg['Start_Index']:\n",
    "            seg_indices = indices[seg['Start_Index']:seg['End_Index']+1]\n",
    "            df.loc[seg_indices, 'Segment_ID'] = seg['Segment_ID']\n",
    "    \n",
    "    # Fill missing\n",
    "    missing_mask = df['Segment_ID'].isna()\n",
    "    if missing_mask.any():\n",
    "        df.loc[missing_mask, 'Segment_ID'] = df.loc[missing_mask, text_id_col] + '_seg_0'\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Process segmentation\n",
    "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step3_segmentation.pkl')\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(\"üìÇ Loading from checkpoint...\")\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        final_data = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(final_data)} countries\")\n",
    "else:\n",
    "    final_data = {}\n",
    "    \n",
    "    for idx, (code, df) in enumerate(processed_data.items(), 1):\n",
    "        config = CONFIG[code]\n",
    "        print(f\"\\n[{idx}/{len(processed_data)}] {config['name']} - {config['mode'].upper()}\")\n",
    "        \n",
    "        df_final = df.copy()\n",
    "        \n",
    "        # English segmentation (only if embeddings exist)\n",
    "        if config['mode'] in ['bilingual', 'english_only'] and df['Speech_Embeddings_English'].notna().any():\n",
    "            segments_en, stats_en = create_segments(df, 'Speech_Embeddings_English', 'Text_English', \n",
    "                                                     ENGLISH_KEYWORDS, optimize_params=True)\n",
    "            df_temp = add_segment_ids_to_df(df, segments_en)\n",
    "            df_final['Segment_ID_English'] = df_temp['Segment_ID']\n",
    "            print(f\"‚úÖ English: {stats_en['total_segments']:,} segments | params: {stats_en['optimized_params']}\")\n",
    "            optimized_params = stats_en['optimized_params']\n",
    "        else:\n",
    "            df_final['Segment_ID_English'] = None\n",
    "            optimized_params = {'window_size': 7, 'min_segment_size': 10}\n",
    "        \n",
    "        # Native segmentation (only if embeddings exist)\n",
    "        if config['mode'] in ['bilingual', 'native_only'] and df['Speech_Embeddings_Native'].notna().any():\n",
    "            segments_native, _ = create_segments(df, 'Speech_Embeddings_Native', 'Text_Native', \n",
    "                                                 config.get('native_keywords', ENGLISH_KEYWORDS), \n",
    "                                                 optimize_params=False, optimized_params=optimized_params)\n",
    "            df_temp = add_segment_ids_to_df(df, segments_native)\n",
    "            df_final['Segment_ID_Native'] = df_temp['Segment_ID']\n",
    "            print(f\"‚úÖ Native: {len(segments_native):,} segments\")\n",
    "        else:\n",
    "            df_final['Segment_ID_Native'] = None\n",
    "        \n",
    "        final_data[code] = df_final\n",
    "    \n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(final_data, f)\n",
    "    \n",
    "    if os.path.exists(os.path.join(CHECKPOINT_DIR, 'step2_speech_embeddings.pkl')):\n",
    "        os.remove(os.path.join(CHECKPOINT_DIR, 'step2_speech_embeddings.pkl'))\n",
    "\n",
    "print(f\"\\n‚úÖ Segmentation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c1f94",
   "metadata": {},
   "source": [
    "## Step 4: Segment Embeddings\n",
    "\n",
    "Generate embeddings for each segment (concatenated speeches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_segment_embeddings(df, text_col, segment_col, checkpoint_prefix=''):\n",
    "    \"\"\"Generate segment embeddings by concatenating speeches with 10% checkpoints.\"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Segment Embeddings ({segment_col})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 64 if device == \"cuda\" else 16\n",
    "    \n",
    "    model = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
    "    tokenizer = model.tokenizer\n",
    "    \n",
    "    # Optimized chunking: 25% overlap with maximum token size\n",
    "    MAX_TOKENS = 8192\n",
    "    CHUNK_SIZE = 8000\n",
    "    STRIDE = 6000\n",
    "    \n",
    "    # Concatenate speeches by segment\n",
    "    segment_texts = df.groupby(segment_col)[text_col].apply(lambda x: ' '.join(x.astype(str)))\n",
    "    segment_ids = segment_texts.index.tolist()\n",
    "    texts = segment_texts.tolist()\n",
    "    \n",
    "    total = len(texts)\n",
    "    checkpoint_interval = max(1, total // 10)  # Every 10%\n",
    "    print(f\"Processing {total:,} segments...\")\n",
    "    \n",
    "    embeddings = []\n",
    "    last_checkpoint = 0  # Track last checkpoint position\n",
    "    \n",
    "    with tqdm(total=total, desc=\"Embedding\", unit=\"segment\") as pbar:\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_emb = []\n",
    "            \n",
    "            for text in batch:\n",
    "                tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "                if len(tokens) <= MAX_TOKENS:\n",
    "                    emb = model.encode([text], convert_to_tensor=False, show_progress_bar=False)[0]\n",
    "                else:\n",
    "                    chunks = []\n",
    "                    for start in range(0, len(tokens), STRIDE):\n",
    "                        end = min(start + CHUNK_SIZE, len(tokens))\n",
    "                        chunk = tokenizer.decode(tokens[start:end], skip_special_tokens=True)\n",
    "                        chunks.append(chunk)\n",
    "                    emb = np.mean(model.encode(chunks, convert_to_tensor=False, show_progress_bar=False), axis=0)\n",
    "                batch_emb.append(emb)\n",
    "            \n",
    "            embeddings.extend(batch_emb)\n",
    "            pbar.update(len(batch))\n",
    "            \n",
    "            # Checkpoint every 10% (improved logic)\n",
    "            if checkpoint_prefix and len(embeddings) - last_checkpoint >= checkpoint_interval:\n",
    "                checkpoint_file = os.path.join(CHECKPOINT_DIR, f'{checkpoint_prefix}_partial.pkl')\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(embeddings, f)\n",
    "                progress = int((len(embeddings) / total) * 100)\n",
    "                print(f\"  üíæ Checkpoint saved: {progress}% complete ({len(embeddings):,}/{total:,})\")\n",
    "                last_checkpoint = len(embeddings)\n",
    "    \n",
    "    # Clean up partial checkpoint\n",
    "    if checkpoint_prefix:\n",
    "        partial_checkpoint = os.path.join(CHECKPOINT_DIR, f'{checkpoint_prefix}_partial.pkl')\n",
    "        if os.path.exists(partial_checkpoint):\n",
    "            os.remove(partial_checkpoint)\n",
    "    \n",
    "    # Map back to dataframe\n",
    "    emb_map = dict(zip(segment_ids, embeddings))\n",
    "    df = df.copy()\n",
    "    df[f'Segment_Embeddings_{text_col}'] = df[segment_col].map(emb_map)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Process segment embeddings\n",
    "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step4_segment_embeddings.pkl')\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(\"üìÇ Loading from checkpoint...\")\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        final_data = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(final_data)} countries\")\n",
    "else:\n",
    "    for idx, (code, df) in enumerate(final_data.items(), 1):\n",
    "        config = CONFIG[code]\n",
    "        print(f\"\\n[{idx}/{len(final_data)}] {config['name']} - {config['mode'].upper()}\")\n",
    "        \n",
    "        # English segment embeddings (only if segments exist)\n",
    "        if config['mode'] in ['bilingual', 'english_only'] and df['Segment_ID_English'].notna().any():\n",
    "            df = add_segment_embeddings(df, 'Text_English', 'Segment_ID_English', f'step4_{code}_en')\n",
    "            df = df.rename(columns={'Segment_Embeddings_Text_English': 'Segment_Embeddings_English'})\n",
    "        else:\n",
    "            df['Segment_Embeddings_English'] = None\n",
    "        \n",
    "        # Native segment embeddings (only if segments exist)\n",
    "        if config['mode'] in ['bilingual', 'native_only'] and df['Segment_ID_Native'].notna().any():\n",
    "            df = add_segment_embeddings(df, 'Text_Native', 'Segment_ID_Native', f'step4_{code}_native')\n",
    "            df = df.rename(columns={'Segment_Embeddings_Text_Native': 'Segment_Embeddings_Native'})\n",
    "        else:\n",
    "            df['Segment_Embeddings_Native'] = None\n",
    "        \n",
    "        final_data[code] = df\n",
    "    \n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(final_data, f)\n",
    "    \n",
    "    if os.path.exists(os.path.join(CHECKPOINT_DIR, 'step3_segmentation.pkl')):\n",
    "        os.remove(os.path.join(CHECKPOINT_DIR, 'step3_segmentation.pkl'))\n",
    "\n",
    "print(f\"\\n‚úÖ Segment embeddings complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34362eb",
   "metadata": {},
   "source": [
    "## Final Verification\n",
    "\n",
    "Verify all processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caac62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä FINAL VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for code, df in final_data.items():\n",
    "    config = CONFIG[code]\n",
    "    mode = config.get('mode', 'unknown').upper()\n",
    "    print(f\"\\n{config['name']} ({code}) - {mode}:\")\n",
    "    print(f\"  Speeches: {len(df):,}\")\n",
    "    print(f\"  Sessions: {df['Text_ID'].nunique():,}\")\n",
    "    \n",
    "    # Check embeddings\n",
    "    has_english_emb = 'Speech_Embeddings_English' in df.columns and df['Speech_Embeddings_English'].notna().any()\n",
    "    has_native_emb = 'Speech_Embeddings_Native' in df.columns and df['Speech_Embeddings_Native'].notna().any()\n",
    "    \n",
    "    if has_english_emb:\n",
    "        sample_emb = df[df['Speech_Embeddings_English'].notna()]['Speech_Embeddings_English'].iloc[0]\n",
    "        print(f\"  ‚úÖ English speech embeddings: {sample_emb.shape}\")\n",
    "    if has_native_emb:\n",
    "        sample_emb = df[df['Speech_Embeddings_Native'].notna()]['Speech_Embeddings_Native'].iloc[0]\n",
    "        print(f\"  ‚úÖ Native speech embeddings: {sample_emb.shape}\")\n",
    "    \n",
    "    # Check segments\n",
    "    if 'Segment_ID_English' in df.columns and df['Segment_ID_English'].notna().any():\n",
    "        print(f\"  ‚úÖ English segments: {df['Segment_ID_English'].nunique():,}\")\n",
    "    if 'Segment_ID_Native' in df.columns and df['Segment_ID_Native'].notna().any():\n",
    "        print(f\"  ‚úÖ Native segments: {df['Segment_ID_Native'].nunique():,}\")\n",
    "    \n",
    "    # Mode-specific details\n",
    "    if mode == 'BILINGUAL':\n",
    "        print(f\"  üìö Languages: English + Native\")\n",
    "    elif mode == 'ENGLISH_ONLY':\n",
    "        print(f\"  üìù Language: English only\")\n",
    "    elif mode == 'NATIVE_ONLY':\n",
    "        print(f\"  üìù Language: Native only\")\n",
    "\n",
    "print(f\"\\n‚úÖ All processing complete!\")\n",
    "print(f\"\\nüíæ Data available in 'final_data' dictionary\")\n",
    "print(f\"   Access via: final_data['AT'], final_data['HR'], final_data['GB']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ SAVING FINAL PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save each country's processed data (one file per country with all columns)\n",
    "for code, df in final_data.items():\n",
    "    config = CONFIG[code]\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{code}_speeches_processed.pkl\")\n",
    "    \n",
    "    # Save the complete dataframe\n",
    "    df.to_pickle(output_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved {config['name']} ({code}):\")\n",
    "    print(f\"   üìÇ File: {output_path}\")\n",
    "    print(f\"   üìä {len(df):,} speeches | {df['Segment_ID'].nunique():,} segments\")\n",
    "    print(f\"   üìã {len(df.columns)} columns:\")\n",
    "    \n",
    "    # List key columns\n",
    "    key_cols = [c for c in df.columns if 'Embedding' in c or 'Segment_ID' in c]\n",
    "    for col in key_cols:\n",
    "        print(f\"      - {col}\")\n",
    "\n",
    "# Delete Step 4 checkpoint to save space\n",
    "step4_checkpoint = os.path.join(CHECKPOINT_DIR, 'step4_segment_embeddings.pkl')\n",
    "if os.path.exists(step4_checkpoint):\n",
    "    os.remove(step4_checkpoint)\n",
    "    print(f\"\\nüóëÔ∏è  Deleted Step 4 checkpoint (final data saved in {OUTPUT_DIR})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ ALL PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nüìÅ Final outputs saved to: {OUTPUT_DIR}\")\n",
    "print(f\"   - {len(final_data)} country files ({', '.join(final_data.keys())})\")\n",
    "print(f\"\\nüìã Ready for next steps:\")\n",
    "print(f\"   ‚Ä¢ Topic modeling (topic_modelling.ipynb)\")\n",
    "print(f\"   ‚Ä¢ Visualization (visualization.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
