{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdce8ef6",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing for Parliamentary Speeches\n",
    "\n",
    "This notebook provides a reusable function for processing parliamentary speech data from ParlaMint corpora. \n",
    "\n",
    "## Simplified Approach:\n",
    "- **Single output**: Returns one clean dataframe with raw text preserved\n",
    "- **Why?** Modern NLP models (embeddings, LIWC) work better with natural language\n",
    "- **Preprocessing**: Only applied where specifically needed (e.g., BERTopic stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e33d17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Note: Removed NLTK preprocessing since we're keeping raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22638075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing parliamentary data from: data folder\\ParlaMint-AT-en.ana\\ParlaMint-AT-en.txt\n",
      "Found 27 year folders: ['1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\n",
      "  Processing 1996: 54 txt files, 54 tsv files\n",
      "  Processing 1997: 51 txt files, 51 tsv files\n",
      "  Processing 1998: 50 txt files, 50 tsv files\n",
      "  Processing 1999: 32 txt files, 32 tsv files\n",
      "  Processing 2000: 48 txt files, 48 tsv files\n",
      "  Processing 2001: 37 txt files, 37 tsv files\n",
      "    Warning: Row count mismatch in ParlaMint-AT-en_2001-07-06-021-XXI-NRSITZ-00076.txt (323) and ParlaMint-AT-en_2001-07-06-021-XXI-NRSITZ-00076-meta.tsv (325)\n",
      "  Processing 2002: 29 txt files, 29 tsv files\n",
      "  Processing 2003: 40 txt files, 40 tsv files\n",
      "  Processing 2004: 50 txt files, 50 tsv files\n",
      "  Processing 2005: 42 txt files, 42 tsv files\n",
      "  Processing 2006: 37 txt files, 37 tsv files\n",
      "  Processing 2007: 35 txt files, 35 tsv files\n",
      "  Processing 2008: 41 txt files, 41 tsv files\n",
      "    Warning: Row count mismatch in ParlaMint-AT-en_2008-09-24-023-XXIII-NRSITZ-00072.txt (486) and ParlaMint-AT-en_2008-09-24-023-XXIII-NRSITZ-00072-meta.tsv (491)\n",
      "  Processing 2009: 43 txt files, 43 tsv files\n",
      "  Processing 2010: 40 txt files, 40 tsv files\n",
      "  Processing 2011: 47 txt files, 47 tsv files\n",
      "  Processing 2012: 47 txt files, 47 tsv files\n",
      "  Processing 2013: 41 txt files, 41 tsv files\n",
      "  Processing 2014: 48 txt files, 48 tsv files\n",
      "  Processing 2015: 54 txt files, 54 tsv files\n",
      "  Processing 2016: 49 txt files, 49 tsv files\n",
      "  Processing 2017: 46 txt files, 46 tsv files\n",
      "  Processing 2018: 52 txt files, 52 tsv files\n",
      "  Processing 2019: 38 txt files, 38 tsv files\n",
      "  Processing 2020: 68 txt files, 68 tsv files\n",
      "  Processing 2021: 63 txt files, 63 tsv files\n",
      "  Processing 2022: 39 txt files, 39 tsv files\n",
      "\n",
      "Combining all data...\n",
      "Original dataframe shape: (231752, 24)\n",
      "\n",
      "Filtering analysis:\n",
      "  Too short (<10 words): 41119\n",
      "  Remaining for topic modeling: 190633\n",
      "Sorted speeches by Sitting_ID and Speech_ID for consistency\n",
      "\n",
      "Processing complete!\n",
      "Original dataframe shape: (231752, 27)\n",
      "Processed dataframe shape: (190633, 28)\n",
      "Filtered out 41119 short speeches\n",
      "Each speech treated as independent unit for topic modeling\n",
      "Saved original complete dataframe to 'AT_original_complete.pkl'\n",
      "Saved processed dataframe to 'AT_for_topic_modeling.pkl'\n"
     ]
    }
   ],
   "source": [
    "def process_parliament_data(parent_folder, min_word_count=10):  # Changed back to 10\n",
    "    \"\"\"\n",
    "    Process parliamentary speech data, preserving raw text for modern NLP pipelines.\n",
    "    \n",
    "    Args:\n",
    "        parent_folder (str): Path to the folder containing year subdirectories\n",
    "        min_word_count (int): Minimum word count for speeches (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (original_df, processed_df)\n",
    "            - original_df: Complete unfiltered data\n",
    "            - processed_df: Filtered data ready for topic modeling\n",
    "    \"\"\"\n",
    "    print(f\"Processing parliamentary data from: {parent_folder}\")\n",
    "    \n",
    "    # Initialize list to collect dataframes\n",
    "    df_list = []\n",
    "    \n",
    "    # Get year folders (assuming they are named as digits)\n",
    "    year_folders = sorted([f for f in os.listdir(parent_folder) if f.isdigit()])\n",
    "    print(f\"Found {len(year_folders)} year folders: {year_folders}\")\n",
    "    \n",
    "    def process_year_folder(year_folder):\n",
    "        \"\"\"Process all files in a specific year folder.\"\"\"\n",
    "        folder_path = os.path.join(parent_folder, year_folder)\n",
    "        txt_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".txt\")])\n",
    "        tsv_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\"-meta.tsv\")])\n",
    "        \n",
    "        print(f\"  Processing {year_folder}: {len(txt_files)} txt files, {len(tsv_files)} tsv files\")\n",
    "        \n",
    "        if len(txt_files) != len(tsv_files):\n",
    "            print(f\"  Warning: Mismatch in TXT and TSV file counts for {year_folder}!\")\n",
    "            return\n",
    "        \n",
    "        for txt_file, tsv_file in zip(txt_files, tsv_files):\n",
    "            txt_path = os.path.join(folder_path, txt_file)\n",
    "            tsv_path = os.path.join(folder_path, tsv_file)\n",
    "\n",
    "            try:\n",
    "                # Read text and metadata files\n",
    "                df_txt = pd.read_csv(txt_path, sep=\"\\t\", header=None, names=[\"ID\", \"Text\"], encoding=\"utf-8\")\n",
    "                df_meta = pd.read_csv(tsv_path, sep=\"\\t\", encoding=\"utf-8\", index_col=False)\n",
    "                \n",
    "                # Merge on ID\n",
    "                merged_df = pd.merge(df_meta, df_txt, on=\"ID\", how=\"inner\")\n",
    "                \n",
    "                if len(df_txt) != len(df_meta):\n",
    "                    print(f\"    Warning: Row count mismatch in {txt_file} ({len(df_txt)}) and {tsv_file} ({len(df_meta)})\")\n",
    "                \n",
    "                df_list.append(merged_df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error reading files {txt_file}, {tsv_file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Process all year folders\n",
    "    for year in year_folders:\n",
    "        process_year_folder(year)\n",
    "    \n",
    "    if not df_list:\n",
    "        raise ValueError(\"No data was successfully loaded from the specified folder structure\")\n",
    "    \n",
    "    # Combine all data into one dataframe\n",
    "    print(\"\\nCombining all data...\")\n",
    "    original_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Original dataframe shape: {original_df.shape}\")\n",
    "    \n",
    "    # Standardize column names for consistency\n",
    "    if 'Text_ID' in original_df.columns:\n",
    "        original_df.rename(columns={'Text_ID': 'Sitting_ID'}, inplace=True)\n",
    "    if 'ID' in original_df.columns and 'Speech_ID' not in original_df.columns:\n",
    "        original_df.rename(columns={'ID': 'Speech_ID'}, inplace=True)\n",
    "    \n",
    "    # # Drop unnecessary columns if they exist\n",
    "    # columns_to_drop = ['Body', 'Term', 'Session', 'Meeting', 'Sitting', 'Agenda', 'Subcorpus', 'Lang']\n",
    "    # existing_columns_to_drop = [col for col in columns_to_drop if col in original_df.columns]\n",
    "    # if existing_columns_to_drop:\n",
    "    #     original_df.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "    #     print(f\"Dropped columns: {existing_columns_to_drop}\")\n",
    "    \n",
    "    # Calculate word counts for ALL speeches\n",
    "    original_df['Word_Count'] = original_df['Text'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Only filter very short speeches (keep long speeches for topic modeling)\n",
    "    original_df['Is_Too_Short'] = original_df['Word_Count'] < min_word_count\n",
    "    original_df['Is_Filtered'] = original_df['Is_Too_Short']\n",
    "    \n",
    "    print(f\"\\nFiltering analysis:\")\n",
    "    print(f\"  Too short (<{min_word_count} words): {original_df['Is_Too_Short'].sum()}\")\n",
    "    print(f\"  Remaining for topic modeling: {(~original_df['Is_Filtered']).sum()}\")\n",
    "    \n",
    "    # Create processed version for topic modeling (no merging step)\n",
    "    processed_df = original_df[~original_df['Is_Filtered']].copy()\n",
    "    \n",
    "    # Sort by sitting and speech order for consistency (but don't merge)\n",
    "    if 'Sitting_ID' in processed_df.columns and 'Speech_ID' in processed_df.columns:\n",
    "        processed_df = processed_df.sort_values(['Sitting_ID', 'Speech_ID']).reset_index(drop=True)\n",
    "        print(\"Sorted speeches by Sitting_ID and Speech_ID for consistency\")\n",
    "    \n",
    "    # Update word count (should be same as original since no merging)\n",
    "    processed_df['Word_Count'] = processed_df['Text'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Add identifier to track which speeches went into topic modeling\n",
    "    processed_df['Used_For_Topic_Modeling'] = True\n",
    "    \n",
    "    # Don't add Processed_Text column - let topic modeling handle the column structure\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Original dataframe shape: {original_df.shape}\")\n",
    "    print(f\"Processed dataframe shape: {processed_df.shape}\")\n",
    "    print(f\"Filtered out {len(original_df) - len(processed_df)} short speeches\")\n",
    "    print(\"Each speech treated as independent unit for topic modeling\")\n",
    "    \n",
    "    return original_df, processed_df\n",
    "\n",
    "# Process Austrian data with simplified logic\n",
    "parent_folder = r\"data folder\\ParlaMint-AT-en.ana\\ParlaMint-AT-en.txt\"\n",
    "AT_original, AT_processed = process_parliament_data(parent_folder)\n",
    "\n",
    "# Save both versions\n",
    "AT_original.to_pickle(r'data folder\\data\\AT_original_complete.pkl')\n",
    "AT_processed.to_pickle(r'data folder\\data\\AT_for_topic_modeling.pkl')\n",
    "\n",
    "print(f\"Saved original complete dataframe to 'AT_original_complete.pkl'\")\n",
    "print(f\"Saved processed dataframe to 'AT_for_topic_modeling.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
