{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf33514",
   "metadata": {},
   "source": [
    "# ParlaMint Data Processing Pipeline\n",
    "\n",
    "**‚ú® Works locally or on Google Colab** - automatically detects environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac5c88",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "### Option 1: Local Execution\n",
    "1. Download ParlaMint 5.0 from [CLARIN.SI](https://www.clarin.si/repository/xmlui/handle/11356/2006)\n",
    "2. Extract to a local folder\n",
    "3. Update `LOCAL_DATA_DIR` in the configuration cell\n",
    "4. Run all cells\n",
    "\n",
    "### Option 2: Google Colab (GPU Recommended)\n",
    "1. Upload data to Google Drive\n",
    "2. Open this notebook in Colab\n",
    "3. Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "4. Update `COLAB_DATA_DIR` in configuration cell\n",
    "5. Run all cells (will auto-mount Drive)\n",
    "\n",
    "**Data Structure:**\n",
    "```\n",
    "data_folder/\n",
    "‚îú‚îÄ‚îÄ AT/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ParlaMint5.0-AT-en.ana/ParlaMint-AT-en.txt/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ParlaMint-AT/ParlaMint-AT.txt/              (optional)\n",
    "‚îú‚îÄ‚îÄ HR/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ GB/\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection & Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üåê Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install packages\n",
    "    print(\"üì¶ Installing packages...\")\n",
    "    os.system('pip install -q sentence-transformers scikit-learn')\n",
    "    \n",
    "    # Verify GPU\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No GPU detected - processing will be slower\")\n",
    "else:\n",
    "    print(\"üíª Running locally\")\n",
    "\n",
    "print(\"‚úÖ Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Update these paths based on your setup\n",
    "LOCAL_DATA_DIR = r\"data folder\"  # ‚Üê For local execution\n",
    "COLAB_DATA_DIR = \"/content/drive/MyDrive/thesis/data\"  # ‚Üê For Colab\n",
    "\n",
    "# Auto-select based on environment\n",
    "BASE_DATA_DIR = COLAB_DATA_DIR if IN_COLAB else LOCAL_DATA_DIR\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DATA_DIR, \"checkpoints\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DATA_DIR, \"processed\")\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Country configurations\n",
    "CONFIG = {\n",
    "    'AT': {\n",
    "        'name': 'Austria',\n",
    "        'bilingual': True,\n",
    "        'english_path': os.path.join(BASE_DATA_DIR, \"AT\", \"ParlaMint5.0-AT-en.ana\", \"ParlaMint-AT-en.txt\"),\n",
    "        'native_path': os.path.join(BASE_DATA_DIR, \"AT\", \"ParlaMint-AT\", \"ParlaMint-AT.txt\"),\n",
    "        'native_keywords': ['tagesordnung', 'tagesordnungspunkt', 'punkt', 'verhandlung', \n",
    "                           'behandlung', 'n√§chster', 'weiter', 'fortsetzen']\n",
    "    },\n",
    "    'HR': {\n",
    "        'name': 'Croatia',\n",
    "        'bilingual': True,\n",
    "        'english_path': os.path.join(BASE_DATA_DIR, \"HR\", \"ParlaMint5.0-HR-en.ana\", \"ParlaMint-HR-en.txt\"),\n",
    "        'native_path': os.path.join(BASE_DATA_DIR, \"HR\", \"ParlaMint-HR\", \"ParlaMint-HR.txt\"),\n",
    "        'native_keywords': ['dnevni', 'red', 'toƒçka', 'taƒçka', 'sljedeƒái', 'sljedeƒáe',\n",
    "                           'prijedlog', 'zakon', 'tema', 'nastavljamo', 'prelazimo']\n",
    "    },\n",
    "    'GB': {\n",
    "        'name': 'Great Britain',\n",
    "        'bilingual': False,\n",
    "        'english_path': os.path.join(BASE_DATA_DIR, \"GB\", \"ParlaMint-GB\", \"ParlaMint-GB.txt\"),\n",
    "        'native_keywords': None\n",
    "    }\n",
    "}\n",
    "\n",
    "ENGLISH_KEYWORDS = ['agenda', 'proceed', 'point', 'item', 'topic', 'next', 'following', 'move on']\n",
    "\n",
    "# Environment-specific settings\n",
    "BATCH_SIZE_SPEECH = 128 if (IN_COLAB and torch.cuda.is_available()) else 64 if torch.cuda.is_available() else 16\n",
    "BATCH_SIZE_SEGMENT = 32 if torch.cuda.is_available() else 8\n",
    "CHECKPOINT_INTERVAL_SPEECH = 5000 if IN_COLAB else 10000\n",
    "CHECKPOINT_INTERVAL_SEGMENT = 1000\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded\")\n",
    "print(f\"üìç Environment: {'Colab (GPU)' if IN_COLAB else 'Local'}\")\n",
    "print(f\"üìÇ Data directory: {BASE_DATA_DIR}\")\n",
    "print(f\"üéØ Batch sizes: Speech={BATCH_SIZE_SPEECH}, Segment={BATCH_SIZE_SEGMENT}\")\n",
    "print(f\"üíæ Checkpoints: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd54794",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "Load parliamentary speeches from year-based folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parlamint_data(parent_folder):\n",
    "    \"\"\"Load ParlaMint data from year folders.\"\"\"\n",
    "    print(f\"Loading from: {parent_folder}\")\n",
    "    \n",
    "    if not os.path.exists(parent_folder):\n",
    "        print(f\"  ‚ö†Ô∏è Path not found: {parent_folder}\")\n",
    "        return None\n",
    "    \n",
    "    df_list = []\n",
    "    year_folders = sorted([f for f in os.listdir(parent_folder) \n",
    "                          if os.path.isdir(os.path.join(parent_folder, f))])\n",
    "    \n",
    "    if not year_folders:\n",
    "        print(f\"  ‚ö†Ô∏è No year folders found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Loading {len(year_folders)} years: {year_folders[0]}-{year_folders[-1]}\")\n",
    "    \n",
    "    for year_folder in year_folders:\n",
    "        folder_path = os.path.join(parent_folder, year_folder)\n",
    "        meta_files = [f for f in os.listdir(folder_path) \n",
    "                     if f.endswith('-meta.tsv') and not f.endswith('-ana-meta.tsv')]\n",
    "        \n",
    "        for meta_file in meta_files:\n",
    "            base = meta_file.replace('-meta.tsv', '')\n",
    "            txt_path = os.path.join(folder_path, base + '.txt')\n",
    "            \n",
    "            if not os.path.exists(txt_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df_meta = pd.read_csv(os.path.join(folder_path, meta_file), sep='\\t', \n",
    "                                     encoding='utf-8', index_col=False)\n",
    "                \n",
    "                text_map = {}\n",
    "                with open(txt_path, encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('\\t', 1)\n",
    "                        if len(parts) == 2:\n",
    "                            text_map[parts[0]] = parts[1]\n",
    "                \n",
    "                df_meta['Text'] = df_meta['ID'].map(text_map)\n",
    "                df_meta = df_meta[df_meta['Text'].notnull() & (df_meta['Text'].str.strip() != '')]\n",
    "                \n",
    "                if len(df_meta) > 0:\n",
    "                    df_list.append(df_meta)\n",
    "            except Exception as e:\n",
    "                print(f\"    Error {meta_file}: {e}\")\n",
    "    \n",
    "    if not df_list:\n",
    "        return None\n",
    "    \n",
    "    df_all = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"  ‚úÖ {len(df_all):,} speeches\")\n",
    "    return df_all\n",
    "\n",
    "\n",
    "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step1_raw_data.pkl')\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(\"üìÇ Loading from checkpoint...\")\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        raw_data = pickle.load(f)\n",
    "    \n",
    "    # Reconstruct mode\n",
    "    for code, df in raw_data.items():\n",
    "        has_en = 'Text_English' in df.columns and df['Text_English'].notna().any()\n",
    "        has_nat = 'Text_Native' in df.columns and df['Text_Native'].notna().any()\n",
    "        if has_en and has_nat:\n",
    "            CONFIG[code]['mode'] = 'bilingual'\n",
    "        elif has_en:\n",
    "            CONFIG[code]['mode'] = 'english_only'\n",
    "        elif has_nat:\n",
    "            CONFIG[code]['mode'] = 'native_only'\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(raw_data)} countries\")\n",
    "else:\n",
    "    print(\"üîÑ Loading data from source...\")\n",
    "    raw_data = {}\n",
    "    \n",
    "    for code, config in CONFIG.items():\n",
    "        print(f\"\\n{config['name']} ({code})\")\n",
    "        \n",
    "        df_english = load_parlamint_data(config['english_path'])\n",
    "        df_native = load_parlamint_data(config['native_path']) if config['bilingual'] else None\n",
    "        \n",
    "        if df_english is not None:\n",
    "            df = df_english.copy().rename(columns={'Text': 'Text_English'})\n",
    "            has_english = True\n",
    "        elif df_native is not None:\n",
    "            df = df_native.copy().rename(columns={'Text': 'Text_Native'})\n",
    "            has_english = False\n",
    "        else:\n",
    "            print(\"  ‚ùå No data\")\n",
    "            continue\n",
    "        \n",
    "        if has_english and df_native is not None:\n",
    "            df = df.merge(df_native[['ID', 'Text']].rename(columns={'Text': 'Text_Native'}), \n",
    "                         on='ID', how='left')\n",
    "            config['mode'] = 'bilingual'\n",
    "        elif has_english:\n",
    "            df['Text_Native'] = None\n",
    "            config['mode'] = 'english_only'\n",
    "        else:\n",
    "            df['Text_English'] = None\n",
    "            config['mode'] = 'native_only'\n",
    "        \n",
    "        raw_data[code] = df\n",
    "        print(f\"  ‚úÖ {config['mode'].upper()}: {len(df):,} speeches\")\n",
    "    \n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(raw_data, f)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded: {list(raw_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25992a",
   "metadata": {},
   "source": [
    "## Step 2: Speech Embeddings\n",
    "\n",
    "Generate BGE-m3 embeddings for each speech with automatic GPU optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_speech_embeddings(df, text_column, checkpoint_prefix=''):\n",
    "    \"\"\"Generate BGE-m3 embeddings with environment-aware settings.\"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    model = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
    "    tokenizer = model.tokenizer\n",
    "    \n",
    "    MAX_TOKENS, CHUNK_SIZE, STRIDE = 8192, 8000, 6000\n",
    "    texts = df[text_column].astype(str).values\n",
    "    \n",
    "    # Check for partial checkpoint\n",
    "    partial_checkpoint = os.path.join(CHECKPOINT_DIR, f'{checkpoint_prefix}_partial.pkl')\n",
    "    if os.path.exists(partial_checkpoint):\n",
    "        with open(partial_checkpoint, 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        start_idx = len(embeddings)\n",
    "        print(f\"  üìÇ Resuming from {start_idx:,}/{len(texts):,} ({start_idx/len(texts)*100:.1f}%)\")\n",
    "    else:\n",
    "        embeddings = []\n",
    "        start_idx = 0\n",
    "    \n",
    "    with tqdm(total=len(texts), initial=start_idx, desc=f\"Embed {text_column}\", unit=\"speech\") as pbar:\n",
    "        for i in range(start_idx, len(texts), BATCH_SIZE_SPEECH):\n",
    "            batch_texts = texts[i:i+BATCH_SIZE_SPEECH]\n",
    "            batch_embeddings = []\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "                \n",
    "                if len(token_ids) <= MAX_TOKENS:\n",
    "                    emb = model.encode([text], convert_to_tensor=False, show_progress_bar=False)[0]\n",
    "                else:\n",
    "                    chunks = [tokenizer.decode(token_ids[start:min(start + CHUNK_SIZE, len(token_ids))], \n",
    "                                              skip_special_tokens=True)\n",
    "                             for start in range(0, len(token_ids), STRIDE)]\n",
    "                    emb = np.mean(model.encode(chunks, convert_to_tensor=False, show_progress_bar=False), axis=0)\n",
    "                \n",
    "                batch_embeddings.append(emb)\n",
    "            \n",
    "            embeddings.extend(batch_embeddings)\n",
    "            pbar.update(len(batch_texts))\n",
    "            \n",
    "            # Checkpoint at intervals\n",
    "            if checkpoint_prefix and len(embeddings) % CHECKPOINT_INTERVAL_SPEECH < BATCH_SIZE_SPEECH:\n",
    "                with open(partial_checkpoint, 'wb') as f:\n",
    "                    pickle.dump(embeddings, f)\n",
    "            \n",
    "            # GPU cleanup\n",
    "            if device == \"cuda\" and i % (BATCH_SIZE_SPEECH * 10) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "    \n",
    "    # Clean up partial checkpoint\n",
    "    if checkpoint_prefix and os.path.exists(partial_checkpoint):\n",
    "        os.remove(partial_checkpoint)\n",
    "    \n",
    "    df_result = df.copy()\n",
    "    df_result['Speech_Embeddings'] = embeddings\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step2_speech_embeddings.pkl')\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(\"üìÇ Loading from checkpoint...\")\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        processed_data = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(processed_data)} countries\")\n",
    "else:\n",
    "    processed_data = {}\n",
    "    \n",
    "    for idx, (code, df) in enumerate(raw_data.items(), 1):\n",
    "        config = CONFIG[code]\n",
    "        print(f\"\\n[{idx}/{len(raw_data)}] {config['name']} - {config['mode'].upper()}\")\n",
    "        \n",
    "        df_emb = df.copy()\n",
    "        \n",
    "        # English embeddings\n",
    "        if config['mode'] in ['bilingual', 'english_only'] and df['Text_English'].notna().any():\n",
    "            df_temp = add_speech_embeddings(df, 'Text_English', f'step2_{code}_en')\n",
    "            df_emb['Speech_Embeddings_English'] = df_temp['Speech_Embeddings']\n",
    "        else:\n",
    "            df_emb['Speech_Embeddings_English'] = None\n",
    "        \n",
    "        # Native embeddings\n",
    "        if config['mode'] in ['bilingual', 'native_only'] and df['Text_Native'].notna().any():\n",
    "            df_temp = add_speech_embeddings(df, 'Text_Native', f'step2_{code}_native')\n",
    "            df_emb['Speech_Embeddings_Native'] = df_temp['Speech_Embeddings']\n",
    "        else:\n",
    "            df_emb['Speech_Embeddings_Native'] = None\n",
    "        \n",
    "        processed_data[code] = df_emb\n",
    "        \n",
    "        # Save checkpoint after each country\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(processed_data, f)\n",
    "    \n",
    "    # Cleanup\n",
    "    if os.path.exists(os.path.join(CHECKPOINT_DIR, 'step1_raw_data.pkl')):\n",
    "        os.remove(os.path.join(CHECKPOINT_DIR, 'step1_raw_data.pkl'))\n",
    "    \n",
    "    del raw_data\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ Speech embeddings complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2745b3",
   "metadata": {},
   "source": [
    "## Step 3: Segmentation & Segment IDs\n",
    "\n",
    "Find segment boundaries using automatic parameter optimization and multi-signal detection.\n",
    "\n",
    "**Strategy:** Favor over-segmentation (easier to merge similar segments later than to split under-segmented ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8859e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_boundaries_by_keywords(texts, roles, keywords):\n",
    "    return [i for i, (text, role) in enumerate(zip(texts, roles))\n",
    "            if 'Chairperson' in str(role) and any(kw in str(text).lower() for kw in keywords)]\n",
    "\n",
    "\n",
    "def detect_boundaries_by_similarity(embeddings, window_size=1, percentile=95):\n",
    "    n = len(embeddings)\n",
    "    if n < window_size * 2 + 1:\n",
    "        return []\n",
    "    \n",
    "    similarity_drops = []\n",
    "    for i in range(window_size, n - window_size + 1):\n",
    "        window_before = embeddings[max(0, i - window_size):i]\n",
    "        window_after = embeddings[i:min(n, i + window_size)]\n",
    "        \n",
    "        if len(window_before) == 0 or len(window_after) == 0:\n",
    "            continue\n",
    "        \n",
    "        mean_before = np.mean(window_before, axis=0)\n",
    "        mean_after = np.mean(window_after, axis=0)\n",
    "        sim = cosine_similarity(mean_before.reshape(1, -1), mean_after.reshape(1, -1))[0][0]\n",
    "        similarity_drops.append((i, 1 - sim))\n",
    "    \n",
    "    if not similarity_drops:\n",
    "        return []\n",
    "    \n",
    "    threshold = np.percentile([d[1] for d in similarity_drops], percentile)\n",
    "    return [pos for pos, drop in similarity_drops if drop >= threshold]\n",
    "\n",
    "\n",
    "def combine_boundaries(keyword_boundaries, similarity_boundaries, min_distance=3, require_similarity_match=True):\n",
    "    if require_similarity_match:\n",
    "        validated_keywords = [kb for kb in keyword_boundaries \n",
    "                             if any(abs(kb - sb) <= min_distance for sb in similarity_boundaries)]\n",
    "        all_boundaries = set(validated_keywords)\n",
    "        all_boundaries.update(similarity_boundaries)\n",
    "    else:\n",
    "        all_boundaries = set(keyword_boundaries)\n",
    "        for sim_b in similarity_boundaries:\n",
    "            if not any(abs(sim_b - kb) < min_distance for kb in keyword_boundaries):\n",
    "                all_boundaries.add(sim_b)\n",
    "    \n",
    "    return sorted(all_boundaries)\n",
    "\n",
    "\n",
    "def evaluate_segmentation(embeddings, boundaries, texts, roles, keywords):\n",
    "    if len(boundaries) == 0:\n",
    "        return 0.0, {'error': 'No boundaries'}\n",
    "    \n",
    "    keyword_boundaries = detect_boundaries_by_keywords(texts, roles, keywords)\n",
    "    keyword_score = sum(1 for b in boundaries if b in keyword_boundaries) / len(boundaries)\n",
    "    \n",
    "    breaks = [0] + boundaries + [len(embeddings)]\n",
    "    coherence_scores, separation_scores = [], []\n",
    "    \n",
    "    for i in range(len(breaks) - 1):\n",
    "        segment = embeddings[breaks[i]:breaks[i+1]]\n",
    "        if len(segment) > 1:\n",
    "            coherence_scores.append(cosine_similarity(segment).mean())\n",
    "            if i < len(breaks) - 2:\n",
    "                next_segment = embeddings[breaks[i+1]:breaks[i+2]]\n",
    "                if len(next_segment) > 0:\n",
    "                    separation_scores.append(1 - cosine_similarity(segment, next_segment).mean())\n",
    "    \n",
    "    coherence = np.mean(coherence_scores) if coherence_scores else 0\n",
    "    separation = np.mean(separation_scores) if separation_scores else 0\n",
    "    semantic_score = (coherence + separation) / 2\n",
    "    \n",
    "    segment_lengths = [breaks[i+1] - breaks[i] for i in range(len(breaks) - 1)]\n",
    "    \n",
    "    return (keyword_score * 0.5 + semantic_score * 0.5), {\n",
    "        'keyword_score': keyword_score,\n",
    "        'semantic_score': semantic_score,\n",
    "        'coherence': coherence,\n",
    "        'separation': separation,\n",
    "        'avg_length': np.mean(segment_lengths),\n",
    "        'num_segments': len(segment_lengths),\n",
    "        'num_boundaries': len(boundaries)\n",
    "    }\n",
    "\n",
    "\n",
    "def optimize_window_size(embeddings, texts, roles, keywords, percentile=95):\n",
    "    results = []\n",
    "    keyword_boundaries = detect_boundaries_by_keywords(texts, roles, keywords)\n",
    "    \n",
    "    for window in range(1, 11):\n",
    "        similarity_boundaries = detect_boundaries_by_similarity(embeddings, window, percentile)\n",
    "        combined_boundaries = combine_boundaries(keyword_boundaries, similarity_boundaries, \n",
    "                                                 require_similarity_match=True)\n",
    "        \n",
    "        if len(combined_boundaries) == 0:\n",
    "            results.append({'window': window, 'score': 0.0})\n",
    "            continue\n",
    "        \n",
    "        score, stats = evaluate_segmentation(embeddings, combined_boundaries, texts, roles, keywords)\n",
    "        results.append({'window': window, 'score': score, **stats})\n",
    "    \n",
    "    valid_results = [r for r in results if r['score'] > 0]\n",
    "    if not valid_results:\n",
    "        return 5, 0.0, results\n",
    "    \n",
    "    best = max(valid_results, key=lambda x: x['score'])\n",
    "    print(f\"  üîç Optimal window={best['window']} (score={best['score']:.3f}, avg_len={best['avg_length']:.1f})\")\n",
    "    return best['window'], best['score'], results\n",
    "\n",
    "\n",
    "def create_segments(dataset, embedding_col, text_col, keywords, text_id_col='Text_ID', percentile=95):\n",
    "    print(f\"\\n{'='*60}\\nSEGMENTING: {text_col}\\n{'='*60}\")\n",
    "    \n",
    "    # Optimize window size\n",
    "    all_sessions = dataset[text_id_col].unique()\n",
    "    sample_size = min(max(10, int(len(all_sessions) * 0.2)), 50)\n",
    "    sample_sessions = np.random.choice(all_sessions, sample_size, replace=False)\n",
    "    sample_data = dataset[dataset[text_id_col].isin(sample_sessions)]\n",
    "    \n",
    "    if len(sample_data) >= 50:\n",
    "        optimal_window, _, _ = optimize_window_size(\n",
    "            np.array(sample_data[embedding_col].tolist()),\n",
    "            sample_data[text_col].values,\n",
    "            sample_data['Speaker_role'].values,\n",
    "            keywords, percentile\n",
    "        )\n",
    "    else:\n",
    "        optimal_window = 5\n",
    "        print(f\"  ‚ö†Ô∏è Using default window=5\")\n",
    "    \n",
    "    # Segment all sessions\n",
    "    all_segments = []\n",
    "    stats = {'optimal_window': optimal_window, 'total_segments': 0, 'segment_lengths': []}\n",
    "    \n",
    "    for session_id in tqdm(dataset[text_id_col].unique(), desc=\"Segmenting\"):\n",
    "        session = dataset[dataset[text_id_col] == session_id].reset_index(drop=True)\n",
    "        \n",
    "        if len(session) < 5:\n",
    "            all_segments.append({\n",
    "                'Text_ID': session_id, 'Segment_ID': f\"{session_id}_seg_1\",\n",
    "                'Start_Index': 0, 'End_Index': len(session) - 1\n",
    "            })\n",
    "            stats['total_segments'] += 1\n",
    "            stats['segment_lengths'].append(len(session))\n",
    "            continue\n",
    "        \n",
    "        embeddings = np.array(session[embedding_col].tolist())\n",
    "        keyword_boundaries = detect_boundaries_by_keywords(session[text_col].values, \n",
    "                                                          session['Speaker_role'].values, keywords)\n",
    "        similarity_boundaries = detect_boundaries_by_similarity(embeddings, optimal_window, percentile)\n",
    "        combined_boundaries = combine_boundaries(keyword_boundaries, similarity_boundaries, \n",
    "                                                require_similarity_match=True)\n",
    "        \n",
    "        breaks = [0] + combined_boundaries + [len(session)]\n",
    "        for seg_idx in range(len(breaks) - 1):\n",
    "            start, end = breaks[seg_idx], breaks[seg_idx + 1] - 1\n",
    "            stats['segment_lengths'].append(end - start + 1)\n",
    "            all_segments.append({\n",
    "                'Text_ID': session_id,\n",
    "                'Segment_ID': f\"{session_id}_seg_{seg_idx + 1}\",\n",
    "                'Start_Index': start, 'End_Index': end\n",
    "            })\n",
    "            stats['total_segments'] += 1\n",
    "    \n",
    "    stats['avg_length'] = np.mean(stats['segment_lengths'])\n",
    "    print(f\"  ‚úÖ {stats['total_segments']:,} segments | avg={stats['avg_length']:.1f} speeches/segment\")\n",
    "    \n",
    "    return all_segments, stats\n",
    "\n",
    "\n",
    "def add_segment_ids_to_df(df, segments, text_id_col='Text_ID'):\n",
    "    df = df.copy()\n",
    "    df['Segment_ID'] = None\n",
    "    \n",
    "    for seg in segments:\n",
    "        mask = df[text_id_col] == seg['Text_ID']\n",
    "        indices = df[mask].index\n",
    "        if len(indices) > seg['Start_Index']:\n",
    "            df.loc[indices[seg['Start_Index']:seg['End_Index']+1], 'Segment_ID'] = seg['Segment_ID']\n",
    "    \n",
    "    missing_mask = df['Segment_ID'].isna()\n",
    "    if missing_mask.any():\n",
    "        df.loc[missing_mask, 'Segment_ID'] = df.loc[missing_mask, text_id_col] + '_seg_0'\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step3_segmentation.pkl')\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(\"üìÇ Loading from checkpoint...\")\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        final_data = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(final_data)} countries\")\n",
    "else:\n",
    "    final_data = {}\n",
    "    \n",
    "    for idx, (code, df) in enumerate(processed_data.items(), 1):\n",
    "        config = CONFIG[code]\n",
    "        print(f\"\\n[{idx}/{len(processed_data)}] {config['name']} - {config['mode'].upper()}\")\n",
    "        \n",
    "        df_final = df.copy()\n",
    "        \n",
    "        # English segmentation\n",
    "        if config['mode'] in ['bilingual', 'english_only'] and df['Speech_Embeddings_English'].notna().any():\n",
    "            segments_en, stats_en = create_segments(df, 'Speech_Embeddings_English', 'Text_English', \n",
    "                                                     ENGLISH_KEYWORDS)\n",
    "            df_temp = add_segment_ids_to_df(df, segments_en)\n",
    "            df_final['Segment_ID_English'] = df_temp['Segment_ID']\n",
    "            optimal_window = stats_en['optimal_window']\n",
    "        else:\n",
    "            df_final['Segment_ID_English'] = None\n",
    "            optimal_window = 5\n",
    "        \n",
    "        # Native segmentation\n",
    "        if config['mode'] in ['bilingual', 'native_only'] and df['Speech_Embeddings_Native'].notna().any():\n",
    "            segments_native, _ = create_segments(df, 'Speech_Embeddings_Native', 'Text_Native', \n",
    "                                                config.get('native_keywords', ENGLISH_KEYWORDS))\n",
    "            df_temp = add_segment_ids_to_df(df, segments_native)\n",
    "            df_final['Segment_ID_Native'] = df_temp['Segment_ID']\n",
    "        else:\n",
    "            df_final['Segment_ID_Native'] = None\n",
    "        \n",
    "        final_data[code] = df_final\n",
    "        \n",
    "        # Save after each country\n",
    "        with open(checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(final_data, f)\n",
    "    \n",
    "    # Cleanup\n",
    "    if os.path.exists(os.path.join(CHECKPOINT_DIR, 'step2_speech_embeddings.pkl')):\n",
    "        os.remove(os.path.join(CHECKPOINT_DIR, 'step2_speech_embeddings.pkl'))\n",
    "    \n",
    "    del processed_data\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ Segmentation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5603e0",
   "metadata": {},
   "source": [
    "## Step 4: Segment Embeddings\n",
    "\n",
    "Generate embeddings for each segment (concatenated speeches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_segment_embeddings(df, text_col, segment_col, checkpoint_prefix=''):\n",
    "    \"\"\"Generate segment embeddings with environment-aware settings.\"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    model = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
    "    tokenizer = model.tokenizer\n",
    "    MAX_TOKENS, CHUNK_SIZE, STRIDE = 8192, 8000, 6000\n",
    "    \n",
    "    # Concatenate speeches by segment\n",
    "    segment_texts = df.groupby(segment_col)[text_col].apply(lambda x: ' '.join(x.astype(str)))\n",
    "    texts = segment_texts.tolist()\n",
    "    segment_ids = segment_texts.index.tolist()\n",
    "    \n",
    "    # Check for partial checkpoint\n",
    "    partial_checkpoint = os.path.join(CHECKPOINT_DIR, f'{checkpoint_prefix}_partial.pkl')\n",
    "    if os.path.exists(partial_checkpoint):\n",
    "        with open(partial_checkpoint, 'rb') as f:\n",
    "            emb_map = pickle.load(f)\n",
    "        start_idx = len(emb_map)\n",
    "        print(f\"  üìÇ Resuming from {start_idx:,}/{len(texts):,}\")\n",
    "    else:\n",
    "        emb_map = {}\n",
    "        start_idx = 0\n",
    "    \n",
    "    with tqdm(total=len(texts), initial=start_idx, desc=f\"Segment Embed\", unit=\"seg\") as pbar:\n",
    "        for i in range(start_idx, len(texts), BATCH_SIZE_SEGMENT):\n",
    "            batch_texts = texts[i:i+BATCH_SIZE_SEGMENT]\n",
    "            batch_ids = segment_ids[i:i+BATCH_SIZE_SEGMENT]\n",
    "            batch_emb = []\n",
    "            \n",
    "            for text in batch_texts:\n",
    "                try:\n",
    "                    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "                    \n",
    "                    if len(tokens) > MAX_TOKENS * 10:\n",
    "                        batch_emb.append(np.zeros(1024))\n",
    "                        continue\n",
    "                    \n",
    "                    if len(tokens) <= MAX_TOKENS:\n",
    "                        emb = model.encode([text], convert_to_tensor=False, show_progress_bar=False)[0]\n",
    "                    else:\n",
    "                        chunks = [tokenizer.decode(tokens[start:min(start + CHUNK_SIZE, len(tokens))], \n",
    "                                                   skip_special_tokens=True)\n",
    "                                 for start in range(0, len(tokens), STRIDE)]\n",
    "                        \n",
    "                        # Process chunks in sub-batches\n",
    "                        chunk_embeddings = []\n",
    "                        for chunk_idx in range(0, len(chunks), 4):\n",
    "                            chunk_batch = chunks[chunk_idx:chunk_idx+4]\n",
    "                            chunk_emb = model.encode(chunk_batch, convert_to_tensor=False, \n",
    "                                                    show_progress_bar=False)\n",
    "                            chunk_embeddings.extend(chunk_emb)\n",
    "                            \n",
    "                            if device == \"cuda\":\n",
    "                                torch.cuda.empty_cache()\n",
    "                        \n",
    "                        emb = np.mean(chunk_embeddings, axis=0)\n",
    "                    \n",
    "                    batch_emb.append(emb)\n",
    "                except Exception as e:\n",
    "                    batch_emb.append(np.zeros(1024))\n",
    "            \n",
    "            # Update map\n",
    "            for seg_id, emb in zip(batch_ids, batch_emb):\n",
    "                emb_map[seg_id] = emb\n",
    "            \n",
    "            pbar.update(len(batch_texts))\n",
    "            \n",
    "            # GPU cleanup\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Checkpoint\n",
    "            if checkpoint_prefix and len(emb_map) % CHECKPOINT_INTERVAL_SEGMENT < BATCH_SIZE_SEGMENT:\n",
    "                with open(partial_checkpoint, 'wb') as f:\n",
    "                    pickle.dump(emb_map, f)\n",
    "                gc.collect()\n",
    "    \n",
    "    # Clean up partial checkpoint\n",
    "    if checkpoint_prefix and os.path.exists(partial_checkpoint):\n",
    "        os.remove(partial_checkpoint)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[f'Segment_Embeddings_{text_col}'] = df[segment_col].map(emb_map)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "checkpoint_file = os.path.join(CHECKPOINT_DIR, 'step4_segment_embeddings.pkl')\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    print(\"üìÇ Loading from checkpoint...\")\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        segment_data = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(segment_data)} countries\")\n",
    "    \n",
    "    # Use segment_data if exists, otherwise use final_data\n",
    "    final_data = segment_data\n",
    "else:\n",
    "    for idx, (code, df) in enumerate(final_data.items(), 1):\n",
    "        config = CONFIG[code]\n",
    "        print(f\"\\n[{idx}/{len(final_data)}] {config['name']} - {config['mode'].upper()}\")\n",
    "        \n",
    "        # English segment embeddings\n",
    "        if 'Segment_ID_English' in df.columns and df['Segment_ID_English'].notna().any():\n",
    "            if not ('Segment_Embeddings_English' in df.columns and df['Segment_Embeddings_English'].notna().any()):\n",
    "                df = add_segment_embeddings(df, 'Text_English', 'Segment_ID_English', f'step4_{code}_en')\n",
    "                df = df.rename(columns={'Segment_Embeddings_Text_English': 'Segment_Embeddings_English'})\n",
    "                \n",
    "                # Drop speech embeddings to save memory\n",
    "                if 'Speech_Embeddings_English' in df.columns:\n",
    "                    df = df.drop(columns=['Speech_Embeddings_English'])\n",
    "                \n",
    "                final_data[code] = df\n",
    "                \n",
    "                # Save immediately\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(final_data, f)\n",
    "                \n",
    "                print(f\"  ‚úÖ English: saved\")\n",
    "        \n",
    "        # Native segment embeddings\n",
    "        if 'Segment_ID_Native' in df.columns and df['Segment_ID_Native'].notna().any():\n",
    "            if not ('Segment_Embeddings_Native' in df.columns and df['Segment_Embeddings_Native'].notna().any()):\n",
    "                df = add_segment_embeddings(df, 'Text_Native', 'Segment_ID_Native', f'step4_{code}_native')\n",
    "                df = df.rename(columns={'Segment_Embeddings_Text_Native': 'Segment_Embeddings_Native'})\n",
    "                \n",
    "                # Drop speech embeddings to save memory\n",
    "                if 'Speech_Embeddings_Native' in df.columns:\n",
    "                    df = df.drop(columns=['Speech_Embeddings_Native'])\n",
    "                \n",
    "                final_data[code] = df\n",
    "                \n",
    "                # Save immediately\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(final_data, f)\n",
    "                \n",
    "                print(f\"  ‚úÖ Native: saved\")\n",
    "        \n",
    "        final_data[code] = df\n",
    "    \n",
    "    # Cleanup\n",
    "    if os.path.exists(os.path.join(CHECKPOINT_DIR, 'step3_segmentation.pkl')):\n",
    "        os.remove(os.path.join(CHECKPOINT_DIR, 'step3_segmentation.pkl'))\n",
    "\n",
    "print(f\"\\n‚úÖ Segment embeddings complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221556a",
   "metadata": {},
   "source": [
    "## Final Verification & Save\n",
    "\n",
    "Verify all processed data and save to output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc56308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä FINAL VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for code, df in final_data.items():\n",
    "    config = CONFIG[code]\n",
    "    mode = config.get('mode', 'unknown').upper()\n",
    "    print(f\"\\n{config['name']} ({code}) - {mode}:\")\n",
    "    print(f\"  Speeches: {len(df):,}\")\n",
    "    print(f\"  Sessions: {df['Text_ID'].nunique():,}\")\n",
    "    \n",
    "    if 'Segment_ID_English' in df.columns and df['Segment_ID_English'].notna().any():\n",
    "        print(f\"  ‚úÖ English segments: {df['Segment_ID_English'].nunique():,}\")\n",
    "    if 'Segment_ID_Native' in df.columns and df['Segment_ID_Native'].notna().any():\n",
    "        print(f\"  ‚úÖ Native segments: {df['Segment_ID_Native'].nunique():,}\")\n",
    "    \n",
    "    # Check segment embeddings\n",
    "    if 'Segment_Embeddings_English' in df.columns and df['Segment_Embeddings_English'].notna().any():\n",
    "        sample = df[df['Segment_Embeddings_English'].notna()]['Segment_Embeddings_English'].iloc[0]\n",
    "        print(f\"  ‚úÖ English segment embeddings: {sample.shape}\")\n",
    "    if 'Segment_Embeddings_Native' in df.columns and df['Segment_Embeddings_Native'].notna().any():\n",
    "        sample = df[df['Segment_Embeddings_Native'].notna()]['Segment_Embeddings_Native'].iloc[0]\n",
    "        print(f\"  ‚úÖ Native segment embeddings: {sample.shape}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ece01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ SAVING FINAL DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for code, df in final_data.items():\n",
    "    config = CONFIG[code]\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{code}_speeches_processed.pkl\")\n",
    "    \n",
    "    df.to_pickle(output_path)\n",
    "    \n",
    "    n_seg = df['Segment_ID_English'].nunique() if 'Segment_ID_English' in df.columns else 0\n",
    "    print(f\"\\n‚úÖ {config['name']} ({code}):\")\n",
    "    print(f\"   üìÇ {output_path}\")\n",
    "    print(f\"   üìä {len(df):,} speeches | {n_seg:,} segments\")\n",
    "\n",
    "# Delete Step 4 checkpoint to save space\n",
    "step4_checkpoint = os.path.join(CHECKPOINT_DIR, 'step4_segment_embeddings.pkl')\n",
    "if os.path.exists(step4_checkpoint):\n",
    "    os.remove(step4_checkpoint)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nüìÅ Output: {OUTPUT_DIR}\")\n",
    "print(f\"üåê Environment: {'Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"\\nüìã Ready for next steps:\")\n",
    "print(f\"   ‚Ä¢ Topic modeling\")\n",
    "print(f\"   ‚Ä¢ ParlaCAP comparison\")\n",
    "print(f\"   ‚Ä¢ LIWC analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
