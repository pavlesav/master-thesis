{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e88f83",
   "metadata": {},
   "source": [
    "# Parliamentary Speech Embeddings - Speech-Level Processing\n",
    "\n",
    "This notebook implements the first stage of parliamentary speech processing:\n",
    "\n",
    "1. **Setup & Configuration** - Google Colab setup and imports\n",
    "2. **Data Loading & Verification** - Load and verify parliamentary data\n",
    "3. **Speech Embeddings Generation** - Generate BGE-m3 embeddings for individual speeches\n",
    "4. **Data Saving** - Save processed data with speech embeddings\n",
    "\n",
    "## Key Features:\n",
    "- **Multi-parliament support**: Austrian, Croatian, British parliaments\n",
    "- **Multi-language support**: English, German, Croatian\n",
    "- **GPU optimization**: A100 optimized with checkpointing\n",
    "- **Robust processing**: Handles long texts with chunking\n",
    "\n",
    "## Output:\n",
    "Saves datasets with speech embeddings that can be used for segmentation in the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GOOGLE COLAB SETUP ===\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install tqdm python-dotenv sentence-transformers\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# GPU optimization and batch size configuration\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {gpu_name} | Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Dynamic batch size based on GPU type\n",
    "    if 'A100' in gpu_name:\n",
    "        DEFAULT_BATCH_SIZE = 64  # A100 optimized\n",
    "        print(f\"üöÄ A100 detected: Using optimized batch size {DEFAULT_BATCH_SIZE}\")\n",
    "    elif 'V100' in gpu_name or 'T4' in gpu_name:\n",
    "        DEFAULT_BATCH_SIZE = 64   # V100/T4 optimized\n",
    "        print(f\"‚ö° {gpu_name} detected: Using batch size {DEFAULT_BATCH_SIZE}\")\n",
    "    else:\n",
    "        DEFAULT_BATCH_SIZE = 32   # Conservative default\n",
    "        print(f\"üîß Generic GPU detected: Using conservative batch size {DEFAULT_BATCH_SIZE}\")\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"No GPU detected - will use CPU (slower)\")\n",
    "    DEFAULT_BATCH_SIZE = 8  # CPU batch size\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"‚úÖ Setup complete! Default batch size: {DEFAULT_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d5d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ENHANCED CONFIGURATION ===\n",
    "data_folder = '/content/drive/MyDrive/thesis data/'\n",
    "\n",
    "# Enhanced Parliament and Language configurations\n",
    "PARLIAMENT_CONFIG = {\n",
    "    'austrian': {\n",
    "        'english': {\n",
    "            'file': 'AT_en.pkl',\n",
    "            'chairperson_role': 'Chairperson'\n",
    "        },\n",
    "        'german': {\n",
    "            'file': 'AT_german.pkl', \n",
    "            'chairperson_role': 'Pr√§sidentIn'\n",
    "        }\n",
    "    },\n",
    "    'croatian': {\n",
    "        'english': {\n",
    "            'file': 'CRO_en.pkl',\n",
    "            'chairperson_role': 'Chairperson'\n",
    "        },\n",
    "        'croatian': {\n",
    "            'file': 'CRO_hr.pkl',\n",
    "            'chairperson_role': 'Predsjedavajuƒái'\n",
    "        }\n",
    "    },\n",
    "    'british': {\n",
    "        'english': {\n",
    "            'file': 'GB_en.pkl',\n",
    "            'chairperson_role': 'Chairperson'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def list_available_options():\n",
    "    \"\"\"Display available parliament and language options.\"\"\"\n",
    "    print(\"üìã Available Processing Options:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for parliament, languages in PARLIAMENT_CONFIG.items():\n",
    "        print(f\"\\nüèõÔ∏è {parliament.upper()} Parliament:\")\n",
    "        for language, config in languages.items():\n",
    "            print(f\"  ‚Ä¢ {language.capitalize()}: {config['file']}\")\n",
    "            print(f\"    - Chairperson role: '{config['chairperson_role']}'\")\n",
    "\n",
    "def get_config(parliament, language):\n",
    "    \"\"\"Get configuration for specific parliament and language combination.\"\"\"\n",
    "    if parliament not in PARLIAMENT_CONFIG:\n",
    "        raise ValueError(f\"Parliament '{parliament}' not supported. Available: {list(PARLIAMENT_CONFIG.keys())}\")\n",
    "    \n",
    "    if language not in PARLIAMENT_CONFIG[parliament]:\n",
    "        available_langs = list(PARLIAMENT_CONFIG[parliament].keys())\n",
    "        raise ValueError(f\"Language '{language}' not available for {parliament} parliament. Available: {available_langs}\")\n",
    "    \n",
    "    return PARLIAMENT_CONFIG[parliament][language]\n",
    "\n",
    "# Display available options\n",
    "list_available_options()\n",
    "\n",
    "print(f\"\\nüîß Usage examples:\")\n",
    "print(f\"  ‚Ä¢ Austrian Parliament in German: parliament='austrian', language='german'\")\n",
    "print(f\"  ‚Ä¢ Croatian Parliament in Croatian: parliament='croatian', language='croatian'\")\n",
    "print(f\"  ‚Ä¢ British Parliament in English: parliament='british', language='english'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA VERIFICATION FUNCTION ===\n",
    "def verify_parliament_data(parliament, language):\n",
    "    \"\"\"Verify data structure for specific parliament and language.\"\"\"\n",
    "    config = get_config(parliament, language)\n",
    "    data_path = f\"{data_folder}{config['file']}\"\n",
    "    \n",
    "    print(f\"üìä {parliament.upper()} Parliament - {language.upper()} Dataset Verification:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_pickle(data_path)\n",
    "        print(f\"‚úÖ Loaded: {df.shape}\")\n",
    "        print(f\"üìÅ File: {config['file']}\")\n",
    "        print(f\"üìã Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = ['Text_ID', 'Text']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ö†Ô∏è Missing required columns: {missing_cols}\")\n",
    "        else:\n",
    "            print(\"‚úÖ All required columns present\")\n",
    "        \n",
    "        # Check Speaker_role column and chairperson role\n",
    "        if 'Speaker_role' in df.columns:\n",
    "            print(f\"\\nüé≠ Speaker roles found:\")\n",
    "            role_counts = df['Speaker_role'].value_counts().head(10)\n",
    "            for role, count in role_counts.items():\n",
    "                percentage = count / len(df) * 100\n",
    "                marker = \"üëë\" if role == config['chairperson_role'] else \"  \"\n",
    "                print(f\"  {marker} {role}: {count:,} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # Verify chairperson role exists\n",
    "            if config['chairperson_role'] in role_counts.index:\n",
    "                chair_count = role_counts[config['chairperson_role']]\n",
    "                chair_pct = chair_count / len(df) * 100\n",
    "                print(f\"\\n‚úÖ Chairperson role '{config['chairperson_role']}': {chair_count:,} ({chair_pct:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è Chairperson role '{config['chairperson_role']}' not found!\")\n",
    "                print(f\"Available roles: {list(role_counts.index)}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No 'Speaker_role' column found\")\n",
    "        \n",
    "        # Check Text_ID for session identification\n",
    "        if 'Text_ID' in df.columns:\n",
    "            print(f\"\\nüèõÔ∏è Sessions: {df['Text_ID'].nunique():,} unique sessions\")\n",
    "            print(f\"üìù Total speeches: {len(df):,}\")\n",
    "            print(f\"üìä Average speeches per session: {len(df) / df['Text_ID'].nunique():.1f}\")\n",
    "        \n",
    "        # Sample Text_ID format\n",
    "        if len(df) > 0:\n",
    "            print(f\"\\nüìù Sample Text_ID: {df['Text_ID'].iloc[0]}\")\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"üîç Data Verification Examples:\")\n",
    "print(\"Uncomment the lines below to verify specific parliament/language combinations:\\n\")\n",
    "print(\"# verify_parliament_data('austrian', 'english')\")\n",
    "print(\"# verify_parliament_data('croatian', 'croatian')\")\n",
    "print(\"# verify_parliament_data('british', 'english')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SPEECH EMBEDDINGS FUNCTIONS ===\n",
    "\n",
    "def load_and_verify_data(parliament, language):\n",
    "    \"\"\"Load and verify data for specified parliament and language.\"\"\"\n",
    "    config = get_config(parliament, language)\n",
    "    data_path = f\"{data_folder}{config['file']}\"\n",
    "    \n",
    "    df = pd.read_pickle(data_path)\n",
    "    print(f\"‚úÖ Loaded {parliament} parliament ({language}): {df.shape}\")\n",
    "    \n",
    "    # Verify required columns\n",
    "    if 'Text_ID' not in df.columns:\n",
    "        raise ValueError(f\"Text_ID column not found in {parliament} {language} dataset\")\n",
    "        \n",
    "    if 'Speaker_role' in df.columns:\n",
    "        role_counts = df['Speaker_role'].value_counts()\n",
    "        if config['chairperson_role'] in role_counts.index:\n",
    "            print(f\"‚úÖ Found '{config['chairperson_role']}': {role_counts[config['chairperson_role']]:,} speeches\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è '{config['chairperson_role']}' not found in Speaker_role\")\n",
    "            print(f\"Available roles: {list(role_counts.index[:5])}\")\n",
    "    \n",
    "    print(f\"üìä {len(df):,} speeches across {df['Text_ID'].nunique():,} sessions\")\n",
    "    return df\n",
    "\n",
    "def embed_long_text(text, model, tokenizer):\n",
    "    \"\"\"Handle texts longer than model max length.\"\"\"\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    starts = list(range(0, len(token_ids), 4096 - 1024))\n",
    "    for start in starts:\n",
    "        end = min(start + 4096, len(token_ids))\n",
    "        chunk_ids = token_ids[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    chunk_embeddings = model.encode(chunks, batch_size=32, convert_to_tensor=False, show_progress_bar=False)\n",
    "    return np.mean(chunk_embeddings, axis=0)\n",
    "\n",
    "def generate_speech_embeddings(df, text_column='Text', model_name=\"BAAI/bge-m3\", batch_size=None, checkpoint_freq=10000):\n",
    "    \"\"\"Generate BGE-m3 embeddings for individual speeches with GPU optimization.\"\"\"\n",
    "    # Use dynamic batch size if not specified\n",
    "    if batch_size is None:\n",
    "        batch_size = DEFAULT_BATCH_SIZE\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"GENERATING SPEECH EMBEDDINGS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Using batch size: {batch_size}\")\n",
    "    \n",
    "    # Setup checkpointing\n",
    "    checkpoint_dir = '/content/drive/MyDrive/checkpoints/'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = f'{checkpoint_dir}speech_embeddings_checkpoint.pkl'\n",
    "    \n",
    "    # Try to load existing checkpoint\n",
    "    checkpoint_data = None\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            checkpoint_data = pickle.load(f)\n",
    "        print(f\"üìÇ Resuming from checkpoint at index {checkpoint_data['last_processed_idx'] + 1}\")\n",
    "    \n",
    "    start_idx = checkpoint_data['last_processed_idx'] + 1 if checkpoint_data else 0\n",
    "    embeddings = checkpoint_data['embeddings'] if checkpoint_data else []\n",
    "    \n",
    "    # Load model\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        model.half()\n",
    "    tokenizer = model.tokenizer\n",
    "    model.max_seq_length = 8192\n",
    "    \n",
    "    texts = df[text_column].astype(str).values\n",
    "    \n",
    "    with tqdm(total=len(texts), initial=start_idx, desc=\"üöÄ Embedding\", unit=\"speech\") as pbar:\n",
    "        for i in range(start_idx, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Process batch (handle long texts)\n",
    "            batch_embeddings = []\n",
    "            short_texts = []\n",
    "            short_indices = []\n",
    "            \n",
    "            for j, text in enumerate(batch_texts):\n",
    "                token_count = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "                if token_count <= 8192:\n",
    "                    short_texts.append(text)\n",
    "                    short_indices.append(j)\n",
    "                else:\n",
    "                    # Handle long text with chunking\n",
    "                    emb = embed_long_text(text, model, tokenizer)\n",
    "                    batch_embeddings.append((j, emb))\n",
    "            \n",
    "            # Batch process short texts\n",
    "            if short_texts:\n",
    "                actual_batch_size = min(batch_size, len(short_texts))\n",
    "                short_embeddings = model.encode(short_texts, batch_size=actual_batch_size, \n",
    "                                              convert_to_tensor=False, show_progress_bar=False)\n",
    "                for idx, emb in zip(short_indices, short_embeddings):\n",
    "                    batch_embeddings.append((idx, emb))\n",
    "            \n",
    "            batch_embeddings.sort(key=lambda x: x[0])\n",
    "            embeddings.extend([emb for _, emb in batch_embeddings])\n",
    "            pbar.update(len(batch_texts))\n",
    "            \n",
    "            # Checkpoint periodically\n",
    "            if (i + batch_size) % checkpoint_freq == 0:\n",
    "                with open(checkpoint_path, 'wb') as f:\n",
    "                    pickle.dump({'embeddings': embeddings, 'last_processed_idx': i + len(batch_texts) - 1}, f)\n",
    "                if (i + batch_size) % (checkpoint_freq * 4) == 0:\n",
    "                    print(f\"\\nüíæ Progress: {i + batch_size:,}/{len(texts):,}\")\n",
    "            \n",
    "            # Memory management\n",
    "            if (i + batch_size) % (checkpoint_freq * 2) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "    \n",
    "    # Cleanup checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "    \n",
    "    df_result = df.copy()\n",
    "    df_result['Speech_Embeddings'] = embeddings\n",
    "    return df_result\n",
    "\n",
    "def process_speech_embeddings_pipeline(parliament, language):\n",
    "    \"\"\"Complete pipeline for generating and saving speech embeddings.\"\"\"\n",
    "    print(f\"\\nüöÄ Starting Speech Embeddings Pipeline\")\n",
    "    print(f\"Parliament: {parliament.upper()}\")\n",
    "    print(f\"Language: {language.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üéØ Using batch size: {DEFAULT_BATCH_SIZE}\")\n",
    "    \n",
    "    # Define output path\n",
    "    output_path = f\"{data_folder}{parliament}_{language}_with_speech_embeddings.pkl\"\n",
    "    \n",
    "    # Check if already processed\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"üéØ SPEECH EMBEDDINGS ALREADY EXIST: {output_path}\")\n",
    "        df_result = pd.read_pickle(output_path)\n",
    "        print(f\"‚úÖ Loaded existing result: {df_result.shape}\")\n",
    "        return df_result\n",
    "    \n",
    "    # Load raw data\n",
    "    print(\"üì• Loading raw data...\")\n",
    "    df = load_and_verify_data(parliament, language)\n",
    "    \n",
    "    # Generate speech embeddings\n",
    "    print(\"üîÑ Generating speech embeddings...\")\n",
    "    df_with_embeddings = generate_speech_embeddings(df)\n",
    "    \n",
    "    # Save result\n",
    "    print(\"üíæ Saving speech embeddings...\")\n",
    "    df_with_embeddings.to_pickle(output_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ SPEECH EMBEDDINGS PIPELINE COMPLETED!\")\n",
    "    print(f\"üìä Final dataset: {df_with_embeddings.shape}\")\n",
    "    print(f\"üíæ Saved to: {output_path}\")\n",
    "    print(f\"üìà Ready for segmentation pipeline!\")\n",
    "    \n",
    "    return df_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7cd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROCESSING CONFIGURATION ===\n",
    "# Choose parliament and language to process\n",
    "\n",
    "# CONFIGURATION - Update these variables to select what to process\n",
    "PARLIAMENT_TO_PROCESS = 'british'       # Options: 'austrian', 'croatian', 'british'\n",
    "LANGUAGE_TO_PROCESS = 'english'        # Options depend on parliament:\n",
    "                                        # Austrian: 'english', 'german'\n",
    "                                        # Croatian: 'english', 'croatian'\n",
    "                                        # British: 'english' (only option)\n",
    "\n",
    "print(f\"üéØ SPEECH EMBEDDINGS PROCESSING CONFIGURATION\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Parliament: {PARLIAMENT_TO_PROCESS}\")\n",
    "print(f\"Language: {LANGUAGE_TO_PROCESS}\")\n",
    "\n",
    "# Validate configuration\n",
    "try:\n",
    "    config = get_config(PARLIAMENT_TO_PROCESS, LANGUAGE_TO_PROCESS)\n",
    "    print(f\"‚úÖ Configuration valid!\")\n",
    "    print(f\"üìÅ Input file: {config['file']}\")\n",
    "    print(f\"üëë Chairperson role: {config['chairperson_role']}\")\n",
    "    \n",
    "    # Optional: Verify data before processing\n",
    "    print(f\"\\nüìä Data verification:\")\n",
    "    verify_result = verify_parliament_data(PARLIAMENT_TO_PROCESS, LANGUAGE_TO_PROCESS)\n",
    "    \n",
    "    if verify_result is not None:\n",
    "        print(f\"\\nüöÄ Ready to process {PARLIAMENT_TO_PROCESS} parliament in {LANGUAGE_TO_PROCESS}!\")\n",
    "        print(f\"üí° Run the next cell to start processing.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Data verification failed. Please check file path and data structure.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    print(f\"\\nüîß Available options:\")\n",
    "    list_available_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d49ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXECUTE SPEECH EMBEDDINGS PROCESSING IN SEQUENCE ===\n",
    "print(f\"üöÄ STARTING SEQUENTIAL SPEECH EMBEDDINGS PROCESSING\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Define processing queue for Croatian (Croatian) and British (English)\n",
    "processing_queue = [\n",
    "    ('croatian', 'croatian'),\n",
    "    ('british', 'english')\n",
    "]\n",
    "\n",
    "results = {}\n",
    "total_start_time = pd.Timestamp.now()\n",
    "\n",
    "for i, (parliament, language) in enumerate(processing_queue, 1):\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PROCESSING {i}/{len(processing_queue)}: {parliament.upper()} Parliament in {language.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        start_time = pd.Timestamp.now()\n",
    "        result = process_speech_embeddings_pipeline(parliament, language)\n",
    "        end_time = pd.Timestamp.now()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        results[f\"{parliament}_{language}\"] = {\n",
    "            'data': result,\n",
    "            'processing_time': processing_time,\n",
    "            'shape': result.shape,\n",
    "            'speeches_count': len(result)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ {parliament.upper()} {language.upper()} SPEECH EMBEDDINGS COMPLETED!\")\n",
    "        print(f\"‚è±Ô∏è Processing time: {processing_time}\")\n",
    "        print(f\"üìä Dataset shape: {result.shape}\")\n",
    "        \n",
    "        # Clear GPU memory between runs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        # Brief pause between processing\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {parliament} {language}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "total_end_time = pd.Timestamp.now()\n",
    "total_processing_time = total_end_time - total_start_time\n",
    "\n",
    "print(f\"\\nüéâ SEQUENTIAL SPEECH EMBEDDINGS PROCESSING COMPLETED!\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"‚è±Ô∏è Total processing time: {total_processing_time}\")\n",
    "print(f\"üìä Successfully processed: {list(results.keys())}\")\n",
    "\n",
    "for key, info in results.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {info['shape']} ({info['speeches_count']:,} speeches) - {info['processing_time']}\")\n",
    "\n",
    "print(f\"\\nüíæ Output files created:\")\n",
    "for parliament, language in processing_queue:\n",
    "    if f\"{parliament}_{language}\" in results:\n",
    "        print(f\"  ‚Ä¢ {parliament}_{language}_with_speech_embeddings.pkl\")\n",
    "\n",
    "print(f\"\\n‚û°Ô∏è Ready for segmentation and segment embeddings pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690025d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROCESSING SUMMARY AND VERIFICATION ===\n",
    "\n",
    "def verify_sequential_results():\n",
    "    \"\"\"Verify the results of sequential processing.\"\"\"\n",
    "    print(\"üîç VERIFYING SEQUENTIAL PROCESSING RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    processing_queue = [('croatian', 'croatian'), ('british', 'english')]\n",
    "    \n",
    "    for parliament, language in processing_queue:\n",
    "        output_path = f\"{data_folder}{parliament}_{language}_with_speech_embeddings.pkl\"\n",
    "        \n",
    "        print(f\"\\nüìã {parliament.upper()} Parliament ({language.upper()}):\")\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            try:\n",
    "                df = pd.read_pickle(output_path)\n",
    "                print(f\"  ‚úÖ File exists: {output_path}\")\n",
    "                print(f\"  üìä Shape: {df.shape}\")\n",
    "                print(f\"  üî¢ Speeches: {len(df):,}\")\n",
    "                print(f\"  üèõÔ∏è Sessions: {df['Text_ID'].nunique():,}\")\n",
    "                \n",
    "                if 'Speech_Embeddings' in df.columns:\n",
    "                    embedding_shape = df['Speech_Embeddings'].iloc[0].shape\n",
    "                    print(f\"  üéØ Embedding dimensions: {embedding_shape}\")\n",
    "                    print(f\"  ‚úÖ Speech embeddings successfully generated\")\n",
    "                else:\n",
    "                    print(f\"  ‚ùå Speech_Embeddings column missing\")\n",
    "                    \n",
    "                if 'Speaker_role' in df.columns:\n",
    "                    config = get_config(parliament, language)\n",
    "                    chairperson_count = len(df[df['Speaker_role'] == config['chairperson_role']])\n",
    "                    chairperson_pct = chairperson_count / len(df) * 100\n",
    "                    print(f\"  üëë Chairperson speeches: {chairperson_count:,} ({chairperson_pct:.1f}%)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error loading file: {e}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå File not found: {output_path}\")\n",
    "\n",
    "# Run verification\n",
    "verify_sequential_results()\n",
    "\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"1. These files are ready for segmentation processing\")\n",
    "print(f\"2. The segmentation pipeline will identify chairperson segments\")\n",
    "print(f\"3. Generate segment-level embeddings for clustering analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
