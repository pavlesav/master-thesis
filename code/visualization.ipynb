{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb917d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import ttest_ind, pearsonr, f_oneway\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11,\n",
    "    'figure.titlesize': 14\n",
    "})\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Output directory\n",
    "output_dir = r\"C:\\Users\\pavle\\OneDrive\\Desktop\\my github\\master-thesis\\figures\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068ceae",
   "metadata": {},
   "source": [
    "# PART 1: Topic Classification Evaluation\n",
    "\n",
    "Evaluating our topic classification against ParlaCAP predictions and human labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets with topic modeling results\n",
    "GB = pd.read_pickle(r\"data folder\\GB\\GB_final_with_topics.pkl\")\n",
    "AT = pd.read_pickle(r\"data folder\\AT\\AT_final_with_topics_combined.pkl\")\n",
    "HR = pd.read_pickle(r\"data folder\\HR\\HR_final_with_topics_combined.pkl\")\n",
    "\n",
    "# Load human labels\n",
    "hr_labels = pd.read_json(\"data folder/HR/ParlaCAP-test-hr.jsonl\", lines=True)\n",
    "gb_labels = pd.read_json(\"data folder/GB/ParlaCAP-test-en.jsonl\", lines=True)\n",
    "\n",
    "# Drop duplicates and merge\n",
    "for df in [HR, GB, AT]:\n",
    "    df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "\n",
    "HR = HR.merge(hr_labels[['id', 'labels']], left_on='ID', right_on='id', how='left')\n",
    "HR.rename(columns={'labels': 'True_label'}, inplace=True)\n",
    "HR.drop(columns=['id'], inplace=True)\n",
    "\n",
    "GB = GB.merge(gb_labels[['id', 'labels']], left_on='ID', right_on='id', how='left')\n",
    "GB.rename(columns={'labels': 'True_label'}, inplace=True)\n",
    "GB.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Load HDBSCAN results\n",
    "GB_hdbscan = pd.read_pickle(r\"data folder\\GB\\GB_with_topics_hdbscan.pkl\")\n",
    "AT_hdbscan = pd.read_pickle(r\"data folder\\AT\\AT_with_topics_hdbscan.pkl\")\n",
    "HR_hdbscan = pd.read_pickle(r\"data folder\\HR\\HR_with_topics_hdbscan.pkl\")\n",
    "\n",
    "# Append HDBSCAN topics\n",
    "GB['my_topic_hdbscan'] = GB_hdbscan['my_topic']\n",
    "HR['my_topic_en_hdbscan'] = HR_hdbscan['my_topic_en']\n",
    "HR['my_topic_hr_hdbscan'] = HR_hdbscan['my_topic_native_language']\n",
    "AT['my_topic_en_hdbscan'] = AT_hdbscan['my_topic_en']\n",
    "AT['my_topic_de_hdbscan'] = AT_hdbscan['my_topic_native_language']\n",
    "\n",
    "# Rename GMM columns\n",
    "HR.rename(columns={'Segment_Category_HR_english': 'my_topic_en_gmm',\n",
    "                   'Segment_Category_HR_croatian': 'my_topic_hr_gmm'}, inplace=True)\n",
    "AT.rename(columns={'Segment_Category_AT_english': 'my_topic_en_gmm',\n",
    "                   'Segment_Category_AT_german': 'my_topic_de_gmm'}, inplace=True)\n",
    "GB.rename(columns={'Segment_Category_GB_english': 'my_topic_gmm'}, inplace=True)\n",
    "\n",
    "print(f\"✅ Loaded: AT={AT.shape}, HR={HR.shape}, GB={GB.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e797c",
   "metadata": {},
   "source": [
    "## Determine Topic Consensus\n",
    "\n",
    "Create consensus topics by combining GMM and HDBSCAN predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_consensus(row, topic_cols, is_chairperson_col='Speaker_role'):\n",
    "    \"\"\"Determine consensus topic from multiple predictions\"\"\"\n",
    "    topics = [row[col] for col in topic_cols]\n",
    "    topics = [t for t in topics if t != 'Other']\n",
    "    \n",
    "    if row.get(is_chairperson_col) == 'Chairperson':\n",
    "        return topics[0] if len(set(topics)) == 1 and len(topics) == len(topic_cols) else 'Other'\n",
    "    else:\n",
    "        if not topics:\n",
    "            return 'Other'\n",
    "        topic_counts = pd.Series(topics).value_counts()\n",
    "        if topic_counts.iloc[0] > 1:\n",
    "            return topic_counts.idxmax()\n",
    "        if len(topic_counts) > 1 and topic_counts.iloc[0] == topic_counts.iloc[1]:\n",
    "            return 'Mix'\n",
    "        return topic_counts.idxmax()\n",
    "\n",
    "# Apply consensus\n",
    "HR['topic_consensus'] = HR.apply(lambda r: determine_consensus(\n",
    "    r, ['my_topic_en_gmm', 'my_topic_hr_gmm', 'my_topic_en_hdbscan', 'my_topic_hr_hdbscan']), axis=1)\n",
    "\n",
    "AT['topic_consensus'] = AT.apply(lambda r: determine_consensus(\n",
    "    r, ['my_topic_en_gmm', 'my_topic_de_gmm', 'my_topic_en_hdbscan', 'my_topic_de_hdbscan']), axis=1)\n",
    "\n",
    "GB['topic_consensus'] = GB.apply(lambda r: determine_consensus(\n",
    "    r, ['my_topic_gmm', 'my_topic_hdbscan']), axis=1)\n",
    "\n",
    "print(\"✅ Consensus topics determined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea88a56",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7175fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_labels(df, evaluation_col, true_label_col, sample_size='all', min_word_count=None,\n",
    "                   exclude_roles=None, exclude_topics=None, text_column='Text', \n",
    "                   plot_size=(16, 12), dataset_name=\"Dataset\"):\n",
    "    \"\"\"Generate confusion matrix and metrics\"\"\"\n",
    "    \n",
    "    exclude_roles = exclude_roles or []\n",
    "    exclude_topics = exclude_topics or []\n",
    "    \n",
    "    # Filter\n",
    "    df = df[~df[true_label_col].isin(['-'])].dropna(subset=[evaluation_col, true_label_col])\n",
    "    \n",
    "    if exclude_roles:\n",
    "        df = df[~df['Speaker_role'].isin(exclude_roles)]\n",
    "    if exclude_topics:\n",
    "        df = df[~df[evaluation_col].isin(exclude_topics)]\n",
    "        df = df[~df[true_label_col].isin(exclude_topics)]\n",
    "    if min_word_count:\n",
    "        df = df[df[text_column].apply(lambda x: len(str(x).split())) >= min_word_count]\n",
    "    \n",
    "    # Sample\n",
    "    if sample_size != 'all':\n",
    "        sampled = []\n",
    "        for cat in df[true_label_col].unique():\n",
    "            cat_df = df[df[true_label_col] == cat]\n",
    "            sampled.append(cat_df.sample(n=min(sample_size, len(cat_df))))\n",
    "        df = pd.concat(sampled, ignore_index=True)\n",
    "    \n",
    "    # Metrics\n",
    "    y_true = df[true_label_col]\n",
    "    y_pred = df[evaluation_col]\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=plot_size)\n",
    "    categories = y_true.value_counts().index.tolist()\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=categories)\n",
    "    sns.heatmap(conf_matrix, xticklabels=categories, yticklabels=categories, annot=True, \n",
    "                fmt='d', cmap='Blues', square=True, annot_kws={'size': 8})\n",
    "    plt.title(f'{dataset_name}: F1 macro={f1_macro:.2f}, micro={f1_micro:.2f}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'confusion_{dataset_name.lower()}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {'f1_macro': f1_macro, 'f1_micro': f1_micro, 'total': len(df)}\n",
    "\n",
    "# Run analysis\n",
    "results = {}\n",
    "results['GB'] = analyze_labels(GB, 'topic_consensus', 'True_label', exclude_topics=['Mix'], dataset_name=\"GB\")\n",
    "results['HR'] = analyze_labels(HR, 'topic_consensus', 'True_label', exclude_topics=['Mix'], dataset_name=\"HR\")\n",
    "results['AT'] = analyze_labels(AT, 'topic_consensus', 'Topic', exclude_topics=['Mix'], \n",
    "                               min_word_count=70, dataset_name=\"AT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37ae29",
   "metadata": {},
   "source": [
    "## Topic Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_topic_distributions(df, our_col, comparison_col, dataset_name, exclude_topics=['Other', 'Mix']):\n",
    "    \"\"\"Compare topic distributions\"\"\"\n",
    "    clean = df.dropna(subset=[our_col, comparison_col])\n",
    "    for topic in exclude_topics:\n",
    "        clean = clean[(clean[our_col] != topic) & (clean[comparison_col] != topic)]\n",
    "    \n",
    "    our_dist = clean[our_col].value_counts(normalize=True) * 100\n",
    "    comp_dist = clean[comparison_col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    comparison = pd.DataFrame({'Our': our_dist, 'Reference': comp_dist}).fillna(0)\n",
    "    diff = (comparison['Our'] - comparison['Reference']).reindex(\n",
    "        (comparison['Our'] - comparison['Reference']).abs().sort_values(ascending=True).index)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    comparison.plot(kind='barh', ax=ax1, width=0.8, alpha=0.8)\n",
    "    ax1.set_title(f'{dataset_name}: Topic Distribution Comparison')\n",
    "    ax1.set_xlabel('Percentage (%)')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    colors = ['red' if x < 0 else 'green' for x in diff]\n",
    "    diff.plot(kind='barh', ax=ax2, color=colors, alpha=0.7)\n",
    "    ax2.set_title(f'{dataset_name}: Difference (Our - Reference)')\n",
    "    ax2.set_xlabel('Percentage Point Difference')\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'dist_{dataset_name.lower()}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "compare_topic_distributions(GB, 'topic_consensus', 'Topic', 'GB')\n",
    "compare_topic_distributions(HR, 'topic_consensus', 'Topic', 'HR')\n",
    "compare_topic_distributions(AT, 'topic_consensus', 'Topic', 'AT')\n",
    "\n",
    "print(\"✅ Topic classification evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771eac5a",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: LIWC Linguistic Analysis\n",
    "\n",
    "Analyzing political discourse using LIWC-22 dimensions across countries, topics, and covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c95b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC data\n",
    "AT_LIWC = pd.read_csv(r\"data folder\\AT\\AT_LIWC_results.csv\")\n",
    "HR_LIWC = pd.read_csv(r\"data folder\\HR\\HR_LIWC_results.csv\")\n",
    "GB_LIWC = pd.read_csv(r\"data folder\\GB\\GB_LIWC_results.csv\")\n",
    "\n",
    "# Merge with existing data\n",
    "AT = AT.merge(AT_LIWC, on='ID', how='inner')\n",
    "HR = HR.merge(HR_LIWC, on='ID', how='inner')\n",
    "GB = GB.merge(GB_LIWC, on='ID', how='inner')\n",
    "\n",
    "# Add country identifier\n",
    "AT['Country'] = 'Austria'\n",
    "HR['Country'] = 'Croatia'\n",
    "GB['Country'] = 'Great Britain'\n",
    "\n",
    "# Combine\n",
    "LIWC_ALL = pd.concat([AT, HR, GB], ignore_index=True)\n",
    "\n",
    "# Process dates\n",
    "LIWC_ALL['Date'] = pd.to_datetime(LIWC_ALL['Date'], errors='coerce')\n",
    "LIWC_ALL['Year'] = LIWC_ALL['Date'].dt.year\n",
    "\n",
    "# Rename and filter\n",
    "LIWC_ALL.rename(columns={'topic_consensus': 'Our_Topic', 'Topic': 'ParlaCAP'}, inplace=True)\n",
    "LIWC_ALL = LIWC_ALL[~LIWC_ALL['Our_Topic'].isin(['Mix', 'Other'])]\n",
    "LIWC_ALL = LIWC_ALL[LIWC_ALL['Speaker_role'] != 'Chairperson']\n",
    "\n",
    "# Calculate age\n",
    "LIWC_ALL['Speaker_birth'] = pd.to_numeric(LIWC_ALL['Speaker_birth'], errors='coerce')\n",
    "LIWC_ALL['Speaker_age'] = LIWC_ALL['Year'] - LIWC_ALL['Speaker_birth']\n",
    "\n",
    "# Load LIWC benchmarks\n",
    "LIWC_statistics_path = r\"data folder\\LIWC-22.Descriptive.Statistics-Test.Kitchen.xlsx\"\n",
    "\n",
    "def load_liwc_benchmarks(file_path):\n",
    "    \"\"\"Load LIWC-22 population norms\"\"\"\n",
    "    raw = pd.read_excel(file_path, sheet_name=0, header=None)\n",
    "    header_row = raw.iloc[0]\n",
    "    total_col_start = [i for i, v in enumerate(header_row) if str(v).strip() == 'Total'][0]\n",
    "    \n",
    "    dimensions = raw.iloc[2:, 0].dropna().reset_index(drop=True)\n",
    "    means = pd.to_numeric(raw.iloc[2:2+len(dimensions), total_col_start], errors='coerce')\n",
    "    stds = pd.to_numeric(raw.iloc[2:2+len(dimensions), total_col_start+1], errors='coerce')\n",
    "    \n",
    "    return pd.DataFrame({'Dimension': dimensions, 'Mean': means.values, 'Std': stds.values}).dropna()\n",
    "\n",
    "LIWC_benchmarks = load_liwc_benchmarks(LIWC_statistics_path)\n",
    "\n",
    "# Key dimensions\n",
    "KEY_LIWC_DIMENSIONS = [\n",
    "    'Analytic', 'Clout', 'Authentic', 'Tone',\n",
    "    'i', 'we', 'you', 'they', 'ipron', 'ppron',\n",
    "    'focuspast', 'focuspresent', 'focusfuture',\n",
    "    'cogproc', 'insight', 'cause', 'discrep', 'tentat', 'certitude',\n",
    "    'Affect', 'tone_pos', 'tone_neg',\n",
    "    'Social', 'conflict', 'moral',\n",
    "    'power', 'politic', 'money', 'work'\n",
    "]\n",
    "\n",
    "print(f\"✅ LIWC data loaded: {len(LIWC_ALL):,} speeches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b07d1",
   "metadata": {},
   "source": [
    "## Cross-Country LIWC Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_country_heatmap(data, benchmarks, dimensions):\n",
    "    \"\"\"Z-score heatmap across countries\"\"\"\n",
    "    country_means = data.groupby('Country')[dimensions].mean()\n",
    "    overall_means = data[dimensions].mean()\n",
    "    country_means.loc['All Countries'] = overall_means\n",
    "    \n",
    "    benchmark_lookup = benchmarks.set_index('Dimension')\n",
    "    z_scores = (country_means - benchmark_lookup.loc[dimensions, 'Mean']) / benchmark_lookup.loc[dimensions, 'Std']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 14))\n",
    "    sns.heatmap(z_scores.T, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0, \n",
    "                cbar_kws={'label': 'Z-Score'}, vmin=-2, vmax=2, ax=ax, square=False)\n",
    "    ax.set_title(\"Political Discourse: LIWC Z-Scores vs Population Norms\", fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'liwc_country_zscores.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "create_country_heatmap(LIWC_ALL, LIWC_benchmarks, KEY_LIWC_DIMENSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ceb577",
   "metadata": {},
   "source": [
    "## Political Covariates Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cce0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_covariate_heatmap(data, benchmarks, covariate_col, categories, dimensions, title, filename):\n",
    "    \"\"\"Covariate analysis heatmap\"\"\"\n",
    "    filtered = data[data[covariate_col].isin(categories)].copy()\n",
    "    \n",
    "    # Z-score normalize\n",
    "    for dim in dimensions:\n",
    "        mean = benchmarks.set_index('Dimension').loc[dim, 'Mean']\n",
    "        std = benchmarks.set_index('Dimension').loc[dim, 'Std']\n",
    "        filtered[f'{dim}_z'] = (filtered[dim] - mean) / std\n",
    "    \n",
    "    means = filtered.groupby(covariate_col)[[f'{d}_z' for d in dimensions]].mean()\n",
    "    means.columns = [c.replace('_z', '') for c in means.columns]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.heatmap(means.T, annot=True, fmt='.2f', cmap='RdBu_r', center=0, \n",
    "                vmin=-1, vmax=1, ax=ax, square=False)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Fixed dimensions for all plots\n",
    "FIXED_DIMS = ['Analytic', 'Authentic', 'Clout', 'Tone', 'power']\n",
    "\n",
    "# Party Status\n",
    "create_covariate_heatmap(LIWC_ALL, LIWC_benchmarks, 'Party_status', \n",
    "                         ['Coalition', 'Opposition'], FIXED_DIMS + ['we', 'i', 'cogproc', 'moral', 'politic'],\n",
    "                         'Coalition vs Opposition', 'liwc_party_status.png')\n",
    "\n",
    "# Gender\n",
    "create_covariate_heatmap(LIWC_ALL, LIWC_benchmarks, 'Speaker_gender', \n",
    "                         ['M', 'F'], FIXED_DIMS + ['we', 'i', 'Affect', 'certitude', 'tentat'],\n",
    "                         'Gender Differences', 'liwc_gender.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725068a",
   "metadata": {},
   "source": [
    "## Temporal Evolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_plot(data, benchmarks, dimensions, countries, title, filename):\n",
    "    \"\"\"Temporal evolution with events\"\"\"\n",
    "    benchmark_lookup = benchmarks.set_index('Dimension')\n",
    "    temporal_data = data[data['Date'].notna()].copy()\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        mean = benchmark_lookup.loc[dim, 'Mean']\n",
    "        std = benchmark_lookup.loc[dim, 'Std']\n",
    "        temporal_data[f'{dim}_z'] = (temporal_data[dim] - mean) / std\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(18, 12))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, country in enumerate(countries):\n",
    "        ax = axes[idx]\n",
    "        country_data = temporal_data[temporal_data['Country'] == country].copy()\n",
    "        country_data['YearMonth'] = country_data['Date'].dt.to_period('M')\n",
    "        \n",
    "        monthly = country_data.groupby('YearMonth')[[f'{d}_z' for d in dimensions]].mean()\n",
    "        monthly.columns = [c.replace('_z', '') for c in monthly.columns]\n",
    "        monthly.index = monthly.index.to_timestamp()\n",
    "        monthly_smooth = monthly.rolling(window=3, center=True).mean()\n",
    "        \n",
    "        for dim in monthly_smooth.columns:\n",
    "            ax.plot(monthly_smooth.index, monthly_smooth[dim], linewidth=2.5, label=dim)\n",
    "        \n",
    "        ax.axhline(y=0, color='gray', linestyle='--', alpha=0.6)\n",
    "        ax.set_title(country, fontweight='bold', loc='left')\n",
    "        ax.set_xlabel('Year', fontweight='bold')\n",
    "        ax.set_ylabel('Z-Score (3-month avg)', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if idx == 0:\n",
    "            ax.legend(loc='upper left', ncol=2)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "create_temporal_plot(LIWC_ALL, LIWC_benchmarks, ['Analytic', 'Authentic', 'Clout', 'Tone'],\n",
    "                    ['Austria', 'Croatia', 'Great Britain'], \n",
    "                    'Temporal Evolution: Summary Variables', 'liwc_temporal_summary.png')\n",
    "\n",
    "create_temporal_plot(LIWC_ALL, LIWC_benchmarks, ['politic', 'we', 'i', 'you'],\n",
    "                    ['Austria', 'Croatia', 'Great Britain'],\n",
    "                    'Temporal Evolution: Political Language', 'liwc_temporal_political.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2006c",
   "metadata": {},
   "source": [
    "## Final Save\n",
    "\n",
    "Save processed datasets with all analysis columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final datasets after all analysis\n",
    "GB.to_pickle(r\"data folder\\GB\\GB_final.pkl\")\n",
    "AT.to_pickle(r\"data folder\\AT\\AT_final.pkl\")\n",
    "HR.to_pickle(r\"data folder\\HR\\HR_final.pkl\")\n",
    "\n",
    "print(\"✅ All analysis complete!\")\n",
    "print(f\"   - Topic classification evaluation\")\n",
    "print(f\"   - LIWC linguistic analysis\")\n",
    "print(f\"   - Final datasets saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
