{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb917d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)  # Set to None for unlimited column width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets with combined topic modeling results from the topic modeling notebook\n",
    "GB = pd.read_pickle(r\"data folder\\GB\\GB_final_with_topics.pkl\")\n",
    "AT = pd.read_pickle(r\"data folder\\AT\\AT_final_with_topics_combined.pkl\")\n",
    "HR = pd.read_pickle(r\"data folder\\HR\\HR_final_with_topics_combined.pkl\")\n",
    "\n",
    "# Load topic information dataframes with classifications\n",
    "# gb_topics = pd.read_pickle(r\"data folder\\GB\\GB_topic_info_classified_english.pkl\")\n",
    "# at_en_topics = pd.read_pickle(r\"data folder\\AT\\AT_topic_info_classified_english.pkl\")\n",
    "# at_de_topics = pd.read_pickle(r\"data folder\\AT\\AT_topic_info_classified_german.pkl\")\n",
    "# hr_en_topics = pd.read_pickle(r\"data folder\\HR\\HR_topic_info_classified_english.pkl\")\n",
    "# hr_hr_topics = pd.read_pickle(r\"data folder\\HR\\HR_topic_info_classified_croatian.pkl\")\n",
    "\n",
    "# load true labels \n",
    "\n",
    "hr_labels = pd.read_json(\"data folder/HR/ParlaCAP-test-hr.jsonl\", lines=True)\n",
    "gb_labels = pd.read_json(\"data folder/GB/ParlaCAP-test-en.jsonl\", lines=True) \n",
    "\n",
    "\n",
    "# Drop duplicates in HR and GB based on ID\n",
    "HR = HR.drop_duplicates(subset=['ID'])\n",
    "GB = GB.drop_duplicates(subset=['ID'])\n",
    "AT = AT.drop_duplicates(subset=['ID'])\n",
    "\n",
    "# Perform the merge\n",
    "HR = HR.merge(hr_labels[['id', 'labels']], how='left', left_on='ID', right_on='id')\n",
    "HR.rename(columns={'labels': 'True_label'}, inplace=True)\n",
    "HR.drop(columns=['id'], inplace=True)\n",
    "\n",
    "GB = GB.merge(gb_labels[['id', 'labels']], how='left', left_on='ID', right_on='id')\n",
    "GB.rename(columns={'labels': 'True_label'}, inplace=True)\n",
    "GB.drop(columns=['id'], inplace=True)\n",
    "\n",
    "\n",
    "# Load the hdbscan dataframes\n",
    "GB_hdbscan = pd.read_pickle(r\"data folder\\GB\\GB_with_topics_hdbscan.pkl\")\n",
    "AT_hdbscan = pd.read_pickle(r\"data folder\\AT\\AT_with_topics_hdbscan.pkl\")\n",
    "HR_hdbscan = pd.read_pickle(r\"data folder\\HR\\HR_with_topics_hdbscan.pkl\")\n",
    "\n",
    "# Append my_topic from GB_hdbscan to GB as my_topic_hdbscan\n",
    "GB['my_topic_hdbscan'] = GB_hdbscan['my_topic']\n",
    "\n",
    "# Append my_topic_en and my_topic_native_language from HR_hdbscan to HR with _hdbscan suffix\n",
    "HR['my_topic_en_hdbscan'] = HR_hdbscan['my_topic_en']\n",
    "HR['my_topic_hr_hdbscan'] = HR_hdbscan['my_topic_native_language']\n",
    "\n",
    "AT['my_topic_en_hdbscan'] = AT_hdbscan['my_topic_en']\n",
    "AT['my_topic_de_hdbscan'] = AT_hdbscan['my_topic_native_language']\n",
    "\n",
    "# Rename columns for consistency\n",
    "HR.rename(columns={\n",
    "    'Segment_Category_HR_english': 'my_topic_en_gmm',\n",
    "    'Segment_Category_HR_croatian': 'my_topic_hr_gmm'\n",
    "}, inplace=True)\n",
    "\n",
    "AT.rename(columns={\n",
    "    'Segment_Category_AT_english': 'my_topic_en_gmm',\n",
    "    'Segment_Category_AT_german': 'my_topic_de_gmm'\n",
    "}, inplace=True)\n",
    "\n",
    "GB.rename(columns={\n",
    "    'Segment_Category_GB_english': 'my_topic_gmm'\n",
    "}, inplace=True)\n",
    "\n",
    "# HR: Determine topic consensus\n",
    "def determine_topic_consensus_hr(row):\n",
    "    topics = [\n",
    "        row['my_topic_en_gmm'],\n",
    "        row['my_topic_hr_gmm'],\n",
    "        row['my_topic_en_hdbscan'],\n",
    "        row['my_topic_hr_hdbscan']\n",
    "    ]\n",
    "    topics = [topic for topic in topics if topic != 'Other']  # Exclude \"Other\" from consideration\n",
    "\n",
    "    if row['Speaker_role'] == 'Chairperson':\n",
    "        # Chairperson: Only keep non-\"Other\" if all 4 columns agree\n",
    "        if len(set(topics)) == 1 and len(topics) == 4:\n",
    "            return topics[0]\n",
    "        return 'Other'\n",
    "    else:\n",
    "        # Non-chairperson: Determine majority topic\n",
    "        topic_counts = pd.Series(topics).value_counts()\n",
    "        if len(topic_counts) == 0:\n",
    "            return 'Other'  # Default to \"Other\" if no valid topics\n",
    "        if topic_counts.iloc[0] > 1:  # Majority exists\n",
    "            return topic_counts.idxmax()\n",
    "        if len(topic_counts) > 1 and topic_counts.iloc[0] == topic_counts.iloc[1]:\n",
    "            return 'Mix'  # Tie between topics\n",
    "        return topic_counts.idxmax()  # Single most common topic\n",
    "\n",
    "HR['topic_consensus'] = HR.apply(determine_topic_consensus_hr, axis=1)\n",
    "\n",
    "# AT: Determine topic consensus\n",
    "def determine_topic_consensus_at(row):\n",
    "    topics = [\n",
    "        row['my_topic_en_gmm'],\n",
    "        row['my_topic_de_gmm'],\n",
    "        row['my_topic_en_hdbscan'],\n",
    "        row['my_topic_de_hdbscan']\n",
    "    ]\n",
    "    topics = [topic for topic in topics if topic != 'Other']  # Exclude \"Other\" from consideration\n",
    "\n",
    "    if row['Speaker_role'] == 'Chairperson':\n",
    "        # Chairperson: Only keep non-\"Other\" if all 4 columns agree\n",
    "        if len(set(topics)) == 1 and len(topics) == 4:\n",
    "            return topics[0]\n",
    "        return 'Other'\n",
    "    else:\n",
    "        # Non-chairperson: Determine majority topic\n",
    "        topic_counts = pd.Series(topics).value_counts()\n",
    "        if len(topic_counts) == 0:\n",
    "            return 'Other'  # Default to \"Other\" if no valid topics\n",
    "        if topic_counts.iloc[0] > 1:  # Majority exists\n",
    "            return topic_counts.idxmax()\n",
    "        if len(topic_counts) > 1 and topic_counts.iloc[0] == topic_counts.iloc[1]:\n",
    "            return 'Mix'  # Tie between topics\n",
    "        return topic_counts.idxmax()  # Single most common topic\n",
    "\n",
    "AT['topic_consensus'] = AT.apply(determine_topic_consensus_at, axis=1)\n",
    "\n",
    "# GB: Determine topic consensus\n",
    "def determine_topic_consensus_gb(row):\n",
    "    topics = [row['my_topic_gmm'], row['my_topic_hdbscan']]\n",
    "    topics = [topic for topic in topics if topic != 'Other']  # Exclude \"Other\" from consideration\n",
    "\n",
    "    if row['Speaker_role'] == 'Chairperson':\n",
    "        # Chairperson: Use the topic if both columns agree\n",
    "        if len(set(topics)) == 1 and len(topics) > 0:\n",
    "            return topics[0]\n",
    "        return 'Other'\n",
    "    else:\n",
    "        # Non-chairperson: Prioritize agreement between columns\n",
    "        if len(set(topics)) == 1 and len(topics) > 0:\n",
    "            return topics[0]  # Both columns agree\n",
    "        if len(set(topics)) > 1:\n",
    "            return 'Mix'  # Columns differ\n",
    "        return 'Other'  # No valid topics\n",
    "\n",
    "GB['topic_consensus'] = GB.apply(determine_topic_consensus_gb, axis=1)\n",
    "\n",
    "HR_only_true = HR.dropna(subset=['True_label'])\n",
    "\n",
    "GB_only_true = GB.dropna(subset=['True_label'])\n",
    "\n",
    "# shapes\n",
    "AT.shape, HR.shape, GB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb5f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of sequence power\n",
    "\n",
    "#GB.loc[91:100, ['Text', 'Topic', 'topic_consensus', 'Speaker_role', 'S]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In full HR dataset when speaker isn't Chairperson % of parlaCAP model predictions 'Other' topic is: {100 * len(HR[(HR['Speaker_role'] != 'Chairperson') & (HR['Topic'] == 'Other')]) / len(HR[HR['Speaker_role'] != 'Chairperson']):.2f}%\")\n",
    "print(f\"In full HR dataset when speaker isn't Chairperson % of my predictions 'Other' topic is: {100 * len(HR[(HR['Speaker_role'] != 'Chairperson') & (HR['topic_consensus'] == 'Other')]) / len(HR[HR['Speaker_role'] != 'Chairperson']):.2f}%\")\n",
    "print(f\"In test HR dataset when speaker isn't Chairperson % of true label 'Other' topic is: {100 * len(HR_only_true[(HR_only_true['Speaker_role'] != 'Chairperson') & (HR_only_true['True_label'] == 'Other')]) / len(HR_only_true[HR_only_true['Speaker_role'] != 'Chairperson']):.2f}%\")\n",
    "print('##########################################')\n",
    "\n",
    "print(f\"In full GB dataset when speaker isn't Chairperson % of parlaCAP model predictions 'Other' topic is: {100 * len(GB[(GB['Speaker_role'] != 'Chairperson') & (GB['Topic'] == 'Other')]) / len(GB[GB['Speaker_role'] != 'Chairperson']):.2f}%\")\n",
    "print(f\"In full GB dataset when speaker isn't Chairperson % of my predictions 'Other' topic is: {100 * len(GB[(GB['Speaker_role'] != 'Chairperson') & (GB['topic_consensus'] == 'Other')]) / len(GB[GB['Speaker_role'] != 'Chairperson']):.2f}%\")\n",
    "print(f\"In test GB dataset when speaker isn't Chairperson % of true label 'Other' topic is: {100 * len(GB_only_true[(GB_only_true['Speaker_role'] != 'Chairperson') & (GB_only_true['True_label'] == 'Other')]) / len(GB_only_true[GB_only_true['Speaker_role'] != 'Chairperson']):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In full HR dataset when speaker is Chairperson % of parlaCAP model predictions 'Other' topic is: {100 * len(HR[(HR['Speaker_role'] == 'Chairperson') & (HR['Topic'] == 'Other')]) / len(HR[HR['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print(f\"In full HR dataset when speaker is Chairperson % of my predictions 'Other' topic is: {100 * len(HR[(HR['Speaker_role'] == 'Chairperson') & (HR['topic_consensus'] == 'Other')]) / len(HR[HR['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print(f\"In test HR dataset when speaker is Chairperson % of true label 'Other' topic is: {100 * len(HR_only_true[(HR_only_true['Speaker_role'] == 'Chairperson') & (HR_only_true['True_label'] == 'Other')]) / len(HR_only_true[HR_only_true['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print('##########################################')\n",
    "\n",
    "print(f\"In full GB dataset when speaker is Chairperson % of parlaCAP model predictions 'Other' topic is: {100 * len(GB[(GB['Speaker_role'] == 'Chairperson') & (GB['Topic'] == 'Other')]) / len(GB[GB['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print(f\"In full GB dataset when speaker is Chairperson % of my predictions 'Other' topic is: {100 * len(GB[(GB['Speaker_role'] == 'Chairperson') & (GB['topic_consensus'] == 'Other')]) / len(GB[GB['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print(f\"In test GB dataset when speaker is Chairperson % of true label 'Other' topic is: {100 * len(GB_only_true[(GB_only_true['Speaker_role'] == 'Chairperson') & (GB_only_true['True_label'] == 'Other')]) / len(GB_only_true[GB_only_true['Speaker_role'] == 'Chairperson']):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === POLICY-FOCUSED CONFUSION MATRIX (FLEXIBLE SAMPLING) ===\n",
    "\n",
    "def analyze_labels(df, evaluation_topic_col, true_label_topic_column,\n",
    "                   sample_size='all', min_word_count=None, max_word_count=None, \n",
    "                   exclude_roles=None, exclude_topics=None, \n",
    "                   text_column='Text', plot_size=(16, 12), dataset_name=\"Dataset\"):\n",
    "\n",
    "    if exclude_roles is None:\n",
    "        exclude_roles = []\n",
    "    if exclude_topics is None:\n",
    "        exclude_topics = []\n",
    "\n",
    "    # Remove invalid rows\n",
    "    df = df[~df[true_label_topic_column].isin(['-'])]\n",
    "    df = df.dropna(subset=[evaluation_topic_col, true_label_topic_column])\n",
    "\n",
    "    # Filter out specified speaker roles\n",
    "    if exclude_roles:\n",
    "        df = df[~df['Speaker_role'].isin(exclude_roles)]\n",
    "\n",
    "    # Remove specified topic labels\n",
    "    if exclude_topics:\n",
    "        for col in [evaluation_topic_col, true_label_topic_column]:\n",
    "            df = df[~df[col].isin(exclude_topics)]\n",
    "\n",
    "    # Filter rows based on word count\n",
    "    if min_word_count is not None:\n",
    "        df = df[df[text_column].apply(lambda x: len(str(x).split())) >= min_word_count]\n",
    "    if max_word_count is not None:\n",
    "        df = df[df[text_column].apply(lambda x: len(str(x).split())) <= max_word_count]\n",
    "\n",
    "    # Flexible sampling per CAP label\n",
    "    if sample_size != 'all':\n",
    "        sampled_dfs = []\n",
    "        for category in df[true_label_topic_column].unique():\n",
    "            category_df = df[df[true_label_topic_column] == category]\n",
    "            if len(category_df) >= sample_size:\n",
    "                sampled_dfs.append(category_df.sample(n=sample_size))\n",
    "            else:\n",
    "                sampled_dfs.append(category_df)\n",
    "        df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "    # Calculate metrics\n",
    "    my_topics = df[evaluation_topic_col]\n",
    "    parlacap_topics = df[true_label_topic_column]\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1_weighted = f1_score(parlacap_topics, my_topics, average='weighted', zero_division=0)\n",
    "    f1_macro = f1_score(parlacap_topics, my_topics, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(parlacap_topics, my_topics, average='micro', zero_division=0)\n",
    "    exact_matches = (my_topics == parlacap_topics).sum()\n",
    "    accuracy_result = exact_matches / len(df)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    plt.figure(figsize=plot_size)\n",
    "    categories = parlacap_topics.value_counts().index.tolist()\n",
    "    conf_matrix = confusion_matrix(parlacap_topics, my_topics, labels=categories)\n",
    "    sns.heatmap(conf_matrix, xticklabels=categories, yticklabels=categories, annot=True, fmt='d',\n",
    "                cmap='Blues', cbar_kws={'label': 'Count'}, square=True, annot_kws={'size': 8},\n",
    "                linewidths=0.5, linecolor='white')\n",
    "    plt.title(f'Confusion Matrix for {dataset_name}, f1 scores: macro={f1_macro:.2f}, micro={f1_micro:.2f}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('ParlaCAP Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'accuracy': accuracy_result,\n",
    "        'exact_matches': exact_matches,\n",
    "        'total_speeches': len(df),\n",
    "        'categories_count': len(categories),\n",
    "        'sample_size': sample_size,\n",
    "        'min_word_count': min_word_count,\n",
    "        'max_word_count': max_word_count,\n",
    "        'exclude_roles': exclude_roles,\n",
    "        'exclude_topics': exclude_topics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119428bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results compared to human labels\n",
    "\n",
    "results_all = {}\n",
    "\n",
    "results_all['GB'] = analyze_labels(\n",
    "    GB, 'topic_consensus', 'True_label',\n",
    "    exclude_topics=['Mix'],\n",
    "    dataset_name=\"GB\"\n",
    ")\n",
    "\n",
    "results_all['HR'] = analyze_labels(\n",
    "    HR, 'topic_consensus', 'True_label',\n",
    "    exclude_topics=['Mix'],\n",
    "    dataset_name=\"HR\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results compared to parlacap\n",
    "\n",
    "results_all = {}\n",
    "\n",
    "results_all['GB'] = analyze_labels(\n",
    "    GB, 'topic_consensus', 'Topic',\n",
    "    #exclude_roles=['Chairperson'],\n",
    "    exclude_topics=['Mix'],\n",
    "    min_word_count=70,\n",
    "    dataset_name=\"GB\"\n",
    ")\n",
    "\n",
    "results_all['HR'] = analyze_labels(\n",
    "    HR, 'topic_consensus', 'Topic',\n",
    "    #exclude_roles=['Chairperson'],\n",
    "    exclude_topics=['Mix'],\n",
    "    min_word_count=70,\n",
    "    dataset_name=\"HR\",\n",
    ")\n",
    "\n",
    "results_all['AT'] = analyze_labels(\n",
    "    AT, 'topic_consensus', 'Topic',\n",
    "    #exclude_roles=['Chairperson'],\n",
    "    exclude_topics=['Mix'],\n",
    "    min_word_count=70,\n",
    "    dataset_name=\"AT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fed66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speech length and filter middle 50%\n",
    "HR['speech_length'] = HR['Text'].apply(lambda x: len(str(x).split()))\n",
    "HR_filtered = HR[HR['speech_length'].between(HR['speech_length'].quantile(0.25), HR['speech_length'].quantile(0.75))]\n",
    "\n",
    "# Filter rows with true labels\n",
    "HR_only_true = HR.dropna(subset=['True_label'])\n",
    "\n",
    "# Total rows with true labels\n",
    "print(f\"Total human annotated rows: {HR_only_true.shape[0]}\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "# Proportion of 'Other' topics\n",
    "print(f\"Percentage of topic 'Other' in entire dataset: {100 * len(HR[HR['Topic'] == 'Other']) / len(HR):.2f}%\")\n",
    "print(f\"Percentage of topic 'Other' in filtered dataset (middle 50% of speeches): {100 * len(HR_filtered[HR_filtered['Topic'] == 'Other']) / len(HR_filtered):.2f}%\")\n",
    "print(f\"Percentage of topic 'Other' in test dataset: {100 * len(HR_only_true[HR_only_true['Topic'] == 'Other']) / len(HR_only_true):.2f}%\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "# Proportion of 'Chairperson' role\n",
    "print(f\"Percentage of 'Chairperson' in entire dataset: {100 * len(HR[HR['Speaker_role'] == 'Chairperson']) / len(HR):.2f}%\")\n",
    "print(f\"Percentage of 'Chairperson' in filtered dataset (middle 50% of speeches): {100 * len(HR_filtered[HR_filtered['Speaker_role'] == 'Chairperson']) / len(HR_filtered):.2f}%\")\n",
    "print(f\"Percentage of 'Chairperson' in test dataset: {100 * len(HR_only_true[HR_only_true['Speaker_role'] == 'Chairperson']) / len(HR_only_true):.2f}%\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "# Proportion of model 'Other' topics for Chairperson role\n",
    "print(f\"In test dataset when role is Chairperson % of model predictions 'Other' topic is: {100 * len(HR_only_true[(HR_only_true['Speaker_role'] == 'Chairperson') & (HR_only_true['Topic'] == 'Other')]) / len(HR_only_true[HR_only_true['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print(f\"In filtered dataset when role is Chairperson % of model predictions 'Other' topic is: {100 * len(HR_filtered[(HR_filtered['Speaker_role'] == 'Chairperson') & (HR_filtered['Topic'] == 'Other')]) / len(HR_filtered[HR_filtered['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print(f\"In full dataset when role is Chairperson % of model predictions 'Other' topic is: {100 * len(HR[(HR['Speaker_role'] == 'Chairperson') & (HR['Topic'] == 'Other')]) / len(HR[HR['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "# Percentage of true label 'Other' topics for Chairperson role\n",
    "print(f\"Percentage of true label 'Other' for 'Chairperson' in test dataset: {100 * len(HR_only_true[(HR_only_true['Speaker_role'] == 'Chairperson') & (HR_only_true['True_label'] == 'Other')]) / len(HR_only_true[HR_only_true['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print(f\"Percentage of model predicted label 'Other' for 'Chairperson' in test dataset: {100 * len(HR_only_true[(HR_only_true['Speaker_role'] == 'Chairperson') & (HR_only_true['Topic'] == 'Other')]) / len(HR_only_true[HR_only_true['Speaker_role'] == 'Chairperson']):.2f}%\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "# Percentage of role when topic is Other in test dataset\n",
    "print(f\"Percentage of 'Chairperson' role when true label is 'Other' in test dataset: {100 * len(HR_only_true[(HR_only_true['True_label'] == 'Other') & (HR_only_true['Speaker_role'] == 'Chairperson')]) / len(HR_only_true[HR_only_true['True_label'] == 'Other']):.2f}%\")\n",
    "print(f\"Percentage of 'Chairperson' role when model label is 'Other' in test dataset: {100 * len(HR_filtered[(HR_filtered['Topic'] == 'Other') & (HR_filtered['Speaker_role'] == 'Chairperson')]) / len(HR_filtered[HR_filtered['Topic'] == 'Other']):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e746fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_topic_distributions(df, our_col, parlacap_col, dataset_name):\n",
    "    \"\"\"Compare distribution of our topics vs ParlaCAP topics\"\"\"\n",
    "    \n",
    "    # Remove missing data\n",
    "    clean_df = df.dropna(subset=[our_col, parlacap_col])\n",
    "    \n",
    "    # Get distributions\n",
    "    our_dist = clean_df[our_col].value_counts(normalize=True) * 100\n",
    "    parlacap_dist = clean_df[parlacap_col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    # Combine into comparison DataFrame\n",
    "    comparison = pd.DataFrame({\n",
    "        'Our_Classification': our_dist,\n",
    "        'Human labels': parlacap_dist\n",
    "    }).fillna(0)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Side-by-side comparison\n",
    "    comparison.plot(kind='barh', ax=ax1, width=0.8, alpha=0.8)\n",
    "    ax1.set_title(f'{dataset_name}: Topic Distribution Comparison')\n",
    "    ax1.set_xlabel('Percentage (%)')\n",
    "    ax1.set_ylabel('Topics')\n",
    "    ax1.legend(title='Source')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Difference plot\n",
    "    diff = comparison['Our_Classification'] - comparison['Human labels']\n",
    "    colors = ['red' if x < 0 else 'green' for x in diff]\n",
    "    diff.plot(kind='barh', ax=ax2, color=colors, alpha=0.7)\n",
    "    ax2.set_title(f'{dataset_name}: Difference (Our - Human labels)')\n",
    "    ax2.set_xlabel('Percentage Point Difference')\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top differences\n",
    "    print(f\"\\n📈 {dataset_name} - Largest Distribution Differences:\")\n",
    "    top_diffs = diff.abs().nlargest(5)\n",
    "    for topic, abs_diff in top_diffs.items():\n",
    "        actual_diff = diff[topic]\n",
    "        direction = \"over-represented\" if actual_diff > 0 else \"under-represented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_topic_distributions(GB, 'topic_consensus', 'Topic', 'GB')\n",
    "\n",
    "compare_topic_distributions(HR, 'topic_consensus', 'Topic', 'HR')\n",
    "\n",
    "compare_topic_distributions(AT, 'topic_consensus', 'Topic', 'AT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_topic_distributions(GB_only_true, 'topic_consensus', 'True_label', 'GB')\n",
    "\n",
    "compare_topic_distributions(HR_only_true, 'topic_consensus', 'True_label', 'HR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77281a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets with consensus topics\n",
    "GB.to_pickle(r\"data folder\\GB\\GB_final.pkl\")\n",
    "AT.to_pickle(r\"data folder\\AT\\AT_final.pkl\")\n",
    "HR.to_pickle(r\"data folder\\HR\\HR_final.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
