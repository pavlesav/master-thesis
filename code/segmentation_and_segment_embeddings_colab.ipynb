{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e523e35",
   "metadata": {},
   "source": [
    "# Parliamentary Speech Segmentation & Segment Embeddings\n",
    "\n",
    "This notebook implements the second stage of parliamentary speech processing:\n",
    "\n",
    "1. **Setup & Configuration** - Google Colab setup and imports\n",
    "2. **Data Loading** - Load datasets with pre-computed speech embeddings\n",
    "3. **Segmentation** - Parliament-aware speech segmentation using embeddings and keywords\n",
    "4. **Segment Embeddings** - Generate embeddings for concatenated segment texts\n",
    "5. **Final Processing** - Save complete datasets ready for topic modeling\n",
    "\n",
    "## Key Features:\n",
    "- **Smart Segmentation**: Agenda-aware boundary detection with parliament-specific keywords\n",
    "- **Multi-parliament support**: Austrian, Croatian, British parliaments with optimized settings\n",
    "- **Segment Embeddings**: Concatenate speeches within segments and embed the combined text\n",
    "- **GPU optimization**: Efficient processing with checkpointing\n",
    "\n",
    "## Input:\n",
    "Requires datasets with speech embeddings from the first processing stage.\n",
    "\n",
    "## Output:\n",
    "Complete datasets with both speech and segment embeddings, ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed047bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GOOGLE COLAB SETUP ===\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install sentence-transformers bertopic umap-learn hdbscan tqdm openai python-dotenv scipy\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# GPU optimization and batch size configuration\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {gpu_name} | Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Dynamic batch size based on GPU type\n",
    "    if 'A100' in gpu_name:\n",
    "        DEFAULT_BATCH_SIZE = 124  # A100 optimized\n",
    "        print(f\"üöÄ A100 detected: Using optimized batch size {DEFAULT_BATCH_SIZE}\")\n",
    "    elif 'V100' in gpu_name or 'T4' in gpu_name:\n",
    "        DEFAULT_BATCH_SIZE = 64   # V100/T4 optimized\n",
    "        print(f\"‚ö° {gpu_name} detected: Using batch size {DEFAULT_BATCH_SIZE}\")\n",
    "    else:\n",
    "        DEFAULT_BATCH_SIZE = 32   # Conservative default\n",
    "        print(f\"üîß Generic GPU detected: Using conservative batch size {DEFAULT_BATCH_SIZE}\")\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    print(\"No GPU detected - will use CPU (slower)\")\n",
    "    DEFAULT_BATCH_SIZE = 8  # CPU batch size\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"‚úÖ Setup complete! Default batch size: {DEFAULT_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a82103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ENHANCED CONFIGURATION ===\n",
    "data_folder = '/content/drive/MyDrive/thesis data/'\n",
    "\n",
    "# Enhanced Parliament and Language configurations with keywords\n",
    "PARLIAMENT_CONFIG = {\n",
    "    'austrian': {\n",
    "        'english': {\n",
    "            'file': 'AT_en.pkl',\n",
    "            'chairperson_role': 'Chairperson',\n",
    "            'agenda_keywords': {\n",
    "                'strong': ['agenda item'],\n",
    "                'medium': ['agenda'],\n",
    "                'weak': ['item', 'point']\n",
    "            }\n",
    "        },\n",
    "        'german': {\n",
    "            'file': 'AT_german.pkl', \n",
    "            'chairperson_role': 'Pr√§sidentIn',\n",
    "            'agenda_keywords': {\n",
    "                'strong': ['tagesordnungspunkt', 'punkt der tagesordnung'],\n",
    "                'medium': ['tagesordnung', 'verhandlung'],\n",
    "                'weak': ['behandlung']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'croatian': {\n",
    "        'english': {\n",
    "            'file': 'CRO_en.pkl',\n",
    "            'chairperson_role': 'Chairperson',\n",
    "            'agenda_keywords': {\n",
    "                'strong': ['agenda item'],\n",
    "                'medium': ['agenda'],\n",
    "                'weak': ['item', 'point']\n",
    "            }\n",
    "        },\n",
    "        'croatian': {\n",
    "            'file': 'CRO_hr.pkl',\n",
    "            'chairperson_role': 'Predsjedavajuƒái',\n",
    "            'agenda_keywords': {\n",
    "                # Based on segmentation_analysis findings - optimized pattern\n",
    "                'strong': ['rijeƒç|sljedeƒái|toƒçka|prelazimo|glasovanje'],\n",
    "                'medium': ['hvala|molim|sada|nastavljamo'],\n",
    "                'weak': ['otvaramo|zatvaramo|poƒçinje|red']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'british': {\n",
    "        'english': {\n",
    "            'file': 'GB_en.pkl',\n",
    "            'chairperson_role': 'Speaker',\n",
    "            'agenda_keywords': {\n",
    "                # British parliamentary terminology\n",
    "                'strong': ['order paper|business of the house|next business'],\n",
    "                'medium': ['honourable member|right honourable|speaker|chair'],\n",
    "                'weak': ['question|division|debate|motion']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def list_available_options():\n",
    "    \"\"\"Display available parliament and language options.\"\"\"\n",
    "    print(\"üìã Available Processing Options:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for parliament, languages in PARLIAMENT_CONFIG.items():\n",
    "        print(f\"\\nüèõÔ∏è {parliament.upper()} Parliament:\")\n",
    "        for language, config in languages.items():\n",
    "            print(f\"  ‚Ä¢ {language.capitalize()}: {config['file']}\")\n",
    "            print(f\"    - Chairperson role: '{config['chairperson_role']}'\")\n",
    "            print(f\"    - Keywords: {len(config['agenda_keywords']['strong']) + len(config['agenda_keywords']['medium'])} patterns\")\n",
    "\n",
    "def get_config(parliament, language):\n",
    "    \"\"\"Get configuration for specific parliament and language combination.\"\"\"\n",
    "    if parliament not in PARLIAMENT_CONFIG:\n",
    "        raise ValueError(f\"Parliament '{parliament}' not supported. Available: {list(PARLIAMENT_CONFIG.keys())}\")\n",
    "    \n",
    "    if language not in PARLIAMENT_CONFIG[parliament]:\n",
    "        available_langs = list(PARLIAMENT_CONFIG[parliament].keys())\n",
    "        raise ValueError(f\"Language '{language}' not available for {parliament} parliament. Available: {available_langs}\")\n",
    "    \n",
    "    return PARLIAMENT_CONFIG[parliament][language]\n",
    "\n",
    "# Display available options\n",
    "list_available_options()\n",
    "\n",
    "print(f\"\\nüîß Usage examples:\")\n",
    "print(f\"  ‚Ä¢ Austrian Parliament in German: parliament='austrian', language='german'\")\n",
    "print(f\"  ‚Ä¢ Croatian Parliament in Croatian: parliament='croatian', language='croatian'\")\n",
    "print(f\"  ‚Ä¢ British Parliament in English: parliament='british', language='english'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA LOADING FUNCTIONS ===\n",
    "\n",
    "def load_speech_embeddings_data(parliament, language):\n",
    "    \"\"\"Load dataset with pre-computed speech embeddings.\"\"\"\n",
    "    # Define input path from speech embeddings stage\n",
    "    input_path = f\"{data_folder}{parliament}_{language}_with_speech_embeddings.pkl\"\n",
    "    \n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"Speech embeddings file not found: {input_path}\")\n",
    "    \n",
    "    df = pd.read_pickle(input_path)\n",
    "    print(f\"‚úÖ Loaded {parliament} parliament ({language}) with speech embeddings: {df.shape}\")\n",
    "    \n",
    "    # Verify speech embeddings exist\n",
    "    if 'Speech_Embeddings' not in df.columns:\n",
    "        raise ValueError(f\"Speech_Embeddings column not found in {input_path}\")\n",
    "    \n",
    "    # Verify required columns\n",
    "    if 'Text_ID' not in df.columns:\n",
    "        raise ValueError(f\"Text_ID column not found in {input_path}\")\n",
    "    \n",
    "    config = get_config(parliament, language)\n",
    "    if 'Speaker_role' in df.columns:\n",
    "        role_counts = df['Speaker_role'].value_counts()\n",
    "        if config['chairperson_role'] in role_counts.index:\n",
    "            print(f\"‚úÖ Found '{config['chairperson_role']}': {role_counts[config['chairperson_role']]:,} speeches\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è '{config['chairperson_role']}' not found in Speaker_role\")\n",
    "    \n",
    "    print(f\"üìä {len(df):,} speeches across {df['Text_ID'].nunique():,} sessions\")\n",
    "    print(f\"üî¢ Speech embedding shape: {df['Speech_Embeddings'][0].shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def verify_speech_embeddings_data(parliament, language):\n",
    "    \"\"\"Verify that speech embeddings data exists and is valid.\"\"\"\n",
    "    input_path = f\"{data_folder}{parliament}_{language}_with_speech_embeddings.pkl\"\n",
    "    \n",
    "    print(f\"üìä Verifying Speech Embeddings Data:\")\n",
    "    print(f\"Path: {input_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"‚ùå File not found: {input_path}\")\n",
    "        print(f\"üí° Please run the speech embeddings processing pipeline first.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_pickle(input_path)\n",
    "        print(f\"‚úÖ File exists and loadable: {df.shape}\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = ['Text_ID', 'Text', 'Speech_Embeddings']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"‚úÖ All required columns present\")\n",
    "        print(f\"üî¢ Speech embeddings shape: {df['Speech_Embeddings'][0].shape}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"üîç Data Verification Examples:\")\n",
    "print(\"# verify_speech_embeddings_data('croatian', 'croatian')\")\n",
    "print(\"# verify_speech_embeddings_data('austrian', 'english')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95499434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEGMENTATION FUNCTIONS ===\n",
    "\n",
    "def segment_speeches(df, parliament, language, window_size=5, min_segment_size=3):\n",
    "    \"\"\"Enhanced parliament-aware segmentation using speech embeddings and keywords.\"\"\"\n",
    "    config = get_config(parliament, language)\n",
    "    \n",
    "    print(f\"üèõÔ∏è {parliament.upper()} Parliament ({language.upper()}) - Enhanced Segmentation\")\n",
    "    print(f\"üîç Chairperson: '{config['chairperson_role']}'\")\n",
    "    print(f\"üîß Keywords: {config['agenda_keywords']}\")\n",
    "    \n",
    "    segment_ids = []\n",
    "    segmentation_metrics = []\n",
    "    sitting_column = 'Text_ID'\n",
    "    \n",
    "    # Get unique sittings for progress tracking\n",
    "    unique_sittings = df[sitting_column].unique()\n",
    "    print(f\"üîÑ Processing {len(unique_sittings)} sessions...\")\n",
    "\n",
    "    for sitting_id in tqdm(unique_sittings, desc=f\"Segmenting {parliament} {language}\", unit=\"session\"):\n",
    "        group = df[df[sitting_column] == sitting_id]\n",
    "        sitting_length = len(group)\n",
    "\n",
    "        if sitting_length < min_segment_size:\n",
    "            # Very small sitting - one segment\n",
    "            sitting_segments = [f\"{sitting_id}_seg_0\"] * len(group)\n",
    "            segment_ids.extend(sitting_segments)\n",
    "            segmentation_metrics.append({\n",
    "                'sitting_id': sitting_id,\n",
    "                'sitting_length': sitting_length,\n",
    "                'num_segments': 1,\n",
    "                'avg_segment_size': sitting_length,\n",
    "                'boundaries_found': 0,\n",
    "                'agenda_boundaries': 0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        embeddings = np.array(group['Speech_Embeddings'].tolist())\n",
    "\n",
    "        # Parliament-specific target segments\n",
    "        if parliament == 'british':\n",
    "            # British Parliament tends to have longer sessions\n",
    "            if sitting_length < 40:\n",
    "                target_segments = max(2, min(6, sitting_length // 10))\n",
    "                threshold_percentile = 40\n",
    "            elif sitting_length < 150:\n",
    "                target_segments = max(4, min(15, sitting_length // 15))\n",
    "                threshold_percentile = 45\n",
    "            else:\n",
    "                target_segments = max(6, min(25, sitting_length // 20))\n",
    "                threshold_percentile = 50\n",
    "        else:\n",
    "            # Austrian/Croatian logic\n",
    "            if sitting_length < 30:\n",
    "                target_segments = max(2, min(5, sitting_length // 8))\n",
    "                threshold_percentile = 35\n",
    "            elif sitting_length < 100:\n",
    "                target_segments = max(3, min(12, sitting_length // 12))\n",
    "                threshold_percentile = 40\n",
    "            else:\n",
    "                target_segments = max(5, min(20, sitting_length // 15))\n",
    "                threshold_percentile = 45\n",
    "\n",
    "        # === ENHANCED CHAIRPERSON AGENDA DETECTION ===\n",
    "        agenda_boundaries = set()\n",
    "        agenda_signals = []\n",
    "\n",
    "        for i, (idx, row) in enumerate(group.iterrows()):\n",
    "            agenda_score = 0\n",
    "\n",
    "            # Check for parliament-specific chairperson role\n",
    "            if 'Speaker_role' in row and pd.notna(row['Speaker_role']) and row['Speaker_role'] == config['chairperson_role']:\n",
    "                text = str(row['Text']).lower()\n",
    "\n",
    "                # Enhanced keyword matching with regex support\n",
    "                for keyword_list in config['agenda_keywords']['strong']:\n",
    "                    keywords = keyword_list.split('|') if '|' in keyword_list else [keyword_list]\n",
    "                    if any(keyword in text for keyword in keywords):\n",
    "                        agenda_score = 1.0\n",
    "                        break\n",
    "                \n",
    "                if agenda_score == 0:  # Check medium keywords\n",
    "                    for keyword_list in config['agenda_keywords']['medium']:\n",
    "                        keywords = keyword_list.split('|') if '|' in keyword_list else [keyword_list]\n",
    "                        if any(keyword in text for keyword in keywords):\n",
    "                            agenda_score = 0.7\n",
    "                            break\n",
    "                \n",
    "                if agenda_score == 0:  # Check weak keywords\n",
    "                    for keyword_list in config['agenda_keywords']['weak']:\n",
    "                        keywords = keyword_list.split('|') if '|' in keyword_list else [keyword_list]\n",
    "                        if any(keyword in text for keyword in keywords):\n",
    "                            agenda_score = 0.5\n",
    "                            break\n",
    "                \n",
    "                # Special handling for session start\n",
    "                if agenda_score == 0 and i == 0:\n",
    "                    agenda_score = 0.3\n",
    "\n",
    "            agenda_signals.append(agenda_score)\n",
    "\n",
    "            # Add strong agenda boundaries\n",
    "            if agenda_score >= 0.7 and i >= min_segment_size and (sitting_length - i) >= min_segment_size:\n",
    "                agenda_boundaries.add(i)\n",
    "\n",
    "        # === SIMILARITY ANALYSIS ===\n",
    "        similarity_signals = {}\n",
    "\n",
    "        # Primary windowed similarity\n",
    "        similarities = []\n",
    "        for i in range(len(embeddings) - window_size):\n",
    "            window1 = np.mean(embeddings[i:i + window_size], axis=0)\n",
    "            window2 = np.mean(embeddings[i + window_size:i + 2*window_size], axis=0)\n",
    "            sim = cosine_similarity(window1.reshape(1, -1), window2.reshape(1, -1))[0][0]\n",
    "            similarities.append(sim)\n",
    "\n",
    "        similarity_signals['primary'] = np.array(similarities)\n",
    "\n",
    "        # Point-to-point similarity\n",
    "        if len(embeddings) > 6:\n",
    "            point_sims = []\n",
    "            for i in range(len(embeddings) - 1):\n",
    "                sim = cosine_similarity(\n",
    "                    embeddings[i].reshape(1, -1),\n",
    "                    embeddings[i + 1].reshape(1, -1)\n",
    "                )[0][0]\n",
    "                point_sims.append(sim)\n",
    "\n",
    "            # Align with primary signal\n",
    "            point_sims = np.array(point_sims)\n",
    "            if len(point_sims) > len(similarities):\n",
    "                point_sims = point_sims[:len(similarities)]\n",
    "            elif len(point_sims) < len(similarities):\n",
    "                padding = len(similarities) - len(point_sims)\n",
    "                point_sims = np.pad(point_sims, (0, padding), mode='edge')\n",
    "\n",
    "            similarity_signals['point'] = point_sims\n",
    "\n",
    "        # Gradient-based change detection\n",
    "        if len(embeddings) > 10:\n",
    "            trajectory = []\n",
    "            for i in range(1, len(embeddings)):\n",
    "                displacement = np.linalg.norm(embeddings[i] - embeddings[i-1])\n",
    "                trajectory.append(float(displacement))\n",
    "\n",
    "            trajectory = np.array(trajectory, dtype=np.float64)\n",
    "            if len(trajectory) > 3:\n",
    "                try:\n",
    "                    from scipy.ndimage import uniform_filter1d\n",
    "                    smoothed = uniform_filter1d(trajectory.astype(np.float64), size=3)\n",
    "                    gradient = np.gradient(smoothed)\n",
    "\n",
    "                    # Align with similarities\n",
    "                    if len(gradient) > len(similarities):\n",
    "                        gradient = gradient[:len(similarities)]\n",
    "                    elif len(gradient) < len(similarities):\n",
    "                        padding = len(similarities) - len(gradient)\n",
    "                        gradient = np.pad(gradient, (0, padding), mode='edge')\n",
    "\n",
    "                    similarity_signals['gradient'] = gradient\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        if len(similarity_signals['primary']) == 0:\n",
    "            sitting_segments = [f\"{sitting_id}_seg_0\"] * len(group)\n",
    "            segment_ids.extend(sitting_segments)\n",
    "            segmentation_metrics.append({\n",
    "                'sitting_id': sitting_id,\n",
    "                'sitting_length': sitting_length,\n",
    "                'num_segments': 1,\n",
    "                'avg_segment_size': sitting_length,\n",
    "                'boundaries_found': 0,\n",
    "                'agenda_boundaries': 0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # === BOUNDARY DETECTION ===\n",
    "        candidate_boundaries = set()\n",
    "\n",
    "        # 1. Add agenda boundaries (highest priority)\n",
    "        candidate_boundaries.update(agenda_boundaries)\n",
    "\n",
    "        # 2. Find boundaries from primary similarity drops\n",
    "        primary_sims = similarity_signals['primary']\n",
    "        threshold = np.percentile(primary_sims, threshold_percentile)\n",
    "\n",
    "        for i in range(len(primary_sims)):\n",
    "            if (primary_sims[i] < threshold and\n",
    "                i >= min_segment_size and\n",
    "                (len(group) - i - window_size) >= min_segment_size):\n",
    "                candidate_boundaries.add(i + window_size)\n",
    "\n",
    "        # 3. Add from point-to-point analysis\n",
    "        if 'point' in similarity_signals:\n",
    "            point_threshold = np.percentile(similarity_signals['point'], threshold_percentile - 10)\n",
    "            for i in range(len(similarity_signals['point'])):\n",
    "                if (similarity_signals['point'][i] < point_threshold and\n",
    "                    i >= min_segment_size and\n",
    "                    (len(group) - i) >= min_segment_size):\n",
    "                    candidate_boundaries.add(i)\n",
    "\n",
    "        # 4. Add from gradient analysis\n",
    "        if 'gradient' in similarity_signals:\n",
    "            gradient = similarity_signals['gradient']\n",
    "            gradient_threshold = np.percentile(np.abs(gradient), 75)\n",
    "            for i in range(len(gradient)):\n",
    "                if (np.abs(gradient[i]) > gradient_threshold and\n",
    "                    i >= min_segment_size and\n",
    "                    (len(group) - i) >= min_segment_size):\n",
    "                    candidate_boundaries.add(i)\n",
    "\n",
    "        candidates = sorted(list(candidate_boundaries))\n",
    "\n",
    "        # === BOUNDARY SELECTION ===\n",
    "        boundaries = []\n",
    "        if candidates:\n",
    "            if len(candidates) <= target_segments - 1:\n",
    "                boundaries = candidates\n",
    "            else:\n",
    "                # Score candidates with parliament-specific agenda boost\n",
    "                candidate_scores = []\n",
    "                for c in candidates:\n",
    "                    score = 0\n",
    "\n",
    "                    # Parliament-specific agenda boost\n",
    "                    if c < len(agenda_signals):\n",
    "                        if parliament == 'british':\n",
    "                            agenda_boost = 4.0\n",
    "                        elif parliament == 'croatian':\n",
    "                            agenda_boost = 5.0\n",
    "                        else:\n",
    "                            agenda_boost = 3.0\n",
    "                        score += agenda_signals[c] * agenda_boost\n",
    "\n",
    "                    # Similarity scores\n",
    "                    if c - window_size >= 0 and c - window_size < len(primary_sims):\n",
    "                        score += (1 - primary_sims[c - window_size]) * 2.0\n",
    "\n",
    "                    if 'point' in similarity_signals and c < len(similarity_signals['point']):\n",
    "                        score += (1 - similarity_signals['point'][c]) * 1.5\n",
    "\n",
    "                    if 'gradient' in similarity_signals and c < len(similarity_signals['gradient']):\n",
    "                        score += np.abs(similarity_signals['gradient'][c]) * 1.0\n",
    "\n",
    "                    candidate_scores.append((c, score))\n",
    "\n",
    "                # Select top scoring boundaries\n",
    "                candidate_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                boundaries = sorted([c for c, _ in candidate_scores[:target_segments-1]])\n",
    "\n",
    "        # === BOUNDARY VALIDATION ===\n",
    "        validated_boundaries = []\n",
    "        for boundary in boundaries:\n",
    "            if not validated_boundaries or (boundary - validated_boundaries[-1]) >= min_segment_size:\n",
    "                validated_boundaries.append(boundary)\n",
    "\n",
    "        boundaries = validated_boundaries\n",
    "\n",
    "        # Assign segment IDs\n",
    "        current_segment = 0\n",
    "        sitting_segments = []\n",
    "\n",
    "        for i in range(len(group)):\n",
    "            if i > 0 and (i - 1) in boundaries:\n",
    "                current_segment += 1\n",
    "            sitting_segments.append(f\"{sitting_id}_seg_{current_segment}\")\n",
    "\n",
    "        segment_ids.extend(sitting_segments)\n",
    "\n",
    "        # Store metrics\n",
    "        num_segments = len(set(sitting_segments))\n",
    "        agenda_bound_count = len([b for b in boundaries if b in agenda_boundaries])\n",
    "\n",
    "        segmentation_metrics.append({\n",
    "            'sitting_id': sitting_id,\n",
    "            'sitting_length': sitting_length,\n",
    "            'num_segments': num_segments,\n",
    "            'avg_segment_size': sitting_length / num_segments,\n",
    "            'boundaries_found': len(boundaries),\n",
    "            'agenda_boundaries': agenda_bound_count,\n",
    "            'target_segments': target_segments,\n",
    "            'candidate_boundaries': len(candidates),\n",
    "            'signals_used': len(similarity_signals) + 1,\n",
    "            'parliament': parliament,\n",
    "            'language': language\n",
    "        })\n",
    "\n",
    "    # At the end, add Segment_ID column to the dataframe\n",
    "    df['Segment_ID'] = segment_ids\n",
    "    return df, segmentation_metrics\n",
    "\n",
    "# === ADD SEGMENT_ID UTILITY FUNCTION ===\n",
    "\n",
    "def add_segment_id_to_dataframe(df, parliament, language):\n",
    "    \"\"\"Add Segment_ID column to existing dataframe.\"\"\"\n",
    "    print(f\"üîß Adding Segment_ID to {parliament} {language} dataframe...\")\n",
    "    \n",
    "    df_with_segments, metrics = segment_speeches(df, parliament, language)\n",
    "    \n",
    "    print(f\"‚úÖ Added Segment_ID column: {df_with_segments['Segment_ID'].nunique():,} unique segments\")\n",
    "    print(f\"üìä Average speeches per segment: {len(df_with_segments) / df_with_segments['Segment_ID'].nunique():.1f}\")\n",
    "    \n",
    "    return df_with_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02204973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEGMENT EMBEDDINGS FUNCTIONS ===\n",
    "\n",
    "def embed_long_text(text, model, tokenizer):\n",
    "    \"\"\"Handle texts longer than model max length.\"\"\"\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    starts = list(range(0, len(token_ids), 4096 - 1024))\n",
    "    for start in starts:\n",
    "        end = min(start + 4096, len(token_ids))\n",
    "        chunk_ids = token_ids[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    chunk_embeddings = model.encode(chunks, batch_size=32, convert_to_tensor=False, show_progress_bar=False)\n",
    "    return np.mean(chunk_embeddings, axis=0)\n",
    "\n",
    "def generate_segment_embeddings(df, text_column='Text', segment_id_column='Segment_ID', batch_size=None):\n",
    "    \"\"\"Generate embeddings for concatenated segment texts.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = DEFAULT_BATCH_SIZE\n",
    "        \n",
    "    print(\"=\" * 60)\n",
    "    print(\"SEGMENT EMBEDDINGS: Concatenated segment texts\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Using batch size: {batch_size}\")\n",
    "    \n",
    "    # Create segment texts by concatenating speeches within each segment\n",
    "    segment_texts = []\n",
    "    segment_ids = []\n",
    "    \n",
    "    for segment_id, group in df.groupby(segment_id_column):\n",
    "        # Concatenate all texts in the segment with separators\n",
    "        concatenated_text = ' [SEP] '.join(group[text_column].astype(str).values)\n",
    "        segment_texts.append(concatenated_text)\n",
    "        segment_ids.append(segment_id)\n",
    "    \n",
    "    print(f\"Processing {len(segment_texts)} segments...\")\n",
    "    \n",
    "    # Generate embeddings for concatenated segment texts\n",
    "    model = SentenceTransformer(\"BAAI/bge-m3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        model.half()\n",
    "    \n",
    "    tokenizer = model.tokenizer\n",
    "    segment_embeddings = []\n",
    "    \n",
    "    with tqdm(total=len(segment_texts), desc=\"üöÄ Embedding segments\", unit=\"segment\") as pbar:\n",
    "        for i in range(0, len(segment_texts), batch_size):\n",
    "            batch_texts = segment_texts[i:i+batch_size]\n",
    "            \n",
    "            # Process each text in batch (handle long texts)\n",
    "            batch_embs = []\n",
    "            for text in batch_texts:\n",
    "                emb = embed_long_text(text, model, tokenizer)\n",
    "                batch_embs.append(emb)\n",
    "            \n",
    "            segment_embeddings.extend(batch_embs)\n",
    "            pbar.update(len(batch_texts))\n",
    "    \n",
    "    # Create mapping from segment_id to embedding\n",
    "    segment_embedding_map = dict(zip(segment_ids, segment_embeddings))\n",
    "    \n",
    "    # Map embeddings back to original dataframe\n",
    "    df_result = df.copy()\n",
    "    df_result['Segment_Embeddings'] = df_result[segment_id_column].map(segment_embedding_map)\n",
    "    \n",
    "    print(f\"‚úÖ Segment embeddings generated for {len(segment_ids)} unique segments\")\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPLETE SEGMENTATION PIPELINE ===\n",
    "\n",
    "def run_segmentation_pipeline(parliament, language):\n",
    "    \"\"\"Complete pipeline for segmentation and segment embeddings.\"\"\"\n",
    "    print(f\"\\nüöÄ Starting Segmentation & Segment Embeddings Pipeline\")\n",
    "    print(f\"Parliament: {parliament.upper()}\")\n",
    "    print(f\"Language: {language.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üéØ Using batch size: {DEFAULT_BATCH_SIZE}\")\n",
    "    \n",
    "    # Define paths\n",
    "    input_path = f\"{data_folder}{parliament}_{language}_with_speech_embeddings.pkl\"\n",
    "    segmentation_checkpoint_path = f\"{data_folder}{parliament}_{language}_segmented_with_speech_embeddings.pkl\"\n",
    "    final_path = f\"{data_folder}{parliament}_{language}_final.pkl\"\n",
    "    \n",
    "    # Check if final result already exists\n",
    "    if os.path.exists(final_path):\n",
    "        print(f\"üéØ FINAL RESULT EXISTS: Loading {final_path}\")\n",
    "        df_final = pd.read_pickle(final_path)\n",
    "        print(f\"‚úÖ Loaded final result: {df_final.shape}\")\n",
    "        print(f\"üéØ Segments: {df_final['Segment_ID'].nunique()}\")\n",
    "        return df_final\n",
    "    \n",
    "    # Check if segmented data exists (can skip to segment embeddings)\n",
    "    if os.path.exists(segmentation_checkpoint_path):\n",
    "        print(f\"üîÑ SEGMENTATION CHECKPOINT FOUND: Loading {segmentation_checkpoint_path}\")\n",
    "        df_segmented = pd.read_pickle(segmentation_checkpoint_path)\n",
    "        print(f\"‚úÖ Loaded segmented data: {df_segmented.shape}\")\n",
    "        print(f\"üìä Continuing from segment embeddings...\")\n",
    "        \n",
    "        # Generate segment embeddings\n",
    "        df_final = generate_segment_embeddings(df_segmented)\n",
    "        \n",
    "        # Final save\n",
    "        df_final.to_pickle(final_path)\n",
    "        print(f\"üíæ FINAL: {final_path}\")\n",
    "        return df_final\n",
    "    \n",
    "    # Load data with speech embeddings\n",
    "    print(\"üì• Loading data with speech embeddings...\")\n",
    "    df_with_embeddings = load_speech_embeddings_data(parliament, language)\n",
    "    \n",
    "    # Perform segmentation\n",
    "    print(\"üîÑ Performing segmentation...\")\n",
    "    df_segmented, seg_metrics = segment_speeches(df_with_embeddings, parliament, language)\n",
    "    \n",
    "    # Display segmentation results\n",
    "    metrics_df = pd.DataFrame(seg_metrics)\n",
    "    print(f\"\\n‚úÖ {parliament.upper()} {language.upper()} segmentation complete!\")\n",
    "    print(f\"üìä Results:\")\n",
    "    print(f\"  ‚Ä¢ Total speeches processed: {len(df_segmented):,}\")\n",
    "    print(f\"  ‚Ä¢ Unique segments created: {df_segmented['Segment_ID'].nunique():,}\")\n",
    "    print(f\"  ‚Ä¢ Average speeches per segment: {len(df_segmented) / df_segmented['Segment_ID'].nunique():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Average segments per session: {metrics_df['num_segments'].mean():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Agenda boundaries used: {metrics_df['agenda_boundaries'].sum()}\")\n",
    "    print(f\"  ‚Ä¢ Total boundaries found: {metrics_df['boundaries_found'].sum()}\")\n",
    "    \n",
    "    # Save segmented data checkpoint\n",
    "    df_segmented.to_pickle(segmentation_checkpoint_path)\n",
    "    print(f\"üíæ SEGMENTATION CHECKPOINT: {segmentation_checkpoint_path}\")\n",
    "    \n",
    "    # Generate segment embeddings\n",
    "    print(f\"\\nüîÑ Generating segment embeddings for {parliament.upper()} {language.upper()}...\")\n",
    "    df_final = generate_segment_embeddings(df_segmented)\n",
    "    \n",
    "    # Final save\n",
    "    df_final.to_pickle(final_path)\n",
    "    print(f\"üíæ FINAL: {final_path}\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c82109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROCESSING CONFIGURATION ===\n",
    "# Choose parliament and language to process\n",
    "\n",
    "# CONFIGURATION - Update these variables to select what to process\n",
    "PARLIAMENT_TO_PROCESS = 'croatian'       # Options: 'austrian', 'croatian', 'british'\n",
    "LANGUAGE_TO_PROCESS = 'croatian'        # Options depend on parliament:\n",
    "                                        # Austrian: 'english', 'german'\n",
    "                                        # Croatian: 'english', 'croatian'\n",
    "                                        # British: 'english' (only option)\n",
    "\n",
    "print(f\"üéØ SEGMENTATION & SEGMENT EMBEDDINGS CONFIGURATION\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Parliament: {PARLIAMENT_TO_PROCESS}\")\n",
    "print(f\"Language: {LANGUAGE_TO_PROCESS}\")\n",
    "\n",
    "# Validate configuration\n",
    "try:\n",
    "    config = get_config(PARLIAMENT_TO_PROCESS, LANGUAGE_TO_PROCESS)\n",
    "    print(f\"‚úÖ Configuration valid!\")\n",
    "    print(f\"üëë Chairperson role: {config['chairperson_role']}\")\n",
    "    print(f\"üîß Keyword patterns: {len(config['agenda_keywords']['strong']) + len(config['agenda_keywords']['medium']) + len(config['agenda_keywords']['weak'])}\")\n",
    "    \n",
    "    # Verify input data exists\n",
    "    print(f\"\\nüìä Verifying input data:\")\n",
    "    if verify_speech_embeddings_data(PARLIAMENT_TO_PROCESS, LANGUAGE_TO_PROCESS):\n",
    "        print(f\"\\nüöÄ Ready to process {PARLIAMENT_TO_PROCESS} parliament in {LANGUAGE_TO_PROCESS}!\")\n",
    "        print(f\"üí° Run the next cell to start processing.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Input data verification failed.\")\n",
    "        print(f\"üí° Please run speech embeddings processing first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration error: {e}\")\n",
    "    print(f\"\\nüîß Available options:\")\n",
    "    list_available_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXECUTE SEGMENTATION PIPELINE ===\n",
    "print(f\"üöÄ STARTING SEGMENTATION & SEGMENT EMBEDDINGS PROCESSING\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Parliament: {PARLIAMENT_TO_PROCESS}\")\n",
    "print(f\"Language: {LANGUAGE_TO_PROCESS}\")\n",
    "\n",
    "try:\n",
    "    # Run the complete segmentation pipeline\n",
    "    result = run_segmentation_pipeline(PARLIAMENT_TO_PROCESS, LANGUAGE_TO_PROCESS)\n",
    "    \n",
    "    print(f\"\\nüéâ SEGMENTATION & SEGMENT EMBEDDINGS COMPLETED!\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"üìä Final dataset: {result.shape}\")\n",
    "    print(f\"üéØ Segments created: {result['Segment_ID'].nunique():,}\")\n",
    "    print(f\"üìù Average speeches per segment: {len(result) / result['Segment_ID'].nunique():.1f}\")\n",
    "    print(f\"üî¢ Speech embedding shape: {result['Speech_Embeddings'][0].shape}\")\n",
    "    print(f\"üî¢ Segment embedding shape: {result['Segment_Embeddings'][0].shape}\")\n",
    "    \n",
    "    if 'Speaker_role' in result.columns:\n",
    "        config = get_config(PARLIAMENT_TO_PROCESS, LANGUAGE_TO_PROCESS)\n",
    "        chairperson_count = len(result[result['Speaker_role'] == config['chairperson_role']])\n",
    "        chairperson_pct = chairperson_count / len(result) * 100\n",
    "        print(f\"üëë Chairperson speeches: {chairperson_count:,} ({chairperson_pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüíæ Final output: {PARLIAMENT_TO_PROCESS}_{LANGUAGE_TO_PROCESS}_final.pkl\")\n",
    "    print(f\"üéâ Ready for topic modeling and analysis!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during processing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\nüîß Troubleshooting tips:\")\n",
    "    print(f\"1. Ensure speech embeddings were processed first\")\n",
    "    print(f\"2. Check sufficient GPU memory and disk space\")\n",
    "    print(f\"3. Verify parliament and language configuration\")\n",
    "    print(f\"4. Try restarting runtime if memory issues occur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BATCH PROCESSING OPTION ===\n",
    "# Uncomment and modify this cell to process multiple parliament/language combinations\n",
    "\n",
    "\"\"\"\n",
    "# Example: Process multiple combinations in sequence\n",
    "processing_queue = [\n",
    "    ('austrian', 'english'),\n",
    "    ('austrian', 'german'), \n",
    "    ('croatian', 'english'),\n",
    "    ('croatian', 'croatian'),\n",
    "    ('british', 'english')\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for parliament, language in processing_queue:\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SEGMENTATION PROCESSING: {parliament.upper()} Parliament in {language.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        result = run_segmentation_pipeline(parliament, language)\n",
    "        results[f\"{parliament}_{language}\"] = result\n",
    "        \n",
    "        print(f\"‚úÖ {parliament.upper()} {language.upper()} SEGMENTATION COMPLETED!\")\n",
    "        \n",
    "        # Clear GPU memory between runs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {parliament} {language}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéâ Batch segmentation processing completed!\")\n",
    "print(f\"üìä Successfully processed: {list(results.keys())}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üí° Batch processing option available above.\")\n",
    "print(\"Uncomment and run to process multiple parliament/language combinations.\")\n",
    "print(\"\\nüéØ Processing Pipeline Summary:\")\n",
    "print(\"1. üìù Speech Embeddings Notebook ‚Üí Calculates speech-level embeddings\")\n",
    "print(\"2. üéØ Segmentation Notebook (this one) ‚Üí Segments + segment embeddings\") \n",
    "print(\"3. üìä Analysis ‚Üí Topic modeling and comparative analysis\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
